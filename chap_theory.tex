\chap Theoretical Background

\sec Natural Language Processing

Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on enabling machines to understand and generate human language. While human languages are built from a finite set of symbols, the possible combinations and meanings are vast and often context-dependent, making NLP a complex challenge.

Traditional NLP methods rely heavily on syntactic structures and statistical patterns to perform tasks like information retrieval, spell-checking, and word-level analysis. However, they struggle with deeper understanding, such as interpreting ambiguous sentences or referencing prior context—areas where humans naturally excel due to background knowledge and cognitive associations.

For example, machines find it difficult to resolve pronouns or understand phrases like “I saw the Golden Gate Bridge flying into San Francisco,” where context is essential. This limitation arises from the lack of high-level symbolic reasoning and semantic understanding in current systems. \cite[fundAI]{}

To bridge this gap, modern NLP increasingly focuses on Natural Language Understanding (NLU), which involves cognitive-like processing such as identifying entities, understanding relationships, and representing abstract concepts. Although recent computational models aim to simulate human-like language comprehension, achieving true understanding remains an ongoing challenge.

NLP plays a critical role in applications like machine translation, chatbots, and question answering, and continues to evolve toward more sophisticated and human-like language interaction.


\sec Text Summarization

Theory
Summary
People are under information overload. With an increasing amount of information in the public space, comes a natural need to reduce the amount of this data overload. Textual summarization is a subfield of NLP that deals with the volumetric reduction of text to preserve the most important messages and merit of a matter \cite[summarizationSurvey, summarizationSurvey2]. Important criteria for summarizing are properties such as conciseness, cohesion, and grammatical and factual correctness. The different methods then represent a trade-off between these qualities. Methods can be classified according to several factors\cite[summarizationSurvey]: 
\begitems
* Input type: Single-document or multi-document
* Aim: Generic, domain-specific, or query-based
* Learning phase: Supervised, Unsupervised
* Output type: Extractive or abstractive.
\enditems

Multi-document summarization introduces added complexity due to semantic relationships among the source documents. Techniques targeting this task must account for such inter-document connections. Domain-specific methods leverage additional knowledge, enabling them to generate outputs that are aligned with domain expectations or enriched by external knowledge. Query-based summarization, on the other hand, constructs summaries by answering pre-defined questions.

\secc Extractive summarization

A primary classification in summarization is between extractive and abstractive methods. This distinction significantly influences the nature of the summarization output. Extractive methods generate summaries composed of original sentences or text segments directly extracted from the source text. The core mechanism of such approaches lies in evaluating the informativeness of textual fragments. Sentences with the highest scores are selected for inclusion in the final summary \cite[summarizationIBM]. These summaries typically do not consider linguistic coherence, as they rely solely on sentence extraction. Most extractive approaches require preprocessing steps such as sentence boundary identification, stop-word elimination, and stemming.

Major Extractive Summarization Methods \cite[summarizationSurvey]:

Frequency-based Methods construct a dictionary of key phrases and compute their relative frequencies. Sentences containing high-frequency terms are selected for the summary.

Position-based Methods are based on the assumption that key information tends to appear in specific textual locations (e.g., the beginning or end of documents or paragraphs), these methods prioritize such positions during sentence selection.

Graph-based Methods: Sentences are modeled as nodes in a graph, and relationships between them are represented as edges. The importance of each sentence is assessed using graph-ranking algorithms, such as PageRank. For instance, PacSumm (Position-Augmented Centrality based Summarization) \cite[pacsumm] integrates graph algorithms with BERT (Bidirectional Encoder Representations from Transformers)\cite[bert] for sentence mapping. EdgeSumm\cite[edgeSum] constructs a graph where nouns are nodes and non-nouns serve as edges, with node weights determined by frequency.

Latent Semantic Analysis (LSA) Methods \cite[summaryLSA] rely on algebraic and statistical techniques to uncover latent semantic relationships between words and sentences. A term-sentence matrix is created, and singular value decomposition (SVD) is applied to identify the most informative sentences.

Clustering-based Methods: Similar sentences are grouped into clusters, and representative sentences from each cluster are selected for the final summary. Methods such as Fuzzy Evolutionary Optimization Modeling (FEOM) \cite[fuzzySum] apply evolutionary operations to phrase clustering. K-Means clustering is also commonly employed to group sentences based on similarity.

Supervised Classifier-based Methods utilize labeled data (original texts and corresponding human-generated summaries) to train classifiers that predict whether a sentence should be included in a summary. Sentences are represented using various features (e.g., word frequency, position, length), and classifiers such as Naive Bayes (NBC) \cite[bayessum] or Support Vector Machines (SVM) \cite[supervisedsum] are trained to estimate sentence importance.

Elementary Discourse Unit (EDU) Deletion Methods iteratively eliminate less important EDUs from the text until the desired summary length is reached. For example, the Weighted Compression Model\cite[eduSum] assigns scores to EDUs and selects those with the highest weight-to-cost ratios.

% Hybrid Methods combine multiple extractive techniques to enhance summary quality. For instance, some models merge clustering, word graphs, and neural networks. Other hybrid methods integrate supervised and unsupervised learning paradigms, such as co-training with Probabilistic Support Vector Machines (PSVM) and NBC.

\secc Abstractive summarization

Abstractive methods differ from extractive ones. Instead of copying segments from the input, they generate a condensed version through paraphrasing, often employing a novel vocabulary that may not overlap with the original text. This technique mirrors the human approach to summarization tasks \cite[summarizationSurvey2]. As a result, these methods tend to be more complex. Language models must comprehend context and generalize effectively to produce coherent and factually accurate summaries.

Semantic structure can guide summarization. The Semantic Abstractive Summarization (SAS) framework \cite[dohare] uses Abstract Meaning Representation (AMR) graphs to encode this structure. The process involves three stages: graph extraction, summary graph construction, and surface text generation. In these graphs, nodes represent lexical units, and edges denote semantic relations, including those obtained through co-reference resolution. Key nodes and edges are then selected to construct the summary graph.

Neural encoders are widely used. The MeanSum model \cite[meansum], for example, performs multi-document summarization for a single entity. Each document is passed through an autoencoder to extract its latent representation. These vectors are averaged, and the resulting mean vector is decoded into a summary in natural language.

Transformers improve abstractive summarization. In \cite[ted], a Transformer model undergoes unsupervised pretraining on a corpus of news articles, using abstracts as reference summaries. Fine-tuning ensures the latent representations of summaries reflect semantic similarity or dissimilarity between input texts. The model also incorporates a denoising autoencoder objective, where noisy input sequences are reconstructed to strengthen robustness and generalization.