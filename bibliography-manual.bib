@InProceedings{prompt-sums,
author="Onan, Aytu{\v{g}}
and Alhumyani, Hesham",
editor="Hu, Gongzhu
and Kambhampaty, Krishna K.
and Roy, Indranil",
title="Assessing the Impact of Prompt Strategies on Text Summarization with Large Language Models",
booktitle="Computer Applications in Industry and Engineering",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="41--55",
abstract="The advent of large language models (LLMs) has significantly advanced the field of text summarization, enabling the generation of coherent and contextually accurate summaries. This paper introduces a comprehensive framework for evaluating the performance of state-of-the-art LLMs in text summarization, with a particular focus on the impact of various prompt strategies, including zero-shot, one-shot, and few-shot learning. Our framework systematically examines how these prompting techniques influence summarization quality across diverse datasets, namely CNN/Daily Mail, XSum, TAC08, and TAC09. To provide a robust evaluation, we employ a range of intrinsic metrics such as ROUGE, BLEU, and BERTScore. These metrics allow us to quantify the quality of the generated summaries in terms of precision, recall, and semantic similarity. We evaluated three prominent LLMs: GPT-3, GPT-4, and LLaMA, each configured to optimize summarization performance under different prompting strategies. Our results reveal significant variations in performance depending on the chosen prompting strategy, highlighting the strengths and limitations of each approach. Furthermore, this study provides insights into the optimal conditions for employing different prompt strategies, offering practical guidelines for researchers and practitioners aiming to leverage LLMs for text summarization tasks. By delivering a thorough comparative analysis, we contribute to the understanding of how to maximize the potential of LLMs in generating high-quality summaries, ultimately advancing the field of natural language processing.",
isbn="978-3-031-76273-4"
}

@article{news-sum,
    author = {Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B.},
    title = {Benchmarking Large Language Models for News Summarization},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {39-57},
    year = {2024},
    month = {01},
    abstract = {Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.},
    issn = {2307-387X},
    doi = {10.1162/tacl\_a\_00632},
    url = {https://doi.org/10.1162/tacl_a_00632},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00632/2325685/tacl_a_00632.pdf},
}

@article{SumSur2024,
title = {A survey of text summarization: Techniques, evaluation and challenges},
journal = {Natural Language Processing Journal},
volume = {7},
pages = {100070},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000189},
author = { Supriyono and Aji Prasetya Wibawa and  Suyono and Fachrul Kurniawan},
keywords = {PRISMA, Natural language processing, Syntactic, Semantic, Text summarization, Transfer learning},
abstract = {This paper explores the complex field of text summarization in Natural Language Processing (NLP), with particular attention to the development and importance of semantic understanding. Text summarization is a crucial component of natural language processing (NLP), which helps to translate large amounts of textual data into clear and understandable representations. As the story progresses, it demonstrates the dynamic transition from simple syntactic structures to sophisticated models with semantic comprehension. In order to effectively summarize, syntactic, semantic, and pragmatic concerns become crucial, highlighting the necessity of capturing not only grammar but also the context and underlying meaning. It examines the wide range of summarization models, from conventional extractive techniques to state-of-the-art tools like pre-trained models. Applications are found in many different fields, demonstrating how versatile summarizing techniques are. Semantic drift and domain-specific knowledge remain obstacles, despite progress. In the future, the study predicts developments like artificial intelligence integration and transfer learning, which motivates academics to investigate these prospects for advancement. The approach, which is based on the PRISMA framework, emphasizes a methodical and open literature review. The work attempts to further natural language processing (NLP) and text summarization by combining various research findings and suggesting future research directions in this dynamic subject.}
}

@misc{survayArxiv,
      title={A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models}, 
      author={Haopeng Zhang and Philip S. Yu and Jiawei Zhang},
      year={2024},
      eprint={2406.11289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11289}, 
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{gpt-3,
 author = {et al, Tom Brown},
 option = "aumax:1",
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{huggingfaceModels,
   author = {Inc. Hugging Face},
   title = {Models - Hugging Face},
   url = {https://huggingface.co/models},
   note = {Accessed: 23/03/2025}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{pointer-generator,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1099/",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
}

@inproceedings{MMR,
author = {Carbonell, Jaime and Goldstein, Jade},
title = {The use of MMR, diversity-based reranking for reordering documents and producing summaries},
year = {1998},
isbn = {1581130155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/290941.291025},
doi = {10.1145/290941.291025},
booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {335–336},
numpages = {2},
location = {Melbourne, Australia},
series = {SIGIR '98}
}

@inproceedings{MDS-text-clasification,
author = {Cao, Ziqiang and Li, Wenjie and Li, Sujian and Wei, Furu},
title = {Improving multi-document summarization via text classification},
year = {2017},
publisher = {AAAI Press},
abstract = {Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies. In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization. TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3053–3059},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{MDS-opt-sentence-mod,
author = {Yin, Wenpeng and Pei, Yulong},
title = {Optimizing sentence modeling and selection for document summarization},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Extractive document summarization aims to conclude given documents by extracting some salient sentences. Often, it faces two challenges: 1) how to model the information redundancy among candidate sentences; 2) how to select the most appropriate sentences. This paper attempts to build a strong summarizer DivSelect+CNNLM by presenting new algorithms to optimize each of them. Concretely, it proposes CNNLM, a novel neural network language model (NNLM) based on convolutional neural network (CNN), to project sentences into dense distributed representations, then models sentence redundancy by cosine similarity. Afterwards, it formulates the selection process as an optimization problem, constructing a diversified selection process (DivSelect) with the aim of selecting some sentences which have high prestige, meantime, are dissimilar with each other. Experimental results on DUC2002 and DUC2004 benchmark data sets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {1383–1389},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{coavoux-etal-2019-unsupervised,
    title = "Unsupervised Aspect-Based Multi-Document Abstractive Summarization",
    author = "Coavoux, Maximin  and
      Elsahar, Hady  and
      Gall{\'e}, Matthias",
    editor = "Wang, Lu  and
      Cheung, Jackie Chi Kit  and
      Carenini, Giuseppe  and
      Liu, Fei",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5405/",
    doi = "10.18653/v1/D19-5405",
    pages = "42--47",
    abstract = "User-generated reviews of products or services provide valuable information to customers. However, it is often impossible to read each of the potentially thousands of reviews: it would therefore save valuable time to provide short summaries of their contents. We address opinion summarization, a multi-document summarization task, with an unsupervised abstractive summarization neural system. Our system is based on (i) a language model that is meant to encode reviews to a vector space, and to generate fluent sentences from the same vector space (ii) a clustering step that groups together reviews about the same aspects and allows the system to generate summary sentences focused on these aspects. Our experiments on the Oposum dataset empirically show the importance of the clustering step."
}

@article{LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}


@inproceedings{amplayo-lapata-2021-informative,
    title = "Informative and Controllable Opinion Summarization",
    author = "Amplayo, Reinald Kim  and
      Lapata, Mirella",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.229/",
    doi = "10.18653/v1/2021.eacl-main.229",
    pages = "2662--2672",
    abstract = "Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target (e.g., a movie or a product). Since the number of reviews for each target can be prohibitively large, neural network-based methods follow a two-stage approach where an extractive step first pre-selects a subset of salient opinions and an abstractive step creates the summary while conditioning on the extracted subset. However, the extractive model leads to loss of information which may be useful depending on user needs. In this paper we propose a summarization framework that eliminates the need to rely only on pre-selected content and waste possibly useful information, especially when customizing summaries. The framework enables the use of all input reviews by first condensing them into multiple dense vectors which serve as input to an abstractive model. We showcase an effective instantiation of our framework which produces more informative summaries and also allows to take user preferences into account using our zero-shot customization technique. Experimental results demonstrate that our model improves the state of the art on the Rotten Tomatoes dataset and generates customized summaries effectively."
}

@article{Bi-LSTM,
author = {Schuster, Mike and Paliwal, Kuldip},
year = {1997},
month = {12},
pages = {2673 - 2681},
title = {Bidirectional recurrent neural networks},
volume = {45},
journal = {Signal Processing, IEEE Transactions on},
doi = {10.1109/78.650093}
}

@inproceedings{yasunaga-etal-2017-graph,
    title = "Graph-based Neural Multi-Document Summarization",
    author = "Yasunaga, Michihiro  and
      Zhang, Rui  and
      Meelu, Kshitijh  and
      Pareek, Ayush  and
      Srinivasan, Krishnan  and
      Radev, Dragomir",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1045/",
    doi = "10.18653/v1/K17-1045",
    pages = "452--462",
    abstract = "We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems."
}

@inproceedings{antognini-faltings-2019-learning,
    title = "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization",
    author = "Antognini, Diego  and
      Faltings, Boi",
    editor = "Wang, Lu  and
      Cheung, Jackie Chi Kit  and
      Carenini, Giuseppe  and
      Liu, Fei",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5404/",
    doi = "10.18653/v1/D19-5404",
    pages = "32--41",
    abstract = "Linking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization."
}

@inproceedings{MDS-scisummnet,
author = {Yasunaga, Michihiro and Kasai, Jungo and Zhang, Rui and Fabbri, Alexander R. and Li, Irene and Friedman, Dan and Radev, Dragomir R.},
title = {ScisummNet: a large annotated corpus and content-impact models for scientific paper summarization with citation networks},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33017386},
doi = {10.1609/aaai.v33i01.33017386},
abstract = {Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article's impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors' original highlights (abstract) and the article's actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {907},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{fabbri-etal-2019-multi,
    title = "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    author = "Fabbri, Alexander  and
      Li, Irene  and
      She, Tianwei  and
      Li, Suyi  and
      Radev, Dragomir",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1102/",
    doi = "10.18653/v1/P19-1102",
    pages = "1074--1084",
    abstract = "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."
}

@inproceedings{lebanoff-etal-2018-adapting,
    title = "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization",
    author = "Lebanoff, Logan  and
      Song, Kaiqiang  and
      Liu, Fei",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1446/",
    doi = "10.18653/v1/D18-1446",
    pages = "4131--4141",
    abstract = "Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors."
}

@inproceedings{MDS-wiki,
  author       = {Peter J. Liu and
                  Mohammad Saleh and
                  Etienne Pot and
                  Ben Goodrich and
                  Ryan Sepassi and
                  Lukasz Kaiser and
                  Noam Shazeer},
  title        = {Generating Wikipedia by Summarizing Long Sequences},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=Hyg0vbWC-},
  timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiuSPGSKS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jin-etal-2020-multi,
    title = "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization",
    author = "Jin, Hanqi  and
      Wang, Tianming  and
      Wan, Xiaojun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.556/",
    doi = "10.18653/v1/2020.acl-main.556",
    pages = "6244--6254",
    abstract = "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."
}

@inproceedings{MDS-dialog-sum,
author = {Liu, Chunyi and Wang, Peng and Xu, Jiang and Li, Zang and Ye, Jieping},
title = {Automatic Dialogue Summary Generation for Customer Service},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330683},
doi = {10.1145/3292500.3330683},
abstract = {Dialogue summarization extracts useful information from a dialogue. It helps people quickly capture the highlights of a dialogue without going through long and sometimes twisted utterances. For customer service, it saves human resources currently required to write dialogue summaries. A main challenge of dialogue summarization is to design a mechanism to ensure the logic, integrity, and correctness of the summaries. In this paper, we introduce auxiliary key point sequences to solve this problem. A key point sequence describes the logic of the summary. In our training procedure, a key point sequence acts as an auxiliary label. It helps the model learn the logic of the summary. In the prediction procedure, our model predicts the key point sequence first and then uses it to guide the prediction of the summary. Along with the auxiliary key point sequence, we propose a novel Leader-Writer network. The Leader net predicts the key point sequence, and the Writer net predicts the summary based on the decoded key point sequence. The Leader net ensures the summary is logical and integral. The Writer net focuses on generating fluent sentences. We test our model on customer service scenarios. The results show that our model outperforms other models not only on BLEU and ROUGE-L score but also on logic and integrity.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1957–1965},
numpages = {9},
keywords = {leader-writer net, hierarchical encoder-decoder, dialogue summarization, customer service},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{li-etal-2020-leveraging-graph,
    title = "Leveraging Graph to Improve Abstractive Multi-Document Summarization",
    author = "Li, Wei  and
      Xiao, Xinyan  and
      Liu, Jiachen  and
      Wu, Hua  and
      Wang, Haifeng  and
      Du, Junping",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.555/",
    doi = "10.18653/v1/2020.acl-main.555",
    pages = "6232--6243",
    abstract = "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
}

@inproceedings{pasunuru-etal-2021-efficiently,
    title = "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
    author = "Pasunuru, Ramakanth  and
      Liu, Mengwen  and
      Bansal, Mohit  and
      Ravi, Sujith  and
      Dreyer, Markus",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.380/",
    doi = "10.18653/v1/2021.naacl-main.380",
    pages = "4768--4779",
    abstract = "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents."
}

@article{gpt-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  note = {Accessed: 2-20-2025}
}

@misc{mlflow,
  title        = {mlflow/mlflow},
  author       = {MLflow Project},
  year         = 2025,
  note         = {\url{https://github.com/mlflow/mlflow} Accessed: 5-15-2025}
}


@INPROCEEDINGS {MDS-topic-centric,
author = { Alambo, Amanuel and Lohstroh, Cori and Madaus, Erik and Padhee, Swati and Foster, Brandy and Banerjee, Tanvi and Thirunarayan, Krishnaprasad and Raymer, Michael },
booktitle = { 2020 IEEE International Conference on Big Data (Big Data) },
title = {{ Topic-Centric Unsupervised Multi-Document Summarization of Scientific and News Articles }},
year = {2020},
volume = {},
ISSN = {},
pages = {591-596},
abstract = { Recent advances in natural language processing have enabled automation of a wide range of tasks, including machine translation, named entity recognition, and sentiment analysis. Automated summarization of documents, or groups of documents, however, has remained elusive, with many efforts limited to extraction of keywords, key phrases, or key sentences. Accurate abstractive summarization has yet to be achieved due to the inherent difficulty of the problem, and limited availability of training data. In this paper, we propose a topic-centric unsupervised multi-document summarization framework to generate extractive and abstractive summaries for groups of scientific articles across 20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles from DUC-2004 Task 2. The proposed algorithm generates an abstractive summary by developing salient language unit selection and text generation techniques. Our approach matches the state-of-the-art when evaluated on automated extractive evaluation metrics and performs better for abstractive summarization on five human evaluation metrics (entailment, coherence, conciseness, readability, and grammar). We achieve a kappa score of 0.68 between two co-author linguists who evaluated our results. We plan to publicly share MAG- 20, a human-validated gold standard dataset of topic-clustered research articles and their summaries to promote research in abstractive summarization. },
keywords = {Measurement;Sentiment analysis;Training data;Big Data;Grammar;Machine translation;Task analysis},
doi = {10.1109/BigData50022.2020.9378403},
url = {https://doi.ieeecomputersociety.org/10.1109/BigData50022.2020.9378403},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec}

@inproceedings{PEGASUS,
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
title = {PEGASUS: pre-training with extracted gap-sentences for abstractive summarization},
year = {2020},
publisher = {JMLR.org},
abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pretraining large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {1051},
numpages = {12},
series = {ICML'20}
}

@inproceedings{wu-etal-2021-controllable,
    title = "Controllable Abstractive Dialogue Summarization with Sketch Supervision",
    author = "Wu, Chien-Sheng  and
      Liu, Linqing  and
      Liu, Wenhao  and
      Stenetorp, Pontus  and
      Xiong, Caiming",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.454/",
    doi = "10.18653/v1/2021.findings-acl.454",
    pages = "5108--5122"
}

@inproceedings{multi-news-dataset,
    title = "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    author = "Fabbri, Alexander  and
      Li, Irene  and
      She, Tianwei  and
      Li, Suyi  and
      Radev, Dragomir",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1102/",
    doi = "10.18653/v1/P19-1102",
    pages = "1074--1084",
    abstract = "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."
}

@inproceedings{arxiv-dataset,
    title = "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    author = "Cohan, Arman  and
      Dernoncourt, Franck  and
      Kim, Doo Soon  and
      Bui, Trung  and
      Kim, Seokhwan  and
      Chang, Walter  and
      Goharian, Nazli",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2097/",
    doi = "10.18653/v1/N18-2097",
    pages = "615--621",
    abstract = "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models."
}

@inproceedings{MDS-hetero-GNN,
    title = "Heterogeneous Graph Neural Networks for Extractive Document Summarization",
    author = "Wang, Danqing  and
      Liu, Pengfei  and
      Zheng, Yining  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.553/",
    doi = "10.18653/v1/2020.acl-main.553",
    pages = "6209--6219",
    abstract = "As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."
}

@inproceedings{concat-embed,
    title = "Automated Concatenation of Embeddings for Structured Prediction",
    author = "Wang, Xinyu  and
      Jiang, Yong  and
      Bach, Nguyen  and
      Wang, Tao  and
      Huang, Zhongqiang  and
      Huang, Fei  and
      Tu, Kewei",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.206/",
    doi = "10.18653/v1/2021.acl-long.206",
    pages = "2643--2660",
    abstract = "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations."
}

@article{NER,
   author = {Jing Li and Aixin Sun and Jianglei Han and Chenliang Li},
   doi = {10.1109/TKDE.2020.2981314},
   issn = {1041-4347},
   issue = {1},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   month = {1},
   pages = {50-70},
   title = {A Survey on Deep Learning for Named Entity Recognition},
   volume = {34},
   year = {2022},
}

@inproceedings{timeline-sum,
   abstract = {Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to TLS have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for TLS. Our system outperforms ex-tractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong compression. In these cases, our oracle experiments confirm that our approach also has a higher upper bound for ROUGE scores than extractive methods. A study with human judges shows that our ab-stractive system also produces output that is easy to read and understand.},
   author = {Julius Steen and Katja Markert},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/D19-5403},
   booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},
   pages = {21-31},
   publisher = {Association for Computational Linguistics},
   title = {Abstractive Timeline Summarization},
   url = {https://www.aclweb.org/anthology/D19-5403},
   year = {2019},
}
@inproceedings{towards-timeline-sum,
   abstract = {Timeline summarization targets at concisely summarizing the evolution trajectory along the timeline and existing timeline summarization approaches are all based on extractive methods. In this paper, we propose the task of abstractive timeline summa-rization, which tends to concisely paraphrase the information in the time-stamped events. Unlike traditional document summarization, timeline sum-marization needs to model the time series information of the input events and summarize important events in chronological order. To tackle this challenge , we propose a memory-based timeline sum-marization model (MTS). Concretely, we propose a time-event memory to establish a timeline, and use the time position of events on this timeline to guide generation process. Besides, in each decoding step, we incorporate event-level information into word-level attention to avoid confusion between events. Extensive experiments are conducted on a large-scale real-world dataset, and the results show that MTS achieves the state-of-the-art performance in terms of both automatic and human evaluations.},
   author = {Xiuying Chen and Zhangming Chan and Shen Gao and Meng-Hsuan Yu and Dongyan Zhao and Rui Yan},
   city = {California},
   doi = {10.24963/ijcai.2019/686},
   isbn = {978-0-9992411-4-1},
   booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
   keywords = {Natural Language Processing: Natural Language Generation,Natural Language Processing: Natural Language Summarization},
   month = {8},
   pages = {4939-4945},
   publisher = {International Joint Conferences on Artificial Intelligence Organization},
   title = {Learning towards Abstractive Timeline Summarization},
   url = {https://www.ijcai.org/proceedings/2019/686},
   year = {2019},
}

@article{event-extraction,
   abstract = {Event extraction (EE) is a crucial research task for promptly apprehending event information from massive textual data. With the rapid development of deep learning, EE based on deep learning technology has become a research hotspot. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This article fills the research gap by reviewing the state-of-the-art approaches, especially focusing on the general domain EE based on deep learning models. We introduce a new literature classification of current general domain EE research according to the task definition. Afterward, we summarize the paradigm and models of EE approaches, and then discuss each of them in detail. As an important aspect, we summarize the benchmarks that support tests of predictions and evaluation metrics. A comprehensive comparison among different approaches is also provided in this survey. Finally, we conclude by summarizing future research directions facing the research area.},
   author = {Qian Li and Jianxin Li and Jiawei Sheng and Shiyao Cui and Jia Wu and Yiming Hei and Hao Peng and Shu Guo and Lihong Wang and Amin Beheshti and Philip S. Yu},
   doi = {10.1109/TNNLS.2022.3213168},
   issn = {21622388},
   issue = {5},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,evaluation metrics,event extraction (EE),research trends},
   month = {5},
   pages = {6301-6321},
   pmid = {36269921},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey on Deep Learning Event Extraction: Approaches and Applications},
   volume = {35},
   year = {2024},
}


@article{DeepEventMine,
   abstract = {Motivation: Recent neural approaches on event extraction from text mainly focus on flat events in general domain, while there are less attempts to detect nested and overlapping events. These existing systems are built on given entities and they depend on external syntactic tools. Results: We propose an end-to-end neural nested event extraction model named DeepEventMine that extracts multiple overlapping directed acyclic graph structures from a raw sentence. On the top of the bidirectional encoder representations from transformers model, our model detects nested entities and triggers, roles, nested events and their modifications in an end-to-end manner without any syntactic tools. Our DeepEventMine model achieves the new state-of-the-art performance on seven biomedical nested event extraction tasks. Even when gold entities are unavailable, our model can detect events from raw text with promising performance. Availability and implementation: Our codes and models to reproduce the results are available at: https://github. com/aistairc/DeepEventMine.},
   author = {Hai Long Trieu and Thy Thy Tran and Khoa N.A. Duong and Anh Nguyen and Makoto Miwa and Sophia Ananiadou},
   doi = {10.1093/bioinformatics/btaa540},
   issn = {14602059},
   issue = {19},
   journal = {Bioinformatics},
   month = {10},
   pages = {4910-4917},
   pmid = {33141147},
   publisher = {Oxford University Press},
   title = {DeepEventMine: End-to-end neural nested event extraction from biomedical texts},
   volume = {36},
   year = {2020},
}

@article{coref-survay,
  title={A brief survey on recent advances in coreference resolution},
  author={Liu, Ruicheng and Mao, Rui and Luu, Anh Tuan and Cambria, Erik},
  journal={Artificial Intelligence Review},
  volume={56},
  number={12},
  pages={14439--14481},
  year={2023},
  publisher={Springer}
}

@inproceedings{clark-manning-2015-entity,
    title = "Entity-Centric Coreference Resolution with Model Stacking",
    author = "Clark, Kevin  and
      Manning, Christopher D.",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1136/",
    doi = "10.3115/v1/P15-1136",
    pages = "1405--1415"
}

@inproceedings{lee-etal-2018-higher,
    title = "Higher-Order Coreference Resolution with Coarse-to-Fine Inference",
    author = "Lee, Kenton  and
      He, Luheng  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2108/",
    doi = "10.18653/v1/N18-2108",
    pages = "687--692",
    abstract = "We introduce a fully-differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient."
}

@inproceedings{wiseman-etal-2016-learning,
    title = "Learning Global Features for Coreference Resolution",
    author = "Wiseman, Sam  and
      Rush, Alexander M.  and
      Shieber, Stuart M.",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1114/",
    doi = "10.18653/v1/N16-1114",
    pages = "994--1004"
}

@inproceedings{aralikatte-etal-2019-rewarding,
    title = "Rewarding Coreference Resolvers for Being Consistent with World Knowledge",
    author = "Aralikatte, Rahul  and
      Lent, Heather  and
      Gonzalez, Ana Valeria  and
      Hershcovich, Daniel  and
      Qiu, Chen  and
      Sandholm, Anders  and
      Ringaard, Michael  and
      S{\o}gaard, Anders",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1118/",
    doi = "10.18653/v1/D19-1118",
    pages = "1229--1235",
    abstract = "Unresolved coreference is a bottleneck for relation extraction, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning."
}

@inproceedings{emami-etal-2018-generalized,
    title = "A Generalized Knowledge Hunting Framework for the {W}inograd Schema Challenge",
    author = "Emami, Ali  and
      Trischler, Adam  and
      Suleman, Kaheer  and
      Cheung, Jackie Chi Kit",
    editor = "Cordeiro, Silvio Ricardo  and
      Oraby, Shereen  and
      Pavalanathan, Umashanthi  and
      Rim, Kyeongmin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-4004/",
    doi = "10.18653/v1/N18-4004",
    pages = "25--31",
    abstract = "We introduce an automatic system that performs well on two common-sense reasoning tasks, the Winograd Schema Challenge (WSC) and the Choice of Plausible Alternatives (COPA). Problem instances from these tasks require diverse, complex forms of inference and knowledge to solve. Our method uses a knowledge-hunting module to gather text from the web, which serves as evidence for candidate problem resolutions. Given an input problem, our system generates relevant queries to send to a search engine. It extracts and classifies knowledge from the returned results and weighs it to make a resolution. Our approach improves F1 performance on the WSC by 0.16 over the previous best and is competitive with the state-of-the-art on COPA, demonstrating its general applicability."
}

@inproceedings{joshi-etal-2019-bert,
    title = "{BERT} for Coreference Resolution: Baselines and Analysis",
    author = "Joshi, Mandar  and
      Levy, Omer  and
      Zettlemoyer, Luke  and
      Weld, Daniel",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1588/",
    doi = "10.18653/v1/D19-1588",
    pages = "5803--5808",
    abstract = "We apply BERT to coreference resolution, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication."
}

@inproceedings{wu-etal-2020-corefqa,
    title = "{C}oref{QA}: Coreference Resolution as Query-based Span Prediction",
    author = "Wu, Wei  and
      Wang, Fei  and
      Yuan, Arianna  and
      Wu, Fei  and
      Li, Jiwei",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.622/",
    doi = "10.18653/v1/2020.acl-main.622",
    pages = "6953--6963",
    abstract = "In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model`s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark."
}

@inproceedings{caciularu-etal-2021-cdlm-cross,
    title = "{CDLM}: Cross-Document Language Modeling",
    author = "Caciularu, Avi  and
      Cohan, Arman  and
      Beltagy, Iz  and
      Peters, Matthew  and
      Cattan, Arie  and
      Dagan, Ido",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.225/",
    doi = "10.18653/v1/2021.findings-emnlp.225",
    pages = "2648--2662",
    abstract = "We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model), a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM, and work in synergy to set new state-of-the-art results for several multi-text tasks."
}

@inproceedings{dbscan,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {handling nlj4-275oise, efficiency on large spatial databases, clustering algorithms, arbitrary shape of clusters},
location = {Portland, Oregon},
series = {KDD'96}
}

@ARTICLE{em,
  author={Moon, T.K.},
  journal={IEEE Signal Processing Magazine}, 
  title={The expectation-maximization algorithm}, 
  year={1996},
  volume={13},
  number={6},
  pages={47-60},
  keywords={Signal processing algorithms;Parameter estimation;Hidden Markov models;Maximum likelihood estimation;Phase detection;Convergence;Image reconstruction;Probability distribution;Histograms;Estimation theory},
  doi={10.1109/79.543975}}

@incollection{autoclass,
title = {AutoClass: A Bayesian Classification System},
editor = {John Laird},
booktitle = {Machine Learning Proceedings 1988},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {54-64},
year = {1988},
isbn = {978-0-934613-64-4},
doi = {https://doi.org/10.1016/B978-0-934613-64-4.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780934613644500116},
author = {Peter Cheeseman and James Kelly and Matthew Self and John Stutz and Will Taylor and Don Freeman},
abstract = {This paper describes AutoClass II, a program for automatically discovering (inducing) classes from a database, based on a Bayesian statistical technique which automatically determines the most probable number of classes, their probabilistic descriptions, and the probability that each object is a member of each class. AutoClass has been tested on several large, real databases and has discovered previously unsuspected classes. There is no doubt that these classes represent new phenomena.}
}

@article{c-means,
title = {FCM: The fuzzy c-means clustering algorithm},
journal = {Computers \& Geosciences},
volume = {10},
number = {2},
pages = {191-203},
year = {1984},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(84)90020-7},
url = {https://www.sciencedirect.com/science/article/pii/0098300484900207},
author = {James C. Bezdek and Robert Ehrlich and William Full},
keywords = {Cluster analysis, Cluster validity, Fuzzy clustering, Fuzzy QMODEL, Least-squared errors},
abstract = {This paper transmits a FORTRAN-IV coding of the fuzzy c-means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.}
}

@misc{sota-extractive,
      title={Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization}, 
      author={Léo Hemamou and Mehdi Debiane},
      year={2024},
      eprint={2408.15801},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.15801}, 
}

@inproceedings{salesforce,
    title = "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
    author = "Liu, Yixin  and
      Fabbri, Alexander  and
      Chen, Jiawen  and
      Zhao, Yilun  and
      Han, Simeng  and
      Joty, Shafiq  and
      Liu, Pengfei  and
      Radev, Dragomir  and
      Wu, Chien-Sheng  and
      Cohan, Arman",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.280/",
    doi = "10.18653/v1/2024.findings-naacl.280",
    pages = "4481--4501",
    abstract = "While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction."
}

@misc{gpt-4,
      title={GPT-4 Technical Report}, 
      author={OpenAI, Josh Achiam et. al},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{RL3,
      title={Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback}, 
      author={Paul Roit and Johan Ferret and Lior Shani and Roee Aharoni and Geoffrey Cideron and Robert Dadashi and Matthieu Geist and Sertan Girgin and Léonard Hussenot and Orgad Keller and Nikola Momchev and Sabela Ramos and Piotr Stanczyk and Nino Vieillard and Olivier Bachem and Gal Elidan and Avinatan Hassidim and Olivier Pietquin and Idan Szpektor},
      year={2023},
      eprint={2306.00186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.00186}, 
}

Timeline summarizaton
=====================

@inproceedings{0686,
  title     = {Learning towards Abstractive Timeline Summarization},
  author    = {Chen, Xiuying and Chan, Zhangming and Gao, Shen and Yu, Meng-Hsuan and Zhao, Dongyan and Yan, Rui},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {4939--4945},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/686},
  url       = {https://doi.org/10.24963/ijcai.2019/686},
}

@inproceedings{2021.emnlp-main.519,
    title = "Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport",
    author = "Li, Manling  and
      Ma, Tengfei  and
      Yu, Mo  and
      Wu, Lingfei  and
      Gao, Tian  and
      Ji, Heng  and
      McKeown, Kathleen",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.519/",
    doi = "10.18653/v1/2021.emnlp-main.519",
    pages = "6443--6456",
    abstract = "Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events' intra-structures (arguments) and inter-structures (event-event connections). Following a different route, we propose to represent the news articles as an event-graph, thus the summarization becomes compressing the whole graph to its salient sub-graph. The key hypothesis is that the events connected through shared arguments and temporal order depict the skeleton of a timeline, containing events that are semantically related, temporally coherent and structurally salient in the global event graph. A time-aware optimal transport distance is then introduced for learning the compression model in an unsupervised manner. We show that our approach significantly improves on the state of the art on three real-world datasets, including two public standard benchmarks and our newly collected Timeline100 dataset."
}

@inproceedings{2024.acl-long.390,
    title = "From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models",
    author = "Hu, Qisheng  and
      Moon, Geonsik  and
      Ng, Hwee Tou",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.390/",
    doi = "10.18653/v1/2024.acl-long.390",
    pages = "7232--7246",
    abstract = "Timeline summarization (TLS) is essential for distilling coherent narratives from a vast collection of texts, tracing the progression of events and topics over time. Prior research typically focuses on either event or topic timeline summarization, neglecting the potential synergy of these two forms. In this study, we bridge this gap by introducing a novel approach that leverages large language models (LLMs) for generating both event and topic timelines. Our approach diverges from conventional TLS by prioritizing event detection, leveraging LLMs as pseudo-oracles for incremental event clustering and the construction of timelines from a text stream. As a result, it produces a more interpretable pipeline. Empirical evaluation across four TLS benchmarks reveals that our approach outperforms the best prior published approaches, highlighting the potential of LLMs in timeline summarization for real-world applications."
}

@misc{2301.00867v1,
      title={Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order}, 
      author={Xiuying Chen and Mingzhe Li and Shen Gao and Zhangming Chan and Dongyan Zhao and Xin Gao and Xiangliang Zhang and Rui Yan},
      year={2023},
      eprint={2301.00867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00867}, 
}

@misc{2412.17408v1,
      title={Just What You Desire: Constrained Timeline Summarization with Self-Reflection for Enhanced Relevance}, 
      author={Muhammad Reza Qorib and Qisheng Hu and Hwee Tou Ng},
      year={2024},
      eprint={2412.17408},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17408}, 
}

@inproceedings{3626772.3657899,
author = {Sojitra, Daivik and Jain, Raghav and Saha, Sriparna and Jatowt, Adam and Gupta, Manish},
title = {Timeline Summarization in the Era of LLMs},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657899},
doi = {10.1145/3626772.3657899},
abstract = {Timeline summarization is the task of automatically generating concise overviews of documents that capture the key events and their progression on timelines. While this capability is useful for quickly comprehending event sequences without reading lengthy descriptions, timeline summarization remains a relatively underexplored area in recent years when compared to traditional document summarization task and their evolution. The advent of large language models (LLMs) has led some to presume summarization as a solved problem. However, timeline summarization poses unique challenges for LLMs. Our investigation is centered on evaluating the performance of LLMs, against state-of-the-art models in this field. We employed three different approaches: chunking, knowledge graph-based summarization, and TimeRanker. Each of these methods was systematically tested on three benchmark datasets for timeline summarization to assess their effectiveness in capturing and condensing key events and their evolution within timelines. Our findings reveal that while LLMs show promise, timeline summarization remains a complex task that is not yet fully resolved.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2657–2661},
numpages = {5},
keywords = {benchmarking, knowledge graphs, llms, timeline summarization},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{D19-5403,
    title = "Abstractive Timeline Summarization",
    author = "Steen, Julius  and
      Markert, Katja",
    editor = "Wang, Lu  and
      Cheung, Jackie Chi Kit  and
      Carenini, Giuseppe  and
      Liu, Fei",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5403/",
    doi = "10.18653/v1/D19-5403",
    pages = "21--31",
    abstract = "Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to TLS have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for TLS. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong compression. In these cases, our oracle experiments confirm that our approach also has a higher upper bound for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand."
}

Event Extraction
================

@misc{EE-inst-tuning,
      title={Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines}, 
      author={Saurabh Srivastava and Sweta Pati and Ziyu Yao},
      year={2025},
      eprint={2502.16377},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.16377}, 
}

@misc{multi-sources-ee,
      title={Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm}, 
      author={Qiang Gao and Zixiang Meng and Bobo Li and Jun Zhou and Fei Li and Chong Teng and Donghong Ji},
      year={2024},
      eprint={2406.16021},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16021}, 
}

@inproceedings{ee-generative,
    title = "Generative Approaches to Event Extraction: Survey and Outlook",
    author = "Simon, {\'E}tienne  and
      Olsen, Helene  and
      You, Huiling  and
      Touileb, Samia  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    editor = "Tetreault, Joel  and
      Nguyen, Thien Huu  and
      Lamba, Hemank  and
      Hughes, Amanda",
    booktitle = "Proceedings of the Workshop on the Future of Event Detection (FuturED)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.futured-1.7/",
    doi = "10.18653/v1/2024.futured-1.7",
    pages = "73--86",
    abstract = "enter abstract here"
}

@misc{ee-prompt-opt,
      title={Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction}, 
      author={Saurabh Srivastava and Ziyu Yao},
      year={2025},
      eprint={2504.07357},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.07357}, 
}

@ARTICLE{event-extraction-survay,
  author={Li, Qian and Li, Jianxin and Sheng, Jiawei and Cui, Shiyao and Wu, Jia and Hei, Yiming and Peng, Hao and Guo, Shu and Wang, Lihong and Beheshti, Amin and Yu, Philip S.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Survey on Deep Learning Event Extraction: Approaches and Applications}, 
  year={2024},
  volume={35},
  number={5},
  pages={6301-6321},
  keywords={Task analysis;Deep learning;Data mining;Measurement;Feature extraction;Technological innovation;Deep learning;evaluation metrics;event extraction (EE);research trends},
  doi={10.1109/TNNLS.2022.3213168}}

@misc{ACE2005,
  doi = {10.35111/MWXC-VH88},
  url = {https://catalog.ldc.upenn.edu/LDC2006T06},
  author = {{Walker,  Christopher} and {Strassel,  Stephanie} and {Medero,  Julie} and {Maeda,  Kazuaki}},
  title = {ACE 2005 Multilingual Training Corpus},
  publisher = {Linguistic Data Consortium},
  year = {2006}
}