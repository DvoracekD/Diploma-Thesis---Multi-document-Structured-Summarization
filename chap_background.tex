\chap Background and Related Work

\sec Natural Language Processing

Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on enabling machines to understand and generate human language. While human languages are built from a finite set of symbols, the possible combinations and meanings are vast and often context-dependent, making NLP a complex challenge.

Traditional NLP methods rely heavily on syntactic structures and statistical patterns to perform tasks like information retrieval, spell-checking, and word-level analysis. However, they struggle with deeper understanding, such as interpreting ambiguous sentences or referencing prior context—areas where humans naturally excel due to background knowledge and cognitive associations.

For example, machines find it difficult to resolve pronouns or understand phrases like “I saw the Golden Gate Bridge flying into San Francisco,” where context is essential. This limitation arises from the lack of high-level symbolic reasoning and semantic understanding in current systems. \cite[fundAI]{}

To bridge this gap, modern NLP increasingly focuses on Natural Language Understanding (NLU), which involves cognitive-like processing such as identifying entities, understanding relationships, and representing abstract concepts. Although recent computational models aim to simulate human-like language comprehension, achieving true understanding remains an ongoing challenge.

NLP plays a critical role in applications like machine translation, chatbots, and question answering, and continues to evolve toward more sophisticated and human-like language interaction.

\secc Data representations
% TODO

\secc Metrics
%TODO

\sec Neural networks in NLP
%TODO

\sec Transformer architecture
Most modern NLP solutions are based on {\it transformers}. The Transformer architecture, introduced in the paper {\it Attention Is All You Need}\cite[transformer]{}, revolutionized sequence transduction tasks by eliminating recurrence and convolutions, relying instead entirely on self-attention mechanisms. Its encoder-decoder structure leveraged multi-head self-attention layers combined with position-wise feed-forward networks, enhanced by residual connections and layer normalization. The self-attention mechanism efficiently modeled dependencies across all positions in a sequence, while positional encodings preserved order information. This design allowed the Transformer to process sequences with unparalleled parallelism, significantly reducing training time compared to recurrent or convolutional models.

The architecture introduced innovations such as scaled dot-product attention and multi-head attention, enabling the model to attend to multiple representation subspaces simultaneously. In the encoder, all positions could influence each other globally, while the decoder incorporated masked self-attention to ensure autoregressive sequence generation. These design choices allowed the Transformer to achieve state-of-the-art results in tasks like machine translation and demonstrate exceptional adaptability to other domains. Its focus on attention mechanisms and computational efficiency has since become a foundation for modern deep learning systems.

\medskip  \clabel[transformer-architecture]{Transformer model architecture}
\picw=6cm \cinspic figs/transformer.png
\caption/f Transformer model architecture.
\medskip
%TODO

% \secc Constrained inference

\sec Text Summarization

People are under information overload. With an increasing amount of information in the public space, comes a natural need to reduce the amount of this data overload. Textual summarization is a subfield of NLP that deals with the volumetric reduction of text to preserve the most important messages and merit of a matter \cite[summarizationSurvey, summarizationSurvey2]. Important criteria for summarizing are properties such as conciseness, cohesion, and grammatical and factual correctness. The different methods then represent a trade-off between these qualities. Methods can be classified according to several factors\cite[summarizationSurvey]: 
\begitems
* Input type: Single-document or multi-document
* Aim: Generic, domain-specific, or query-based
* Learning phase: Supervised, Unsupervised
* Output type: Extractive or abstractive.
\enditems

Multi-document summarization introduces added complexity due to semantic relationships among the source documents. Techniques targeting this task must account for such inter-document connections. Domain-specific methods leverage additional knowledge, enabling them to generate outputs that are aligned with domain expectations or enriched by external knowledge. Query-based summarization, on the other hand, constructs summaries by answering pre-defined questions.

\secc Extractive summarization

A primary classification in summarization is between extractive and abstractive methods. This distinction significantly influences the nature of the summarization output. Extractive methods generate summaries composed of original sentences or text segments directly extracted from the source text. The core mechanism of such approaches lies in evaluating the informativeness of textual fragments. Sentences with the highest scores are selected for inclusion in the final summary \cite[summarizationIBM]. These summaries typically do not consider linguistic coherence, as they rely solely on sentence extraction. Most extractive approaches require preprocessing steps such as sentence boundary identification, stop-word elimination, and stemming.

Major Extractive Summarization Methods \cite[summarizationSurvey] are the following:

Frequency-based Methods construct a dictionary of key phrases and compute their relative frequencies. Sentences containing high-frequency terms are selected for the summary.

Position-based Methods are based on the assumption that key information tends to appear in specific textual locations (e.g., the beginning or end of documents or paragraphs), these methods prioritize such positions during sentence selection.

Graph-based Methods: Sentences are modeled as nodes in a graph, and relationships between them are represented as edges. The importance of each sentence is assessed using graph-ranking algorithms, such as PageRank. For instance, PacSumm (Position-Augmented Centrality based Summarization) \cite[pacsumm] integrates graph algorithms with BERT (Bidirectional Encoder Representations from Transformers)\cite[bert] for sentence mapping. EdgeSumm\cite[edgeSum] constructs a graph where nouns are nodes and non-nouns serve as edges, with node weights determined by frequency.

Latent Semantic Analysis (LSA) Methods \cite[summaryLSA] rely on algebraic and statistical techniques to uncover latent semantic relationships between words and sentences. A term-sentence matrix is created, and singular value decomposition (SVD) is applied to identify the most informative sentences.

Clustering-based Methods: Similar sentences are grouped into clusters, and representative sentences from each cluster are selected for the final summary. Methods such as Fuzzy Evolutionary Optimization Modeling (FEOM) \cite[fuzzySum] apply evolutionary operations to phrase clustering. K-Means clustering is also commonly employed to group sentences based on similarity.

Supervised Classifier-based Methods utilize labeled data (original texts and corresponding human-generated summaries) to train classifiers that predict whether a sentence should be included in a summary. Sentences are represented using various features (e.g., word frequency, position, length), and classifiers such as Naive Bayes (NBC) \cite[bayessum] or Support Vector Machines (SVM) \cite[supervisedsum] are trained to estimate sentence importance.

Elementary Discourse Unit (EDU) Deletion Methods iteratively eliminate less important EDUs from the text until the desired summary length is reached. For example, the Weighted Compression Model\cite[eduSum] assigns scores to EDUs and selects those with the highest weight-to-cost ratios.

Hybrid Methods combine multiple extractive techniques to enhance summary quality. For instance, some models \cite[hybrid1] merge clustering, word graphs, and neural networks. Other hybrid methods \cite[hybrid2, bayessum] integrate supervised and unsupervised learning paradigms, such as co-training with Probabilistic Support Vector Machines (PSVM) and Na\"{\i}ve Bayes classifier.

\secc Abstractive summarization

Abstractive methods differ from extractive ones. Instead of copying segments from the input, they generate a condensed version through paraphrasing, often employing a novel vocabulary that may not overlap with the original text. This technique mirrors the human approach to summarization tasks \cite[summarizationSurvey2]. As a result, these methods tend to be more complex. Language models must comprehend context and generalize effectively to produce coherent and factually accurate summaries.

Semantic structure can guide summarization. The Semantic Abstractive Summarization (SAS) framework \cite[dohare] uses Abstract Meaning Representation (AMR) graphs to encode this structure. The process involves three stages: graph extraction, summary graph construction, and surface text generation. In these graphs, nodes represent lexical units, and edges denote semantic relations, including those obtained through co-reference resolution. Key nodes and edges are then selected to construct the summary graph.

Neural encoders are widely used. The MeanSum model \cite[meansum], for example, performs multi-document summarization, where the documents relate to single entity (for example reviews of a restaurant). Each document is passed through an autoencoder to extract its latent representation. These vectors are averaged, and the resulting mean vector is decoded into a summary in natural language.

Transformers improve abstractive summarization. In \cite[ted], a Transformer model undergoes unsupervised pretraining on a corpus of news articles, using abstracts as reference summaries. Fine-tuning ensures the latent representations of summaries reflect semantic similarity or dissimilarity between input texts. The model also incorporates a denoising autoencoder objective, where noisy input sequences are reconstructed to strengthen robustness and generalization.

% TODO: Moznost pridat dalsi

\secc Reinforcement learning

A promising avenue of research is reinforcement learning. These methods rely on the iterative refinement of existing approaches through the incorporation of user feedback. One of the proposed methods\cite[autoRL]  presents an extractive single-document summarization approach grounded in reinforcement learning (RL) and enhanced by embedding-based features. The authors employ a Deep Q-Network (DQN) \cite[DQN] to estimate Q-values for candidate sentences, guiding the selection of sentences that maximize the expected quality of the final summary. Each sentence is represented through two types of embeddings: a content vector (from averaged GloVe word embeddings) and a position vector capturing hierarchical structural information (sentence, paragraph, and section positions). The model architecture comprises three modules: one assessing semantic relevance between the document and candidate sentences, another capturing the contextual compatibility between candidate sentences and the partial summary using a GRU-based RNN, and a final module integrating these signals to compute Q-values via regression.

SAAR\cite[SAAR] method also refines the outputs with user feedback. The system is designed for single-document summarization. The system combines standard preprocessing methods and term weighting schemes—such as term frequency and inverse sentence frequency—with a sentence scoring strategy based on information gain, which factors in sentence position, normalized length, and numerical content. A term-sentence matrix is constructed using these weights, and reinforcement learning algorithms are applied to optimize sentence selection. The summary is iteratively refined through user feedback, allowing the system to adjust based on user-provided keywords to generate more contextually relevant and personalized summaries.

\secc Large language models

In recent years, large transformer-based language models have emerged as the dominant approach in the field of text summarization. This shift is largely attributed to advances in neural networks and deep learning, particularly with the introduction of the Transformer architecture\cite[transformer], which has revolutionized how machines process and summarize textual data. As noted in recent research, instruction-tuned models like GPT-3 are capable of producing abstractive summaries with quality comparable to those generated by human annotators \cite[prompt-sums, news-sum].

The prominence of transformer models began with BERT (Bidirectional Encoder Representations from Transformers)\cite[bert]. BERT contributed significantly to text summarization through its bidirectional encoding mechanism, which captures contextual information from both preceding and succeeding tokens. Pre-trained on massive textual corpora, BERT produces deeply contextualized word embeddings, enabling it to detect subtle linguistic cues and semantic dependencies. This results in a highly accurate extraction and condensation of information, particularly effective in summarizing structured content such as news articles.

GPT-3 (Generative Pre-trained Transformer)\cite[gpt-3] further exemplifies the capabilities of large-scale transformer models. By being trained on an extensive dataset, GPT-3 acquired a nuanced understanding of linguistic structure and semantics. This allows it to generate fluent, logically consistent summaries that retain the essence of complex or technical source texts, such as academic publications. The strength of GPT-3 lies in its ability to synthesize information while ensuring coherence and relevance, highlighting the transformative potential of generative pre-trained models in summarization tasks.

At present, the development of transformer-based models is accelerating rapidly. On platforms like HuggingFace, there is a clear trend of frequent releases of increasingly large and powerful instruction-tuned models by major organizations\cite[huggingfaceModels]. These models, characterized by their growing parameter counts and improved generative capabilities, hold considerable promise for advancing abstractive summarization through large language models.

\secc Multi-document Summarization

Multi-document summarization introduces new challenges. Compared to single-document summarization (SDS), multi-document summarization (MDS) involves the inclusion of multiple source texts, which may result in semantic conflicts between individual statements. Semantic unit duplication is another issue, leading to increased cognitive load and a reduction of usable context. However, such duplications can also serve as indicators of importance, based on the frequency of occurrence. Moreover, documents can complement each other, increasing the quantity of salient information and consequently extending the length of the summary. This introduces a difficult trade-off in determining which high-value information to omit. Unlike SDS, the field of MDS suffers from a lack of annotated datasets, and the evaluation metrics for MDS are computationally more demanding \cite[multi-survay].

Convolutional neural networks (CNNs) are utilized in MDS tasks.
Certain proposed approaches employ CNN architectures to model and abstract textual content for MDS purposes. These typically operate on simplified n-gram-based feature models. CNNs provide highly efficient training potential but lack the capability to model long-range textual dependencies. Methods such as \cite[MDS-text-clasification] and \cite[MDS-opt-sentence-mod] apply CNN classifiers to evaluate the importance of textual units in extractive MDS.

Recurrent neural networks (RNNs) are widely adopted in MDS methods.
Various RNN modifications demonstrate an ability to capture temporality and context. However, their performance deteriorates on longer sequences, and they face limitations in parallelizing both training and inference compared to more modern architectures. In RNNs, the processing unit receives not only the current token input but also the hidden state output from the previous unit. Specific approaches such as \cite[coavoux-etal-2019-unsupervised] and \cite[meansum] leverage the Long Short-Term Memory (LSTM) \cite[LSTM] variant of RNNs, while others like \cite[amplayo-lapata-2021-informative] use Bi-LSTM-based networks \cite[Bi-LSTM].

Graph neural networks (GNNs) offer a theoretically optimal model of text semantics\cite[multi-survay].
The components of natural language exhibit relationships that can be represented as edges within a general graph. GNNs are thus capable of explicitly modeling hierarchical and, in the context of MDS, cross-document dependencies. A drawback of these approaches lies in their high computational cost, particularly for large graph-based architectures. Techniques proposed in \cite[yasunaga-etal-2017-graph, antognini-faltings-2019-learning, MDS-scisummnet] utilize Graph Convolutional Networks to analyze pre-constructed dependency graphs, while \cite[MDS-hetero-GNN] integrates Graph Attention Networks to capture inter-document relationships.

Pointer-generator architecture\cite[pointer-generator] marked a turning point in MDS. These methods are based on an attention-driven sequence-to-sequence model augmented with a pointer mechanism. The pointer enables referencing vocabulary from the original text, whereas the generator can produce new lexical content. This results in a trade-off between preserving factual information and introducing novel expressions to maintain the language fluency, which can be explicitly managed. A coverage component is included to monitor already summarized parts and to prevent redundancy. Methods like \cite[fabbri-etal-2019-multi] and \cite[lebanoff-etal-2018-adapting] additionally implement Maximal Marginal Relevance (MMR) \cite[MMR] to score sentences based on importance and redundancy.

% TODO: add Deep Hybrid Models

Transformer architecture significantly advanced the field of MDS. Transformers\cite[transformer] leverage the self-attention mechanism to capture long-range semantic dependencies while also enabling efficient parallelization of training and inference. Their advantages include high performance and compatibility with unsupervised pre-training, as discussed in the Transformers chapter. However, these models are computationally expensive. Transformer-based models such as \cite[MDS-wiki] have been used to generate Wikipedia articles from source documents, while \cite[jin-etal-2020-multi] introduces multi-level granularity (word, sentence, document), with each level processed by a dedicated attention layer.

Hierarchical Transformers efficiently handle large document sets.
In an effort to scale MDS to larger datasets, \cite[MDS-dialog-sum] proposed a hierarchical Transformer structure. This approach first selects the top-K most informative paragraphs using a logistic regression model. Each paragraph is encoded using a local transformer, and inter-paragraph dependencies are modeled via a global transformer. The GraphSum model \cite[li-etal-2020-leveraging-graph] extends this by integrating a hierarchical dependency graph directly into the encoding phase. To reduce memory usage, \cite[pasunuru-etal-2021-efficiently] introduces a method that linearizes the graph representation of inter-document dependencies.

Pre-trained language models achieve state-of-the-art MDS performance.
As with SDS, the use of pre-trained transformer-based models has proven effective for MDS tasks \cite[multi-survay]. Pre-training on large corpora helps to overcome the scarcity of annotated MDS data and significantly enhances generalization capabilities \cite[multi-survay]. Nevertheless, pre-trained models—especially large language models (LLMs)—require substantial resources and may hallucinate facts not grounded in the source documents. Models like BERT \cite[bert] and GPT-2 \cite[gpt-2] have been successfully employed for MDS. For instance, \cite[MDS-topic-centric] utilizes these models for abstract summarization of scientific papers.

Several studies propose adaptations to the pre-training phase.
Article \cite[PEGASUS] introduces the PEGASUS encoder-decoder model, which uses the Gap Sentences Generation technique. During pre-training, the model predicts entire masked sentences instead of individual words or tokens, which better reflects summarization generalization. In \cite[wu-etal-2021-controllable], the authors present the PRIMER model, which uses an Entity Pyramid masking strategy. The model identifies the most critical sentences using Named Entity Recognition. From the extracted entities, a frequency-based pyramid is constructed across all documents. For each entity, the sentence with the highest ROUGE overlap is selected, removed from the candidate pool, and added to the list of important sentences for masking. This pre-training strategy enabled the model to achieve state-of-the-art performance on datasets such as Multi-News \cite[multi-news-dataset] and arXiv \cite[arxiv-dataset] in 2021.

\secc Summary metrics

Quality is evaluated using various metrics. All presented metrics rely on the availability of a reference summary. Due to the nature of the task, these metrics are analogous to those used in machine translation evaluation, where system outputs are compared against one or more reference translations. Different metrics capture different dimensions of summarization quality.

{\bf ROUGE (Recall-Oriented Understudy for Gisting Evaluation)} \cite[prompt-sums, survayArxiv]

ROUGE is a set of evaluation metrics that assess lexical overlap between the generated and reference summaries.

{\bf ROUGE-N}

Measures the overlap of $n$-grams between a generated summary $S$ and reference summary $R$.

Precision:

$$
P_n = {{\sum_{gram_n \in S \cap R} \hbox{Count}_{S}(gram_n)}\over{\sum_{gram_n \in S} \hbox{Count}_{S}(gram_n)}}
$$

Recall:
$$
R_n = {{\sum_{gram_n \in S \cap R} \hbox{Count}_{S}(gram_n)}\over{\sum_{gram_n \in R} \hbox{Count}_{R}(gram_n)}}
$$

F1 Score:
$$
F1_n = {{2 \cdot P_n \cdot R_n}\over{P_n + R_n}}
$$

{\bf ROUGE-L}

Uses the length of the Longest Common Subsequence (LCS) between the candidate and reference summaries.

Precision:
$$
P_L = {{LCS(S, R)}\over{\hbox{length}(S)}}
$$

Recall:
$$
R_L = {{LCS(S, R)}\over{\hbox{length}(R)}}
$$

F1 Score:
$$
F1_L = {{(1 + \beta^2) \cdot P_L \cdot R_L}\over{R_L + \beta^2 \cdot P_L}}
$$
Typically, $\beta = 1$.

{\bf BLEU (Bilingual Evaluation Understudy)} \cite[prompt-sums, survayArxiv]

BLEU focuses on precision. It measures the extent to which $n$-grams in the generated summary match those in the reference summary. A brevity penalty accounts for differences in length between the generated and reference summaries.

$$
\hbox{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right) 
$$

Where:
\begitems
    * $p_n$ is the modified $n$-gram precision.
    * $w_n$ is the weight for $n$-gram order (often uniform).
    * $BP$ is the brevity penalty:
    $$
    BP = \cases{
        1                         & if $c > r$, \cr
        e^{1 - {r \over c}}       & if $c \leq r$.}
    $$ 
    where $c$ is the length of the candidate summary and $r$ is the length of the reference.
\enditems

{\bf BERTScore} \cite[bert-score]

BERTScore evaluates semantic similarity. It computes the cosine similarity between BERT embeddings of tokens in the candidate and reference summaries.

Let $x_i$ be token embeddings from the candidate summary $S$ and $y_j$ from the reference $R$.

Precision:
$$
P = {1\over|S|} \sum_{x_i \in S} \max_{y_j \in R} \hbox{cosine}(x_i, y_j)
$$

Recall:
$$
R = {1\over|R|} \sum_{y_j \in R} \max_{x_i \in S} \hbox{cosine}(x_i, y_j)
$$

F1 Score:
$$
F1 = {2PR \over P + R}
$$

where $cosine$ is cosine similarity function.


\sec Information extraction

\secc Named Entity Recognition (NER)
Important knowledge extraction sub-problem is {\it Named Entity Recognition} (NER). It deals with extracting mentions of entities falling into predefined semantic types such as person, location, organization. Many challenges are associated with this problem such as uniqueness of entities, duplicate mentions etc.\cite[NER]{} NER serves as an important pre-processing step for many nlp applications.

NER is a hot research topic. According to \cite[pwc-ner]{} the community provides several datasets and benchmarks, intended for solutions comparison. Among the current solutions, the following arise in particular. Automated Concatenation of Embeddings for Structured Prediction \cite[concat-embed]{} seeks the optimal way to represent entities numerically. Thus, it combines different kinds of embeddings, depending on the type of the current task. Another approach is models based on the BERT \cite[bert]{} architecture such as \cite[dice-loss]{}. It introduces a new loss function to compensate for the disproportion of easy and hard examples in the training dataset.


\secc Coreference Resolution

Coreference resolution addresses the task of entity deduplication.
For many information extraction applications, the ideal output is a list of actual mentioned entities. These {\it discourse entities} are typically referenced multiple times throughout a text, either directly (via proper names as identifiers) or indirectly through pronouns or descriptive phrases. Individual occurrences of such references are termed {\it mentions}. The real-world entity to which these mentions refer is known as the {\it referent}. Mentions referring to the same entity are said to {\it corefer}. These coreferences subsequently form {\it coreference chains} or {\it clusters}. Each chain or cluster corresponds to a real-world entity. The terminology presented here is adapted from \cite[stanfordNLP].

The reader constructs a so-called {\it discourse model} in their mind.
This model represents the current textual context. The individual entities within the discourse model correspond to real persons, institutions, or other named entities. When an entity is mentioned for the first time, it is said to be {\it evoked}. Each subsequent mention is referred to as {\it accessing} the entity. From a linguistic perspective, the reference to a previously mentioned entity is called an {\it anaphora}, and the form of the mention is termed an {\it anaphor}. The first occurrence of an entity in the text is designated as the {\it antecedent}. If an entity is mentioned only once within a text, it is called a {\it singleton}.

\medskip  \clabel[discourse-model]{Discourse model}
\picw=12cm \cinspic figs/discourse-model.png
\caption/f Individual pictograms correspond to specific real-world discourse entities. The words ``Victoria'' and ``she'' refer to the same entity, meaning they {\it corefer}. The mention ``Victoria'' appears first and thus is categorized as an {\it evocation}. The pronoun ``she'' appears later, merely {\it accessing} the already established discourse entity \cite[stanfordNLP].
\medskip

A related problem is {\it entity linking}.
Entity linking involves a one-to-one mapping between discourse entities and an ontology---a structured list of real-world entities. The key distinction lies in the fact that discourse modeling is concerned with local text coherence, whereas entity linking aims for global uniqueness according to the ontology. Wikipedia, the online encyclopedia, can serve as a commonly used ontology. For this specific subproblem, the term {\it wikification} has become established.

Another specific subproblem is {\it event coreference resolution}.
This task involves deduplicating extracted events. According to \cite[stanfordNLP], {\it ''Event mentions are much harder to detect than entity mentions, since they can be verbal as well as nominal.''}


Many methods utilize deep and recurrent neural networks.
This approach enables end-to-end learning for mention detection and coreference resolution, which are optimized jointly. Methods such as those proposed by \cite[clark-manning-2015-entity] and \cite[lee-etal-2018-higher] employ encoded embeddings of mentions for their comparison and disambiguation. \cite[wiseman-etal-2016-learning] propose encoding entire coreference clusters (i.e., all coreferring mentions) using trainable RNN embeddings.

Knowledge-based methods incorporate external knowledge.
These models have recently also been based on neural networks, but introduce external knowledge into the process in the form of ontologies such as Wikidata. Reward-based fine-tuning, as proposed by \cite[aralikatte-etal-2019-rewarding], uses reinforcement learning algorithms to fine-tune the neural network depending on how well the model’s output covers ground-truth entities. \cite[emami-etal-2018-generalized] conceptualize entity linking as a web search task, comparing references to the retrieved search results.

Pretrained transformer-based models are considered state of the art. {\it''The emergence of pre-trained transformer-based models has fundamentally changed coreference resolution.\cite[coref-survay]''} The pre-training process enables models to some extent to internalize syntactic and semantic properties of language. Thus, the models used are generally language models fine-tuned for the task of coreference resolution. \cite[joshi-etal-2019-bert] propose using BERT embeddings, where the text is divided into overlapping segments that are encoded, and similar references are retrieved based on them. \cite[wu-etal-2020-corefqa] reformulate the problem as a question-answering task, utilizing BIO (Beginning, Inside, Outside) tokens to symbolize the identified reference spans. This method provides the possibility of correcting erroneous reference detections during the linking process.

Cross-Document Language Modeling (CDLM) is capable of processing multiple documents.
\cite[caciularu-etal-2021-cdlm-cross] employ pre-training on a large labeled dataset for this purpose. The training algorithm randomly selects examples of references belonging to the same discourse entity. A binary probability of coreference between two tokens is obtained by analyzing the scalar product of the embeddings of two references using a feed-forward neural network (FFNN). Based on this principle, inference also operates during fine-tuning on datasets with known coreferences.


% \secc Relation Extraction

\secc Timeline Summarization
A view on temporal structure of the text provides Timeline Summarization. A multidocument summarization problem where the presence of atomic events is taken into account and they need to be concisely described and assigned to a timeline. Papers such as \cite[timeline-sum]{} and \cite[towards-timeline-sum]{} approach the problem in an abstract manner. They try to create a general description of the event by aggregating individual mentions. The MTS\cite[towards-timeline-sum]{} system achieves a value of 39.78 ROUGE-1 on the CNN/DailyMail dataset\cite[cnn-dailymail]{}. On the Papers With Code website \cite[paperswithcode-timeline-sum]{}, it is possible to follow the development of solutions in this problem category.

\secc Event Extraction
{\it Event extraction} is a relaxation of the timeline extraction task.  The methods extract structured information from often unstructured text. The system is responsible for detecting sections that are semantically and temporally related. Often questions such as who, when, where, what, why and how are answered.\cite[event-extraction]{} The problem is further divided into open-domain and close-domain. Open-domain understands an event as a set of related mentions over which clustering takes place. Close-domain is concerned with detecting events that correspond to a particular schema (e.g., a temporal event). 

Several solutions can be found in the literature. Based on a comparison of the quality \cite[pwc-event-extraction]{} of the output, the solutions DeepEventMine \cite[DeepEventMine]{} and DeepStruct \cite[DeepStruct]{} stand out from the recent ones. The older of the two, DeepEventMine, is based on the bioinformatics domain. Directed acyclic graphs are generated over the textual input, representing the sematic hierarchy. These hierarchies can be interleaved and further hierarchized in the text. The second method focuses on the possibility of general applicability to different types of event structures. The transferable problem is data event detection from twitter posts. A representative solution is for example \cite[twitter-events]{}. These solutions rely on the short document format; however, they also involve external knowledge in the detection process. 

\secc Clustering in NLP
Clustering represents a tool for unsupervised analysis. Analyzed events can be sorted into a number of clusters based on sematics. Clustering in NLP in an extensive topic. Clustering methods usually depend on the ability to compare text elements against each other. For this reason, various numerical representations of text and distance measures and similarity functions are introduced. Clustering methods are divided into: hierarchical, partitioning, density-based, model-based, grid-based and soft-computing categories. \cite[clustering]{} 

Hierarchical methods recursively partition or merge data points in a top-down or bottom-up fashion, creating a dendrogram that shows nested groupings based on similarity. Agglomerative (bottom-up) merges clusters, while divisive (top-down) splits a single cluster into smaller sub-clusters.

Partitioning methods divide the data into a set number of clusters $K$ by relocating points between clusters, optimizing an error criterion such as the Sum of Squared Errors (SSE). K-means\cite[clustering]{}  is a popular partitioning method, where each cluster is represented by its mean and the algorithm iteratively minimizes the SSE.

Density-based methods focus on discovering clusters by identifying regions of high data point density, using algorithms like DBSCAN, which can detect clusters of arbitrary shapes by evaluating local density maxima. These methods assume the data is drawn from a mixture of probability distributions, often using the maximum likelihood principle. 

Model-based methods aim to optimize the fit between the data and a mathematical model, such as Gaussian or Poisson distributions, to describe each cluster. Algorithms like Expectation-Maximization (EM)\cite[clustering]{}  and AUTOCLASS\cite[clustering]{}  use statistical models to assign data points to clusters based on probabilistic distributions.

Grid-based methods partition the data space into a grid structure and perform clustering operations within the cells, offering fast processing times. These methods are efficient for large datasets but can struggle with high-dimensional data.

Soft-computing methods incorporate approaches like fuzzy clustering and genetic algorithms, which deal with uncertainty and optimization in clustering. These methods, including fuzzy C-means and evolutionary techniques, allow for flexible, probabilistic assignments of data points to clusters, adapting to complex data patterns.

Just as important as the clustering algorithm is the method of extracting the numerical representation of the text elements, so called {\it embeddings}. Based on the benchmark\cite[clustering-benchmark]{} performed in 2023, the ST5-XXL\cite[ST5-XXL]{}\cite[hf-ST5-XXL]{} model appears to be the most promising, but the quality of the resulting clustering changes significantly with the change of the model depending on the actual task.


\sec Prompt engineering and instruction tuning
