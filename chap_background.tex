\chap Background and Related Work

This chapter reviews the theoretical foundations and recent advances relevant to multi-document structured summarization. It first introduces the principles of natural language processing and transformer-based language models that underpin current summarization systems. Special emphasis is placed on methods for summarizing multiple documents, extracting structured knowledge, and leveraging large language models. The discussion covers knowledge-extraction tasks such as temporal event relevance and entity recognition, which are central to structured summarization. Additionally, the chapter surveys prompt-based techniques for knowledge extraction, providing the necessary background for the analysis, pipeline design, and evaluation presented in subsequent chapters

\sec Natural Language Processing

Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on enabling machines to understand and generate human language. While human languages are built from a finite set of symbols, the possible combinations and meanings are vast and often context-dependent, making NLP a complex challenge.

Traditional NLP methods rely heavily on syntactic structures and statistical patterns to perform tasks like information retrieval, spell-checking, and word-level analysis. However, they struggle with deeper understanding, such as interpreting ambiguous sentences or referencing prior context—areas where humans naturally excel due to background knowledge and cognitive associations.~\cite[fundAI]{}

For example, machines find it difficult to resolve pronouns or understand phrases like “I saw the Golden Gate Bridge flying into San Francisco,” where context is essential. This limitation arises from the lack of high-level symbolic reasoning and semantic understanding in current systems.~\cite[fundAI]{}

To bridge this gap, modern NLP increasingly focuses on Natural Language Understanding (NLU), which involves cognitive-like processing such as identifying entities, understanding relationships, and representing abstract concepts.~\cite[fundAI]{} Although recent computational models aim to simulate human-like language comprehension, achieving true understanding remains an ongoing challenge.

Importantly, NLP has become an interdisciplinary subject, combining knowledge from linguistics, computer science, mathematics, psychology, philosophy, statistics, electronic engineering, and biology. The research and development of NLP not only require formalizing language in strict mathematical terms, designing algorithms, and implementing practical systems, but also benefit from psychological models of human language behavior, philosophical theories of meaning, statistical methods for pattern recognition, and advances in signal processing and neural networks.~\cite[Feng2023]

NLP plays a critical role in applications like machine translation, chatbots, and question answering, and continues to evolve toward more sophisticated and human-like language interaction.

\sec Transformer Architecture
Most modern NLP solutions are based on {\it transformers}. The Transformer architecture, introduced in the paper {\it Attention Is All You Need}~\cite[transformer]{}, revolutionized sequence transduction tasks by eliminating recurrence and convolutions, relying instead entirely on self-attention mechanisms. Its encoder-decoder structure leveraged multi-head self-attention layers combined with position-wise feed-forward networks, enhanced by residual connections and layer normalization. The self-attention mechanism efficiently modeled dependencies across all positions in a sequence, while positional encodings preserved order information. This design allowed the Transformer to process sequences with unparalleled parallelism, significantly reducing training time compared to recurrent or convolutional models.

\medskip  \clabel[transformer-architecture]{Transformer model architecture}
\picw=6cm \cinspic figs/transformer.png
\caption/f This figure illustrates the main components of the Transformer model, which consists of an encoder (left) and a decoder (right). The encoder processes input embeddings combined with positional encodings through multiple stacked layers, each containing a multi-head self-attention mechanism followed by a feed-forward network, with residual connections and layer normalization. The decoder receives output embeddings (shifted right) with positional encodings and processes them through masked multi-head self-attention, encoder-decoder attention, and a feed-forward network, also using residual connections and layer normalization. The decoder outputs are then passed through a linear layer and softmax to generate the final output probabilities.
\medskip
%TODO

The architecture introduced innovations such as scaled dot-product attention and multi-head attention, enabling the model to attend to multiple representation subspaces simultaneously. In the encoder, all positions could influence each other globally, while the decoder incorporated masked self-attention to ensure autoregressive sequence generation. These design choices allowed the Transformer to achieve state-of-the-art results in tasks like machine translation and demonstrate exceptional adaptability to other domains. Its focus on attention mechanisms and computational efficiency has since become a foundation for modern deep learning systems.


% \secc Constrained inference

\sec Text Summarization

People are in a state of information overload. With an increasing amount of information in the public space, a natural need arises to reduce the amount of this data overload. Textual summarization is a subfield of NLP that deals with the volumetric reduction of text to preserve the most important messages and merit of a matter~\cite[summarizationSurvey, summarizationSurvey2]. Important criteria for summarizing are properties such as conciseness, cohesion, and grammatical and factual correctness. The different methods then represent a trade-off between these qualities. Methods can be classified according to several factors~\cite[summarizationSurvey]: 
\begitems
* Input type:
    \begitems
    * {\it Single document}: The summary is created from one text source.
    * {\it Multi-document}: The summary is generated from several text sources.
    \enditems
* Aim:
    \begitems
    * {\it Generic}: The aim is to provide a general overview of the main points of the text.
    * {\it Domain-specific}: The summarization is adapted to a particular domain or topic and can utilize external knowledge from the relevant area.
    * {\it Query-based}: The summary is composed to answer a specific question or set of questions.
    \enditems
* Learning phase:
    \begitems
    * {\it Supervised}: Methods utilize annotated data for training, where both the inputs and the corresponding summaries are known.
    * {\it Unsupervised}: Methods do not use annotated data during learning and instead search for patterns and relationships directly within the data.
    \enditems
* Output type:
    \begitems
    * {\it Extractive}: The resulting summary consists of the direct selection of sentences or segments from the original text, with little linguistic modification.
    * {\it Abstractive}: The summary is formed by paraphrasing and generating new sentences, which may be linguistically different from the original text.
    \enditems
\enditems

Multi-document summarization introduces added complexity due to semantic relationships among the source documents. Techniques targeting this task must account for such inter-document connections. Domain-specific methods leverage additional knowledge, enabling them to generate outputs that are aligned with domain expectations or enriched by external knowledge. Query-based summarization, on the other hand, constructs summaries by answering pre-defined questions.

\secc Extractive Summarization

A primary classification in summarization is between extractive and abstractive methods. This distinction significantly influences the nature of the summarization output. Extractive methods generate summaries composed of original sentences or text segments directly extracted from the source text. The core mechanism of such approaches lies in evaluating the informativeness of textual fragments. Sentences with the highest scores are selected for inclusion in the final summary~\cite[summarizationIBM]. These summaries typically do not consider linguistic coherence, as they rely solely on sentence extraction. Most extractive approaches require preprocessing steps such as sentence boundary identification, stop-word elimination, and stemming.

Major extractive summarization methods can be categorized into several distinct approaches, each leveraging different computational and linguistic principles~\cite[summarizationSurvey]. {\it Frequency-based methods} construct a dictionary of key phrases and compute their relative frequencies within a document. Sentences containing high-frequency terms are selected for inclusion in the summary, capitalizing on the assumption that frequently mentioned concepts are central to the text.~\cite[summarizationSurvey]

{\it Position-based methods} assume that important information tends to appear in specific textual locations, such as the beginning or end of documents or paragraphs. These methods prioritize sentences found in such positions during the summary generation process.~\cite[summarizationSurvey]

{\it Graph-based methods} represent sentences as nodes in a graph, with edges indicating relationships between them. Sentence importance is assessed using graph-ranking algorithms such as PageRank. For example, PacSumm~\cite[pacsumm] (Position-Augmented Centrality based Summarization) uses graph algorithms for sentence mapping, and EdgeSumm~\cite[edgeSum] constructs a graph where nouns are nodes and non-nouns serve as edges, with node weights determined by frequency.

{\it Latent Semantic Analysis (LSA) methods} utilize algebraic and statistical techniques to uncover latent semantic relationships between words and sentences. These methods create a term-sentence matrix and apply singular value decomposition (SVD) to identify the most informative sentences.~\cite[summaryLSA]

{\it Clustering-based methods} group similar sentences into clusters and select representative sentences from each cluster for the summary. Techniques such as Fuzzy Evolutionary Optimization Modeling~\cite[fuzzySum] (FEOM) apply evolutionary algorithms to phrase clustering, while k-means clustering is also commonly used to cluster sentences based on similarity.

{\it Supervised classifier-based methods} train classifiers using labeled data consisting of original texts and their human-generated summaries. Sentences are represented using features such as word frequency, position, and length, and classifiers like Naive Bayes~\cite[bayessum] and Support Vector Machines (SVM)~\cite[supervisedsum] are employed to estimate sentence importance.

{\it Elementary Discourse Unit (EDU) deletion methods} iteratively eliminate less important EDUs from the text until a desired summary length is reached. The Weighted Compression Model, for example, assigns scores to EDUs and selects those with the highest weight-to-cost ratios.~\cite[eduSum]

{\it Hybrid methods} combine multiple extractive techniques to improve summary quality. Some approaches merge clustering, word graphs, and neural networks~\cite[hybrid1], while others integrate supervised and unsupervised learning paradigms, such as co-training with Probabilistic Support Vector Machines~\cite[hybrid2] (PSVM) and the Na\"{\i}ve Bayes classifier~\cite[bayessum].

{\it Transformer-based methods}, most notably those utilizing BERT~\cite[bert] (Bidirectional Encoder Representations from Transformers), represent a significant advancement in extractive summarization.~\cite[summarizationSurvey] BERT employs a bidirectional encoding mechanism that captures contextual information from both preceding and succeeding tokens. Pre-trained on massive textual corpora, BERT produces deeply contextualized word embeddings, enabling detection of subtle linguistic cues and semantic dependencies. This results in highly accurate extraction and condensation of information, especially effective for summarizing structured content such as news articles.

Recent state-of-the-art in extractive summarization employs Large Language Models (LLMs) with parameter-efficient fine-tuning methods. The EYEGLAXS~\cite[sota-extractive] framework, using LLAMA2-7B and ChatGLM2-6B models alongside LoRA and Flash Attention, surpasses prior transformer-based and lexical approaches on benchmarks like PubMed and arXiv, setting new performance standards for accuracy and efficiency in long-text extractive summarization.~\cite[sota-extractive]


\secc Abstractive Summarization

Abstractive summarization generates novel summaries by paraphrasing and synthesizing content. Unlike extractive methods that select and copy segments directly from the input, abstractive techniques produce condensed versions using new vocabulary, closely mimicking the human approach to summarization tasks~\cite[summarizationSurvey2]. As a result, these methods tend to be more complex, requiring language models to understand context and generalize in order to generate coherent and factually accurate summaries.

Semantic structure enhances abstractive summarization. The {\it Semantic Abstractive Summarization} (SAS) framework leverages Abstract Meaning Representation (AMR) graphs to encode the semantic relationships within the text~\cite[dohare]. In this approach, the process consists of graph extraction, summary graph construction, and surface text generation. Nodes in these graphs represent lexical units while edges capture semantic relations, including those revealed through co-reference resolution, allowing the system to identify and extract key information for summary generation.

Neural architectures form the foundation of most modern abstractive summarization models. Approaches such as the {\it MeanSum} model perform multi-document summarization by encoding each document into a latent representation and averaging these representations to produce a summary~\cite[meansum]. The resulting mean vector is decoded into natural language, providing a unified summary across related documents, such as collections of reviews for a single product.

Transformers have revolutionized abstractive summarization. The {\it Transformer} architecture, has fundamentally changed how machines process textual data by leveraging attention mechanisms to model long-range dependencies in text~\cite[transformer]. Models such as those described in~\cite[ted] utilize unsupervised pretraining and fine-tuning to ensure summary representations capture semantic similarity and difference, while also employing denoising autoencoder objectives to improve robustness and generalization.

Large-scale transformer-based models now dominate the field of text summarization.~\cite[summarizationSurvey] Instruction-tuned models like {\it GPT-3} demonstrated the ability to produce summaries that rival those written by human annotators~\cite[prompt-sums, news-sum]. {\it GPT-3}, trained on extensive corpora, achieved a nuanced understanding of linguistic structure and semantics, enabling it to generate fluent and logically consistent summaries even for complex or technical texts such as academic publications~\cite[gpt-3]. Its strength lies in synthesizing relevant information while maintaining coherence, underscoring the transformative potential of generative pre-trained models in abstractive summarization.

It is difficult to compare the latest models. This is because current objective scientific articles are still primarily focused on older models, likely due to the long peer-review cycle. Furthermore, many of the highly performant models are often private and therefore cannot be fully and objectively evaluated. The latest benchmark~\cite[salesforce] by researchers from Salesforce demonstrates the superior performance of the {\it GPT-4}~\cite[gpt-4] model. However, it is likely, even judging from empirical use of current models, that the performance boundaries of transformer-based generative summarization have advanced further.

The development of transformer-based language models continues to accelerate. Recent years have seen a proliferation of ever-larger and more capable instruction-tuned models released on platforms such as {\it HuggingFace}, as organizations strive to push the boundaries of generative language modeling~\cite[huggingfaceModels]. These advances are expected to further enhance the quality and utility of abstractive summarization, making large language models a central technology in the field.

% TODO: Moznost pridat dalsi

\secc Reinforcement Learning Approach

A promising avenue of research is reinforcement learning. These methods rely on the iterative refinement of existing approaches through the incorporation of user feedback. One of the proposed methods~\cite[autoRL]  presents an extractive single-document summarization approach grounded in reinforcement learning (RL) and enhanced by embedding-based features. The authors employ a Deep Q-Network (DQN)~\cite[DQN] to estimate Q-values for candidate sentences, guiding the selection of sentences that maximize the expected quality of the final summary. Each sentence is represented through two types of embeddings: a content vector (from averaged GloVe word embeddings) and a position vector capturing hierarchical structural information (sentence, paragraph, and section positions). The model architecture comprises three modules: one assessing semantic relevance between the document and candidate sentences, another capturing the contextual compatibility between candidate sentences and the partial summary using a GRU-based RNN, and a final module integrating these signals to compute Q-values via regression.

SAAR~\cite[SAAR] method also refines the outputs with user feedback. The system is designed for single-document summarization. The system combines standard preprocessing methods and term weighting schemes—such as term frequency and inverse sentence frequency—with a sentence scoring strategy based on information gain, which factors in sentence position, normalized length, and numerical content. A term-sentence matrix is constructed using these weights, and reinforcement learning algorithms are applied to optimize sentence selection. The summary is iteratively refined through user feedback, allowing the system to adjust based on user-provided keywords to generate more contextually relevant and personalized summaries.

In 2023, researchers developed a reinforcement learning method~\cite[RL3] that uses a textual entailment (natural language inference) model as a reward signal to improve the factual accuracy of abstractive summarization systems. By fine-tuning large summarization models with this entailment-based feedback, the approach significantly boosts the faithfulness, conciseness, and salience of generated summaries compared to traditional supervised and previous reinforcement learning methods. Both automatic metrics and human evaluations showed clear improvements, and the study found that balancing regularization and model size is important for maintaining informativeness while minimizing factual errors.


\secc Multi-document Summarization

Multi-document summarization introduces new challenges. Compared to single-document summarization (SDS), multi-document summarization (MDS) involves the inclusion of multiple source texts, which may result in semantic conflicts between individual statements. Semantic unit duplication is another issue, leading to increased cognitive load and a reduction of usable context. However, such duplications can also serve as indicators of importance, based on the frequency of occurrence. Moreover, documents can complement each other, increasing the quantity of salient information and consequently extending the length of the summary. This introduces a difficult trade-off in determining which high-value information to omit. Unlike SDS, the field of MDS suffers from a lack of annotated datasets, and the evaluation metrics for MDS are computationally more demanding~\cite[multi-survay].

Early neural approaches are now mostly considered obsolete.~\cite[multi-survay]
Earlier approaches to MDS include convolutional neural networks (CNNs), recurrent neural networks (RNNs), graph neural networks (GNNs), and pointer-generator models. While these architectures were instrumental in shaping the development of neural summarization, they are now largely considered obsolete due to the emergence of more effective models—most notably the transformer-based architectures. Nevertheless, these methods remain relevant as sources of inspiration and as important baselines for understanding the evolution of MDS systems.

Convolutional neural networks (CNNs) are utilized in MDS tasks.
Certain proposed approaches employ CNN architectures to model and abstract textual content for MDS purposes. These typically operate on simplified n-gram-based feature models. CNNs provide highly efficient training potential but lack the capability to model long-range textual dependencies. Methods such as~\cite[MDS-text-clasification] and~\cite[MDS-opt-sentence-mod] apply CNN classifiers to evaluate the importance of textual units in extractive MDS.

Recurrent neural networks (RNNs) are widely adopted in MDS methods.
Various RNN modifications demonstrate an ability to capture temporality and context. However, their performance deteriorates on longer sequences, and they face limitations in parallelizing both training and inference compared to more modern architectures. In RNNs, the processing unit receives not only the current token input but also the hidden state output from the previous unit. Specific approaches such as~\cite[coavoux-etal-2019-unsupervised] and~\cite[meansum] leverage the Long Short-Term Memory (LSTM)~\cite[LSTM] variant of RNNs, while others like~\cite[amplayo-lapata-2021-informative] use Bi-LSTM-based networks~\cite[Bi-LSTM].

Graph neural networks (GNNs) offer a theoretically optimal model of text semantics~\cite[multi-survay].
The components of natural language exhibit relationships that can be represented as edges within a general graph. GNNs are thus capable of explicitly modeling hierarchical and, in the context of MDS, cross-document dependencies. A drawback of these approaches lies in their high computational cost, particularly for large graph-based architectures. Techniques proposed in~\cite[yasunaga-etal-2017-graph, antognini-faltings-2019-learning, MDS-scisummnet] utilize Graph Convolutional Networks to analyze pre-constructed dependency graphs, while~\cite[MDS-hetero-GNN] integrates Graph Attention Networks to capture inter-document relationships.

Pointer-generator architecture~\cite[pointer-generator] marked a turning point in MDS. These methods are based on an attention-driven sequence-to-sequence model augmented with a pointer mechanism. The pointer enables referencing vocabulary from the original text, whereas the generator can produce new lexical content. This results in a trade-off between preserving factual information and introducing novel expressions to maintain the language fluency, which can be explicitly managed. A coverage component is included to monitor already summarized parts and to prevent redundancy. Methods like~\cite[fabbri-etal-2019-multi] and~\cite[lebanoff-etal-2018-adapting] additionally implement Maximal Marginal Relevance (MMR)~\cite[MMR] to score sentences based on importance and redundancy.

% TODO: add Deep Hybrid Models

Transformer architecture significantly advanced the field of MDS. Transformers~\cite[transformer] leverage the self-attention mechanism to capture long-range semantic dependencies while also enabling efficient parallelization of training and inference. Their advantages include high performance and compatibility with unsupervised pre-training, as discussed in the Transformers chapter. However, these models are computationally expensive. Transformer-based models such as~\cite[MDS-wiki] have been used to generate Wikipedia articles from source documents, while~\cite[jin-etal-2020-multi] introduces multi-level granularity (word, sentence, document), with each level processed by a dedicated attention layer.

Hierarchical Transformers efficiently handle large document sets.
In an effort to scale MDS to larger datasets,~\cite[MDS-dialog-sum] proposed a hierarchical Transformer structure. This approach first selects the top-K most informative paragraphs using a logistic regression model. Each paragraph is encoded using a local transformer, and inter-paragraph dependencies are modeled via a global transformer. The GraphSum model~\cite[li-etal-2020-leveraging-graph] extends this by integrating a hierarchical dependency graph directly into the encoding phase. To reduce memory usage,~\cite[pasunuru-etal-2021-efficiently] introduces a method that linearizes the graph representation of inter-document dependencies.

Pre-trained language models achieve state-of-the-art MDS performance.
As with SDS, the use of pre-trained transformer-based models has proven effective for MDS tasks~\cite[multi-survay]. Pre-training on large corpora helps to overcome the scarcity of annotated MDS data and significantly enhances generalization capabilities~\cite[multi-survay]. Nevertheless, pre-trained models—especially large language models (LLMs)—require substantial resources and may hallucinate facts not grounded in the source documents. Models like BERT~\cite[bert] and GPT-2~\cite[gpt-2] have been successfully employed for MDS. For instance,~\cite[MDS-topic-centric] utilizes these models for abstract summarization of scientific papers.

Several studies propose adaptations to the pre-training phase.
Article~\cite[PEGASUS] introduces the PEGASUS encoder-decoder model, which uses the Gap Sentences Generation technique. During pre-training, the model predicts entire masked sentences instead of individual words or tokens, which better reflects summarization generalization. In~\cite[wu-etal-2021-controllable], the authors present the PRIMER model, which uses an Entity Pyramid masking strategy. The model identifies the most critical sentences using Named Entity Recognition. From the extracted entities, a frequency-based pyramid is constructed across all documents. For each entity, the sentence with the highest ROUGE overlap is selected, removed from the candidate pool, and added to the list of important sentences for masking. This pre-training strategy enabled the model to achieve state-of-the-art performance~\cite[pwc-multi-doc] on datasets such as Multi-News~\cite[multi-news-dataset] and arXiv~\cite[arxiv-dataset].

\secc Timeline Summarization

Timeline Summarization offers insight into the temporal structure of the text. Timeline summarization is the task of generating a chronologically ordered sequence of summaries that concisely capture the major developments within a large, temporally annotated document collection. The aim is to distill the evolution of events or topics by selecting key moments and presenting their descriptions alongside explicit time labels. Unlike traditional multi-document summarization, timeline summarization must account for the temporal progression and dependencies between events. Each summary entry is anchored to a specific time point, typically a date, thus allowing users to understand not only what happened but also when it occurred.~\cite[2024.acl-long.390,2021.emnlp-main.519,D19-5403]

Results can be evaluated automatically. Evaluation of timeline summarization is centered around both content relevance and temporal alignment. Automatic evaluation typically employs ROUGE scores to measure n-gram overlap between system and reference summaries for each date. These metrics are extended in timeline summarization to incorporate temporal fidelity. Metrics such as Date-F1 assess the correctness of the selected dates, while alignment-based ROUGE penalizes mismatches between the temporal positions of generated and gold-standard summaries. Human evaluation is also used to judge coherence, informativeness, and temporal accuracy. This ensures the system output is not only factually correct but also readable and faithful to the temporal structure of the reference timeline.~\cite[2024.acl-long.390,2301.00867v1,D19-5403]

Early timeline summarization systems predominantly used extractive strategies. The extractive affinity propagation method groups sentences using similarity and temporal constraints. Its principle is to cluster sentences that likely describe the same event, using measures such as cosine similarity of TF-IDF vectors and date matching. Sentences are anchored to specific dates via explicit or implicit time expressions, and one representative sentence per cluster and date is selected for the timeline.~\cite[D19-5403] The submodular optimization method models timeline summarization as a set selection problem. It employs a submodular function to maximize content coverage, relevance, and diversity, all while ensuring temporal distribution across the timeline. This method systematically selects sentences to provide an optimal balance between event importance and redundancy for each date.~\cite[D19-5403]

Handling of event duration in these methods is generally simplistic: if a sentence or event refers to a time range, it may be associated with each possible date within the range for clustering purposes. However, only a single date—typically the start or a representative point—is ultimately chosen for the timeline. Consequently, most systems represent events as time points, not intervals, and do not extract or display both start and end times for individual events. Temporal granularity is therefore limited to the level of days or dates, and the duration of events is not explicitly modeled or displayed in the generated timeline~\cite[D19-5403,2021.emnlp-main.519].

Event graph-based approaches construct a graph where nodes are events and edges represent temporal or semantic relationships. The main principle is to extract events and their relations, normalize time expressions, and then compress the event graph to select a salient and temporally coherent subgraph. This subgraph forms the timeline, where each event is anchored to a single, representative date.~\cite[2021.emnlp-main.519] Memory-based neural models, such as MTS~\cite[0686], use neural networks to learn event representations and store them in a time-aware memory module. Their principle is to encode each event with both content and temporal features, then generate the summary in chronological order by querying the memory for salient events at each time point. Event duration is handled by representing each event as a time point, not as an interval.~\cite[0686,2301.00867v1]

Large language model (LLM) approaches leverage the semantic capabilities of pre-trained models for clustering, event detection, and summary generation. Their core principle is to use LLMs to assign sentences to event clusters, infer event salience, and generate abstractive summaries, often after explicit preprocessing to normalize time expressions. The output is a timeline of pointwise event summaries, each associated with a date.~\cite[2024.acl-long.390,2412.17408v1] This limitation persists even as event graph and neural approaches introduce more sophisticated content selection; ultimately, timelines are presented as sequences of dated, pointwise summaries, and event intervals are not part of the standard output. This approach reflects both the demands of available evaluation datasets and the design of mainstream TLS models, which focus on date-anchored event representation and comparison.~\cite[D19-5403,2021.emnlp-main.519,0686]


\sec Information Extraction

Information extraction (IE) is a fundamental area in natural language processing (NLP) focused on transforming unstructured textual data into structured representations. This chapter presents a comprehensive overview of key subproblems in information extraction, with an emphasis on techniques, challenges, and state-of-the-art approaches relevant to modern NLP applications.

The chapter first introduces Named Entity Recognition (NER), which addresses the detection and classification of entity mentions in text, forming the basis for structured information extraction. It then discusses Coreference Resolution, encompassing the deduplication of entity mentions and the linking of references to unique discourse or knowledge base entities. Event Extraction is covered as a method for structuring information about events by identifying triggers, arguments, and their semantic roles within predefined schemas. Finally, the chapter surveys Clustering in NLP, an unsupervised approach for grouping semantically related elements, highlighting various clustering paradigms used for event analysis and representation. Each subsection details current methodologies, evaluation protocols, and advances driven by machine learning and deep learning techniques.

\secc Named Entity Recognition (NER)

Important knowledge extraction sub-problem is {\it Named Entity Recognition} (NER). NER refers to the identification and classification of mentions of entities in text, where named entities are items referred to by a proper name such as people, locations, organizations, and geo-political entities. The task of NER is to detect spans of text corresponding to these proper names and assign them appropriate type labels, such as PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). In practice, the definition often extends to temporal expressions like dates and times, as well as numerical expressions such as monetary values.~\cite[stanfordNLP]{}

NER is a complex and challenging task. Many challenges are associated with this problem, including the uniqueness of entities, duplicate mentions, and type ambiguities, where a name like “Apple” may refer to a company, a fruit, or even a musical label depending on context.~\cite[stanfordNLP] NER serves as an important preprocessing step for many NLP applications by providing structured annotations, which support downstream tasks such as linking to knowledge bases, sentiment analysis, and anonymization for privacy protection.~\cite[NER]{}

NER systems are evaluated using entity-level metrics and specialized tests. Evaluation is based on the F1 measure at the entity level, rather than at the word level, which introduces difficulties due to the segmentation component of NER. Incorrect segmentation leads to errors such as partial recognition of entities. Furthermore, there is a mismatch between using entities as evaluation units and words as training units. Statistical significance of differences in F1 scores between systems is usually tested by paired bootstrap or randomization methods.~\cite[stanfordNLP]{}

NER methods have evolved significantly over time. Early approaches were based on rule-based systems and statistical models, followed by machine learning techniques such as sequence labeling with recurrent neural networks and convolutional neural networks.~\cite[NER] More recent methods leverage large-scale pre-trained language models and advanced architectures that incorporate external knowledge and context, continually pushing the boundaries of accuracy and generalization.~\cite[NER]

Recently, new methods utilize large language models for NER by reformulating the task from sequence labeling to text generation. These approaches are particularly effective in low-resource and few-shot learning scenarios, narrowing the gap with supervised models and improving adaptability in practical applications.~\cite[NER]

NER is a hot research topic with continuous advancements. According to~\cite[pwc-ner]{}, the community provides several datasets and benchmarks intended for solutions comparison. The following solutions arise among the present solutions. Automated Concatenation of Embeddings for Structured Prediction~\cite[concat-embed]{} seeks the optimal way to represent entities numerically. Thus, it combines different kinds of embeddings, depending on the type of the current task. Another approach is models based on the BERT~\cite[bert]{} architecture such as~\cite[dice-loss]{}. It introduces a new loss function to compensate for the disproportion of easy and hard examples in the training dataset.

\secc Coreference Resolution

Coreference resolution addresses the task of entity deduplication.
For many information extraction applications, the ideal output is a list of actual mentioned entities. These {\it discourse entities} are typically referenced multiple times throughout a text, either directly (via proper names as identifiers) or indirectly through pronouns or descriptive phrases. Individual occurrences of such references are termed {\it mentions}. The real-world entity to which these mentions refer is known as the {\it referent}. Mentions referring to the same entity are said to {\it corefer}. These coreferences subsequently form {\it coreference chains} or {\it clusters}. Each chain or cluster corresponds to a real-world entity. The terminology presented here is adapted from~\cite[stanfordNLP].

The reader constructs a so-called {\it discourse model} in their mind.
This model represents the current textual context. The individual entities within the discourse model correspond to real persons, institutions, or other named entities. When an entity is mentioned for the first time, it is said to be {\it evoked}. Each subsequent mention is referred to as {\it accessing} the entity. From a linguistic perspective, the reference to a previously mentioned entity is called an {\it anaphora}, and the form of the mention is termed an {\it anaphor}. The first occurrence of an entity in the text is designated as the {\it antecedent}. If an entity is mentioned only once within a text, it is called a {\it singleton}.

\medskip  \clabel[discourse-model]{Discourse model}
\picw=12cm \cinspic figs/discourse-model.png
\caption/f Individual pictograms correspond to specific real-world discourse entities. The words ``Victoria'' and ``she'' refer to the same entity, meaning they {\it corefer}. The mention ``Victoria'' appears first and thus is categorized as an {\it evocation}. The pronoun ``she'' appears later, merely {\it accessing} the already established discourse entity~\cite[stanfordNLP].
\medskip

A related problem is {\it entity linking}.
Entity linking involves a one-to-one mapping between discourse entities and an ontology---a structured list of real-world entities. The key distinction lies in the fact that discourse modeling is concerned with local text coherence, whereas entity linking aims for global uniqueness according to the ontology. Wikipedia, the online encyclopedia, can serve as a commonly used ontology. For this specific subproblem, the term {\it wikification} has become established.

Another specific subproblem is {\it event coreference resolution}.
This task involves deduplicating extracted events. According to~\cite[stanfordNLP], {\it ''Event mentions are much harder to detect than entity mentions, since they can be verbal as well as nominal.''}


Many methods utilize deep and recurrent neural networks.
This approach enables end-to-end learning for mention detection and coreference resolution, which are optimized jointly. Methods such as those proposed by~\cite[clark-manning-2015-entity] and~\cite[lee-etal-2018-higher] employ encoded embeddings of mentions for their comparison and disambiguation.~\cite[wiseman-etal-2016-learning] propose encoding entire coreference clusters (i.e., all coreferring mentions) using trainable RNN embeddings.

Knowledge-based methods incorporate external knowledge.
These models have recently also been based on neural networks, but introduce external knowledge into the process in the form of ontologies such as Wikidata. Reward-based fine-tuning, as proposed by~\cite[aralikatte-etal-2019-rewarding], uses reinforcement learning algorithms to fine-tune the neural network depending on how well the model’s output covers ground-truth entities.~\cite[emami-etal-2018-generalized] conceptualize entity linking as a web search task, comparing references to the retrieved search results.

Pretrained transformer-based models are considered state of the art. {\it''The emergence of pre-trained transformer-based models has fundamentally changed coreference resolution.''}~\cite[coref-survay] The pre-training process enables models to some extent to internalize syntactic and semantic properties of language. Thus, the models used are generally language models fine-tuned for the task of coreference resolution.~\cite[joshi-etal-2019-bert] propose using BERT embeddings, where the text is divided into overlapping segments that are encoded, and similar references are retrieved based on them.~\cite[wu-etal-2020-corefqa] reformulate the problem as a question-answering task, utilizing BIO (Beginning, Inside, Outside) tokens to symbolize the identified reference spans. This method provides the possibility of correcting erroneous reference detections during the linking process.

Cross-Document Language Modeling (CDLM) is capable of processing multiple documents.
~\cite[caciularu-etal-2021-cdlm-cross] employ pre-training on a large labeled dataset for this purpose. The training algorithm randomly selects examples of references belonging to the same discourse entity. A binary probability of coreference between two tokens is obtained by analyzing the scalar product of the embeddings of two references using a feed-forward neural network (FFNN). Based on this principle, inference also operates during fine-tuning on datasets with known coreferences.


% \secc Relation Extraction

\secc Event Extraction
Event extraction (EE) structures event information from text. In its most common formulation, EE aims to identify event triggers, assign event types, and extract event arguments and their roles, according to a predefined schema.~\cite[event-extraction-survay] An event is described by specifying what happened, who participated, when and where it occurred, and how the participants are involved. The extraction process typically focuses on two subtasks: trigger extraction, which localizes and classifies event triggers, and argument extraction, which identifies the spans or entities in the text filling particular semantic roles within an event instance.~\cite[event-extraction-survay]

EE systems are evaluated using standardized benchmarks. Evaluation of event extraction is usually conducted over one of the standardized datasets such as ACE2005~\cite[ACE2005], which provides detailed annotations of event types, triggers, and arguments. The main evaluation metrics are F1 scores for trigger identification, trigger classification, argument identification, and argument classification, comparing predicted spans and roles to gold annotations using span matching or structured outputs.~\cite[ee-generative, EE-inst-tuning]

Traditional EE methods have evolved significantly. Early approaches relied on feature-based machine learning, pipelines where subtasks are solved sequentially, and joint models that predict triggers and arguments together to reduce error propagation. Sequence labeling and classification-based methods frame EE as a word-level tagging or multi-label classification problem. More recent strategies include machine reading comprehension (MRC)~\cite[event-extraction-survay], which casts EE as a question answering task, and sequence-to-structure generation with encoder–decoder models for direct structured output.~\cite[event-extraction-survay]

Generative models unify EE as a generation problem. In generative EE, encoder– decoder models generate structured event representations—such as JSON or S-expressions—with all required fields in a single output. These models use schema information and constrained decoding to ensure validity, enabling joint extraction and flexible adaptation to new event types through prompt engineering.~\cite[ee-generative]

Prompt optimization enhances EE by refining model instructions. This approach iteratively updates instructions and schema prompts based on model errors, using automated or LLM-based optimizers. The result is concise, targeted prompts that reduce errors and boost both trigger and argument extraction performance, especially for large reasoning models.~\cite[ee-prompt-opt]

According to~\cite[EE-inst-tuning], instruction-tuning improves EE through explicit guidance. By providing clear annotation guidelines in prompts, instruction-tuned models better distinguish fine-grained event types and roles, which is particularly beneficial in low-resource scenarios. Both human and machine-generated guidelines can be used, improving interpretability and generalization, with evaluation focused on structured outputs according to ACE2005 standards.~\cite[EE-inst-tuning]


\secc Clustering in NLP
Clustering represents a tool for unsupervised analysis. Analyzed events can be sorted into a number of clusters based on semantics. Clustering in NLP in an extensive topic. Clustering methods usually depend on the ability to compare text elements against each other. For this reason, various numerical representations of text and distance measures and similarity functions are introduced. Clustering methods are divided into: hierarchical, partitioning, density-based, model-based, grid-based and soft-computing categories.~\cite[clustering]{} 

Hierarchical methods recursively partition or merge data points in a top-down or bottom-up fashion, creating a dendrogram that shows nested groupings based on similarity. Agglomerative (bottom-up) merges clusters, while divisive (top-down) splits a single cluster into smaller sub-clusters.

Partitioning methods divide the data into a set number of clusters $K$ by relocating points between clusters, optimizing an error criterion such as the Sum of Squared Errors (SSE). K-means~\cite[clustering]{}  is a popular partitioning method, where each cluster is represented by its mean and the algorithm iteratively minimizes the SSE.

Density-based methods focus on discovering clusters by identifying regions of high data point density, using algorithms like DBSCAN~\cite[dbscan], which can detect clusters of arbitrary shapes by evaluating local density maxima. These methods assume the data is drawn from a mixture of probability distributions, often using the maximum likelihood principle. 

Model-based methods aim to optimize the fit between the data and a mathematical model, such as Gaussian or Poisson distributions, to describe each cluster. Algorithms like Expectation-Maximization (EM)~\cite[em]{}  and AutoClass~\cite[autoclass]{}  use statistical models to assign data points to clusters based on probabilistic distributions.

Grid-based methods~\cite[clustering]{}  partition the data space into a grid structure and perform clustering operations within the cells, offering fast processing times. These methods are efficient for large datasets but can struggle with high-dimensional data.

Soft-computing methods incorporate approaches like fuzzy clustering and genetic algorithms, which deal with uncertainty and optimization in clustering. These methods, including fuzzy C-means~\cite[c-means] and evolutionary techniques~\cite[clustering]{}, allow for flexible, probabilistic assignments of data points to clusters, adapting to complex data patterns.


\sec Prompt Engineering and Instruction Tuning

Prompt engineering is an emerging field of research. It is a discipline whose aim is to maximize the success of a generative model by systematically improving instruction prompts.~\cite[prompt-engineering] Prompt refers to an instruction that, in modern large language models, helps to guide subsequent text generation. Typically, a prompt consists of four parts. These are instructions, questions, input data, and examples.~\cite[prompt-engineering2] Most of these fields are optional; however, models usually expect at least one of the instruction or question fields. Input data and examples are then an integral part of certain prompt engineering techniques.

There is a wide range of prompt tuning techniques. Only the basic ones will be discussed here, some of which are part of the methodology of this thesis. The fundamental concept is the so-called {\it Zero-Shot Prompting}. This involves querying or instructing the model without any additional information. The user thus fully relies on the model’s knowledge, which has been acquired during the pre-training phase or through subsequent supervised learning.

{\it Few-shot prompting} inserts examples of the input and the expected output. This makes use of the “examples” section in the prompt template. “Providing even a few high-quality examples has improved model performance on complex tasks compared to no demonstration”~\cite[prompt-engineering]. However, examples limit the usable context of the model. At the same time, they introduce bias, which can manifest when the prompt is applied to domains other than those anticipated. Additionally, with some models, there is a risk of example data leaking into the resulting output.

The {\it Chain-of-Thought (CoT)}~\cite[chain-of-thought] technique mimics the way humans approach problems. The model is forced to generate a series of logical steps leading to the desired goal before producing the final output. As a result, the model is less susceptible to self-reinforcement, which may occur during greedy inference. The disadvantage is increased model latency. A “thinking” phase must be performed before generating the first tokens of the output sequence. Some language models inference setups are directly based on this principle.

{\it Retrieval Augmented Generation (RAG)}~\cite[RAG] dynamically adjusts the prompt based on the user’s query. The resulting prompt is enriched with examples that are dynamically retrieved from a knowledge base so that they relate to the original question or instruction. Embedding models trained for semantic similarity can be used for this purpose. The vectorized semantic message can then be compared with semantic fragments in the knowledge base.

\sec Metrics in NLP

Evaluation metrics are fundamental for assessing the performance of natural language processing systems. Many widely used metrics—especially in tasks such as summarization and machine translation—require a reference text for meaningful comparison. This section presents core automatic metrics applied across diverse NLP tasks, including information extraction, text generation, and classification. The first subsection introduces classical information retrieval metrics such as precision, recall, and F-measure. Subsequent subsections detail standard reference-based metrics for text generation, including ROUGE and BLEU, and discuss semantic evaluation with BERTScore. Finally, the section describes the emerging paradigm of using large language models as surrogate judges for qualitative evaluation.

\secc Information Retrieval

Information retrieval metrics are fundamental for evaluating classification models. This subsection introduces the concepts of precision, recall and F-measure, and explains their relevance to information extraction.

Precision is the ratio of correctly identified positive instances to all instances identified as positive. It evaluates the model’s ability to return only relevant results, thereby minimizing false positives. Formally, precision is defined as:

$$
\hbox{Precision} = {\hbox{TP} \over {\hbox{TP} + \hbox{FP}}}
$$

where $TP$ denotes true positives and $FP$ denotes false positives.~\cite[stanfordNLP]

Recall measures the ratio of correctly identified positive instances to all actual positive instances. It reflects the model’s ability to find all relevant instances, thus minimizing false negatives. Recall is given by:

$$
\hbox{Recall} = {\hbox{TP} \over {\hbox{TP} + \hbox{FN}}}
$$

where $TP$ denotes true positives and $FN$ denotes false negatives.~\cite[stanfordNLP]

The F$_\beta$ score~\cite[stanfordNLP] represents a generalized harmonic mean of precision and recall. This metric introduces the parameter $\beta$ to adjust the balance between recall and precision, enabling users to emphasize either recall ($\beta > 1$) or precision ($\beta < 1$). The general formula for the F$_\beta$ score is:

$$
F_{\beta} = (1 + \beta^2) \cdot {{\hbox{Precision} \cdot \hbox{Recall}} \over {(\beta^2 \cdot \hbox{Precision}) + \hbox{Recall}}}
$$

The F1 score~\cite[stanfordNLP] is a special case of the F$_\beta$ score, where $\beta = 1$. It provides an equal balance between precision and recall, making it suitable for scenarios where both metrics are equally important. The F1 score is defined as:

$$
F1 = 2 \cdot {{Precision \cdot Recall}\over{Precision + Recall}}
$$

These information retrieval metrics are widely used for the evaluation of information extraction systems, enabling a comprehensive assessment of their effectiveness in identifying relevant entities and events.

\secc ROUGE

ROUGE (Recall-Oriented Understudy for Gisting Evaluation)~\cite[prompt-sums, survayArxiv] is a family of metrics designed to measure the degree of lexical overlap between a system-generated summary and a human-written reference summary.

{\bf ROUGE-N}

ROUGE-N measures the overlap of $n$-grams between the generated summary $S$ and the reference summary $R$.

\begitems
* $gram_n$ denotes an $n$-gram, that is, a contiguous sequence of $n$ tokens.
* $S$ is the set of $n$-grams in the candidate (generated) summary.
* $R$ is the set of $n$-grams in the reference summary.
* $\hbox{Count}{S}(gram_n)$ is the count of $n$-gram $gram_n$ in $S$.
* $\hbox{Count}{R}(gram_n)$ is the count of $n$-gram $gram_n$ in $R$.
\enditems

Precision:

$$
P_n = {{\sum_{gram_n \in S \cap R} \hbox{Count}_{S}(gram_n)}\over{\sum_{gram_n \in S} \hbox{Count}_{S}(gram_n)}}
$$

Recall:
$$
R_n = {{\sum_{gram_n \in S \cap R} \hbox{Count}_{S}(gram_n)}\over{\sum_{gram_n \in R} \hbox{Count}_{R}(gram_n)}}
$$

F1 Score:
$$
F1_n = {{2 \cdot P_n \cdot R_n}\over{P_n + R_n}}
$$
{\bf ROUGE-L}

ROUGE-L measures the overlap based on the Longest Common Subsequence (LCS) between the candidate summary $S$ and the reference summary $R$.

\begitems
* $LCS(S, R)$ is the length of the longest common subsequence between $S$ and $R$.
* $\hbox{length}(S)$ is the total number of tokens in the generated summary $S$.
* $\hbox{length}(R)$ is the total number of tokens in the reference summary $R$.
* $\beta$ is a parameter weighting recall and precision (typically $\beta=1$).
\enditems

Precision:
$$
P_L = {{LCS(S, R)}\over{\hbox{length}(S)}}
$$

Recall:
$$
R_L = {{LCS(S, R)}\over{\hbox{length}(R)}}
$$

F1 Score:
$$
F1_L = {{(1 + \beta^2) \cdot P_L \cdot R_L}\over{R_L + \beta^2 \cdot P_L}}
$$

\secc BLEU 

BLEU (Bilingual Evaluation Understudy)~\cite[prompt-sums, survayArxiv] evaluates the precision of $n$-gram matches between a candidate summary and a reference, penalizing short outputs via a brevity penalty.

\begitems
* $p_n$ is the modified $n$-gram precision, i.e., the fraction of matched $n$-grams between candidate and reference.
* $w_n$ is the weight for $n$-gram order (typically $w_n$ is uniform).
* $N$ is the highest $n$-gram order considered (commonly $N=4$).
* $BP$ is the brevity penalty correcting for length difference.
* $c$ is the length (number of tokens) of the candidate summary.
* $r$ is the length of the reference summary.
\enditems

$$
\hbox{BLEU} = BP \cdot \exp \left( \sum_{n=1}^{N} w_n \log p_n \right) 
$$

The brevity penalty $BP$ is defined as:
$$
BP = \cases{
    1                         & if $c > r$, \cr
    e^{1 - {r \over c}}       & if $c \leq r$.}
$$ 

\secc BERTScore

BERTScore~\cite[bert-score] measures semantic similarity between candidate and reference summaries using contextual embeddings from a pre-trained BERT model.

\begitems
* $x_i$ denotes the embedding vector of the $i$-th token in the candidate summary $S$.
* $y_j$ denotes the embedding vector of the $j$-th token in the reference summary $R$.
* $|S|$ and $|R|$ are the numbers of tokens in $S$ and $R$, respectively.
* $\hbox{cosine}(x_i, y_j)$ is the cosine similarity between token embeddings $x_i$ and $y_j$.
\enditems

Precision:
$$
P = {1\over|S|} \sum_{x_i \in S} \max_{y_j \in R} \hbox{cosine}(x_i, y_j)
$$

Recall:
$$
R = {1\over|R|} \sum_{y_j \in R} \max_{x_i \in S} \hbox{cosine}(x_i, y_j)
$$

F1 Score:
$$
F1 = {2PR \over P + R}
$$

Here, each token in one summary is matched to its most similar token in the other summary according to cosine similarity, and then averaged.

\secc LLM as a Judge

LLM-as-a-judge is an emerging automated approach for evaluating natural language outputs. The central idea is to use large language models (LLMs) as surrogate judges to compare and assess system responses in open-ended tasks where traditional metrics like ROUGE or BLEU fall short. Instead of relying on lexical overlap with a reference summary, LLM judges evaluate responses according to helpfulness, relevance, accuracy, and other qualitative criteria, often aligning with human preference in complex conversational and instruction-following.~\cite[llm-as-a-judge]

LLM-based judging offers important advantages for NLP evaluation. The approach is highly scalable and cost-effective, enabling rapid benchmarking of multiple models without the slow and expensive process of recruiting human annotators. In addition, LLM judges can provide not only quantitative ratings but also natural language explanations for their decisions, making the evaluation process more interpretable and transparent. Experiments show that strong LLMs such as GPT-4~\cite[gpt-4] reach over 80\% agreement with human annotators on open-ended benchmarks like MT-bench and Chatbot Arena—comparable to inter-human agreement rates—thus establishing LLM-as-a-judge as a practical substitute for human evaluation in many cases.~\cite[llm-as-a-judge]

Despite these strengths, several limitations of LLM-as-a-judge have been identified.~\cite[llm-as-a-judge] LLM judges may exhibit position bias (favoring the first response), verbosity bias (preferring longer outputs), and in some cases self-enhancement bias (preferring outputs from the same model). Furthermore, judgments can be influenced by misleading context.