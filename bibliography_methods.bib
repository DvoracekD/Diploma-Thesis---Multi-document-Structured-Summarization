@Inbook{preprocessing,
author="Anandarajan, Murugan
and Hill, Chelsey
and Nolan, Thomas",
title="Text Preprocessing",
bookTitle="Practical Text Analytics: Maximizing the Value of Text Data",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="45--59",
abstract="This chapter starts the process of preparing text data for analysis. This chapter introduces the choices that can be made to cleanse text data, including tokenizing, standardizing and cleaning, removing stop words, and stemming. The chapter also covers advanced topics in text preprocessing, such as n-grams, part-of-speech tagging, and custom dictionaries. The text preprocessing decisions influence the text document representation created for analysis.",
isbn="978-3-319-95663-3",
doi="10.1007/978-3-319-95663-3_4",
url="https://doi.org/10.1007/978-3-319-95663-3_4"
}

@article{jaro-winkler,
author = {Winkler, William},
year = {1990},
month = {01},
pages = {},
title = {String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage},
journal = {Proceedings of the Section on Survey Research Methods}
}

@inproceedings{sentence-transformer,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@inproceedings{multilingual-sentence-bert,
  title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2020",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/2004.09813",
}

@misc{prompt-engineering,
      title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications}, 
      author={Pranab Sahoo and Ayush Kumar Singh and Sriparna Saha and Vinija Jain and Samrat Mondal and Aman Chadha},
      year={2025},
      eprint={2402.07927},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.07927}, 
}

@inproceedings{positional-bias,
    title = "Can We Instruct {LLM}s to Compensate for Position Bias?",
    author = "Zhang, Meiru  and
      Meng, Zaiqiao  and
      Collier, Nigel",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.732/",
    doi = "10.18653/v1/2024.findings-emnlp.732",
    pages = "12545--12556",
    abstract = "Position bias in large language models (LLMs) leads to difficulty in accessing information retrieved from the retriever, thus downgrading the effectiveness of Retrieval-Augmented Generation (RAG) approaches in open-question answering. Recent studies reveal that this bias is related to disproportional attention across the context. In this work, we examine how to direct LLMs to allocate more attention towards a selected segment of the context through prompting, aiming to compensate for the shortage of attention. We find that language models do not have relative position awareness of the context but can be directed by promoting instruction with an exact document index. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications. The code and data in our study have been made publicly available."
}

@misc{e-infra,
  title        = {Chat AI},
  author       = {e-INFRA CZ},
  year         = 2025,
  note         = {\url{https://docs.cerit.io/en/docs/web-apps/chat-ai} Accessed: (5-15-2025)}
}

@misc{qwen3,
    title  = {Qwen3},
    url    = {https://qwenlm.github.io/blog/qwen3/},
    author = {Qwen Team},
    month  = {April},
    year   = {2025}
}

@misc{llama4,
  title        = {meta-llama/Llama-4-Scout-17B-16E-Instruct},
  author       = {Meta Inc.},
  year         = 2025,
  note         = {\url{https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct} Accessed: 5-15-2025}
}

@misc{deepseek,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{gemini,
  title        = {https://deepmind.google/technologies/gemini/},
  author       = {Google Inc.},
  year         = 2025,
  note         = {\url{} Accessed: 5-15-2025}
}

@misc{ai-studio,
  title        = {Google AI Studio},
  author       = {Google Inc.},
  year         = 2025,
  note         = {\url{https://aistudio.google.com/} Accessed: 5-15-2025}
}

@misc{gpt4.1,
  title        = {Introducing GPT-4.1 in the API},
  author       = {OpenAI Inc.},
  year         = 2025,
  note         = {\url{https://openai.com/index/gpt-4-1/} Accessed: 5-15-2025}
}

@misc{openai-platform,
  title        = {OpenAI Platform},
  author       = {OpenAI Inc.},
  year         = 2025,
  note         = {\url{https://platform.openai.com/} Accessed: 5-15-2025}
}

@misc{litellm,
  title        = {BerriAI/litellm},
  author       = {BerriAI},
  year         = 2025,
  note         = {\url{https://github.com/BerriAI/litellm/} Accessed: 5-15-2025}
}





