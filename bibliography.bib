@book{Feng2023,
   author = {Zhiwei Feng},
   city = {Singapore},
   doi = {10.1007/978-981-16-5172-4},
   isbn = {978-981-16-5171-7},
   publisher = {Springer Nature Singapore},
   title = {Formal Analysis for Natural Language Processing: A Handbook},
   year = {2023}
}

@article{RNN,
   author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
   doi = {10.1038/323533a0},
   issn = {0028-0836},
   issue = {6088},
   journal = {Nature},
   month = {10},
   pages = {533-536},
   title = {Learning representations by back-propagating errors},
   volume = {323},
   year = {1986}
}
@article{multi-survay,
   abstract = {Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind, systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state of the art. We highlight the differences between various objective functions that are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting field.},
   author = {Congbo Ma and Wei Emma Zhang and Mingyu Guo and Hu Wang and Quan Z. Sheng},
   doi = {10.1145/3529754},
   issn = {15577341},
   issue = {5},
   journal = {ACM Computing Surveys},
   keywords = {Multi-document summarization,deep neural networks,machine learning},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Multi-document Summarization via Deep Learning Techniques: A Survey},
   volume = {55},
   year = {2022}
}

@article{Sheng2020,
	abstract = {Given the overwhelming amounts of information in our current 24/7 stream of new incoming articles, new techniques are needed to enable users to focus on just the key entities and concepts along with their relationships. Examples include news articles but also business reports and social media. The fact that relevant information may be distributed across diverse sources makes it particularly challenging to identify relevant connections. In this paper, we propose a system called MuReX to aid users in quickly discerning salient connections and facts from a set of related documents and viewing the resulting information as a graph-based visualization. Our approach involves open information extraction, followed by a careful transformation and filtering approach. We rely on integer linear programming to ensure that we retain only the most confident and compatible facts with regard to a user query, and finally apply a graph ranking approach to obtain a coherent graph that represents meaningful and salient relationships, which users may explore visually. Experimental results corroborate the effectiveness of our proposed approaches, and the local system we developed has been running for more than one year.},
	author = {Sheng, Yongpan and Xu, Zenglin and Wang, Yafang and de Melo, Gerard},
	date = {2020/05/01},
	date-added = {2025-05-23 19:36:12 +0200},
	date-modified = {2025-05-23 19:36:33 +0200},
	doi = {10.1007/s11280-020-00790-2},
	id = {Sheng2020},
	isbn = {1573-1413},
	journal = {World Wide Web},
	number = {3},
	pages = {2043--2077},
	read = {0},
	title = {Multi-document semantic relation extraction for news analytics},
	url = {https://doi.org/10.1007/s11280-020-00790-2},
	volume = {23},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s11280-020-00790-2}
}


@misc{pwc-multi-doc,
  title        = {Multi-Document Summarization | Papers With Code},
  author       = {Meta AI Research},
  year         = 2025,
  note         = {\url{https://paperswithcode.com/task/multi-document-summarization} Accessed: 5-15-2025}
}

@misc{paperswithcode-timeline-sum,
  title        = {Timeline Summarization | Papers With Code},
  author       = {Meta AI Research},
  year         = 2025,
  note         = {\url{https://paperswithcode.com/task/timeline-summarization} Accessed: 5-15-2025}
}
@inproceedings{cnn-dailymail,
   author = {Abigail See and Peter J. Liu and Christopher D. Manning},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/P17-1099},
   booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
   pages = {1073-1083},
   publisher = {Association for Computational Linguistics},
   title = {Get To The Point: Summarization with Pointer-Generator Networks},
   year = {2017}
}
@inproceedings{twitter-events,
   abstract = {Event Detection has been one of the research areas in Text Mining that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of hashtags and the small word limit of Twitter that ensures concise presentation of events. Previous works on event detection from tweets are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and hash-tag into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results.},
   author = {Keval Morabia and Neti Lalita Bhanu Murthy and Aruna Malapati and Surender Samant},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/N19-3011},
   booktitle = {Proceedings of the 2019 Conference of the North},
   keywords = {Event detection,Hashtag,Microblogging,Social Media,Text Mining,Tweet segmentation,Twitter,Wikipedia},
   pages = {77-85},
   publisher = {Association for Computational Linguistics},
   title = {SEDTWik: Segmentation-based Event Detection from Tweets using
Wikipedia},
   url = {http://aclweb.org/anthology/N19-3011},
   year = {2019}
}
@article{Trieu2020,
   abstract = {Motivation: Recent neural approaches on event extraction from text mainly focus on flat events in general domain, while there are less attempts to detect nested and overlapping events. These existing systems are built on given entities and they depend on external syntactic tools. Results: We propose an end-to-end neural nested event extraction model named DeepEventMine that extracts multiple overlapping directed acyclic graph structures from a raw sentence. On the top of the bidirectional encoder representations from transformers model, our model detects nested entities and triggers, roles, nested events and their modifications in an end-to-end manner without any syntactic tools. Our DeepEventMine model achieves the new state-of-the-art performance on seven biomedical nested event extraction tasks. Even when gold entities are unavailable, our model can detect events from raw text with promising performance. Availability and implementation: Our codes and models to reproduce the results are available at: https://github. com/aistairc/DeepEventMine.},
   author = {Hai Long Trieu and Thy Thy Tran and Khoa N.A. Duong and Anh Nguyen and Makoto Miwa and Sophia Ananiadou},
   doi = {10.1093/bioinformatics/btaa540},
   issn = {14602059},
   issue = {19},
   journal = {Bioinformatics},
   month = {10},
   pages = {4910-4917},
   pmid = {33141147},
   publisher = {Oxford University Press},
   title = {DeepEventMine: End-to-end neural nested event extraction from biomedical texts},
   volume = {36},
   year = {2020}
}
@inproceedings{DeepStruct,
   abstract = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models on a collection of task-agnostic corpora to generate structures from text. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition , relation classification, semantic role labeling , event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretrain-ing with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. 1},
   author = {Chenguang Wang and Xiao Liu and Zui Chen and Haoyun Hong and Jie Tang and Dawn Song},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2022.findings-acl.67},
   booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
   pages = {803-823},
   publisher = {Association for Computational Linguistics},
   title = {DeepStruct: Pretraining of Language Models for Structure Prediction},
   url = {https://aclanthology.org/2022.findings-acl.67},
   year = {2022}
}
@article{Li2024,
   abstract = {Event extraction (EE) is a crucial research task for promptly apprehending event information from massive textual data. With the rapid development of deep learning, EE based on deep learning technology has become a research hotspot. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This article fills the research gap by reviewing the state-of-the-art approaches, especially focusing on the general domain EE based on deep learning models. We introduce a new literature classification of current general domain EE research according to the task definition. Afterward, we summarize the paradigm and models of EE approaches, and then discuss each of them in detail. As an important aspect, we summarize the benchmarks that support tests of predictions and evaluation metrics. A comprehensive comparison among different approaches is also provided in this survey. Finally, we conclude by summarizing future research directions facing the research area.},
   author = {Qian Li and Jianxin Li and Jiawei Sheng and Shiyao Cui and Jia Wu and Yiming Hei and Hao Peng and Shu Guo and Lihong Wang and Amin Beheshti and Philip S. Yu},
   doi = {10.1109/TNNLS.2022.3213168},
   issn = {21622388},
   issue = {5},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,evaluation metrics,event extraction (EE),research trends},
   month = {5},
   pages = {6301-6321},
   pmid = {36269921},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey on Deep Learning Event Extraction: Approaches and Applications},
   volume = {35},
   year = {2024}
}
@misc{pwc-event-extraction,
  title        = {Event Extraction | Papers With Code},
  author       = {Meta AI Research},
  year         = 2025,
  note         = {\url{https://paperswithcode.com/task/event-extraction} Accessed: 5-15-2025}
}
@article{Li2022,
   author = {Jing Li and Aixin Sun and Jianglei Han and Chenliang Li},
   doi = {10.1109/TKDE.2020.2981314},
   issn = {1041-4347},
   issue = {1},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   month = {1},
   pages = {50-70},
   title = {A Survey on Deep Learning for Named Entity Recognition},
   volume = {34},
   year = {2022}
}
@misc{pwc-ner,
  title        = {Named Entity Recognition (NER) | Papers With Code},
  author       = {Meta AI Research},
  year         = 2025,
  note         = {\url{https://paperswithcode.com/task/named-entity-recognition-ner} Accessed: 5-15-2025}
}
@techReport{,
   abstract = {Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embed-dings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embed-dings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong base-lines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations. 1},
   author = {Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},
   pages = {2643-2660},
   title = {Automated Concatenation of Embeddings for Structured Prediction},
   url = {https://github.}
}
@inproceedings{dice-loss,
   abstract = {Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen-Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably , we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.},
   author = {Xiaoya Li and Xiaofei Sun and Yuxian Meng and Junjun Liang and Fei Wu and Jiwei Li},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2020.acl-main.45},
   booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   pages = {465-476},
   publisher = {Association for Computational Linguistics},
   title = {Dice Loss for Data-imbalanced NLP Tasks},
   url = {https://www.aclweb.org/anthology/2020.acl-main.45},
   year = {2020}
}
@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}
@Inbook{clustering,
author="Rokach, Lior
and Maimon, Oded",
editor="Maimon, Oded
and Rokach, Lior",
title="Clustering Methods",
bookTitle="Data Mining and Knowledge Discovery Handbook",
year="2005",
publisher="Springer US",
address="Boston, MA",
pages="321--352",
abstract="This chapter presents a tutorial overview of the main clustering methods used in Data Mining. The goal is to provide a self-contained review of the concepts and the mathematics underlying clustering techniques. The chapter begins by providing measures and criteria that are used for determining whether two objects are similar or dissimilar. Then the clustering methods are presented, divided into: hierarchical, partitioning, density-based, model-based, grid-based, and soft-computing methods. Following the methods, the challenges of performing clustering in large data sets are discussed. Finally, the chapter presents how to determine the number of clusters.",
isbn="978-0-387-25465-4",
doi="10.1007/0-387-25465-X_15",
url="https://doi.org/10.1007/0-387-25465-X_15"
}
@misc{clustering-benchmark,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}
@inproceedings{ST5-XXL,
    title = "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models",
    author = "Ni, Jianmo  and
      Hernandez Abrego, Gustavo  and
      Constant, Noah  and
      Ma, Ji  and
      Hall, Keith  and
      Cer, Daniel  and
      Yang, Yinfei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.146/",
    doi = "10.18653/v1/2022.findings-acl.146",
    pages = "1864--1874",
    abstract = "We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings."
}
@misc{hf-ST5-XX,
  title        = {sentence-transformers/sentence-t5-xxl | Hugging Face},
  author       = {Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},
  year         = 2025,
  note         = {\url{https://huggingface.co/sentence-transformers/sentence-t5-xxl} Accessed: 2-14-2025}
}
@inproceedings{black-box-prompt,
    title = "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training",
    author = "Cheng, Jiale  and
      Liu, Xiao  and
      Zheng, Kehan  and
      Ke, Pei  and
      Wang, Hongning  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Huang, Minlie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.176/",
    doi = "10.18653/v1/2024.acl-long.176",
    pages = "3201--3219",
    abstract = "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective{---}Black-Box Prompt Optimization (BPO){---}to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22{\%} increase in the win rate against its original version and 10{\%} for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO."
}

@misc{evolution-prompt,
      title={Large Language Models as Evolutionary Optimizers}, 
      author={Shengcai Liu and Caishun Chen and Xinghua Qu and Ke Tang and Yew-Soon Ong},
      year={2024},
      eprint={2310.19046},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2310.19046}, 
}
@misc{rci,
   author = {Research Center for Informatics},
   title = {RCI Cluster},
   url = {https://login.rci.cvut.cz/wiki/start},
   note = {\url{https://login.rci.cvut.cz/wiki/start} Accessed: 11-3-2024}
}
@misc{slurm,
  title        = {Slurm Workload Manager - Documentation},
  author       = {Slurm Project},
  year         = 2025,
  note         = {\url{https://slurm.schedmd.com/documentation.html} Accessed: 5-12-2024}
}

@inproceedings{transformer,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@misc{huggingface,
   author = {Inc. Hugging Face},
   title = {Hugging Face - The AI community building the future.},
   note = {\url{https://huggingface.co/} Accessed: 1-12-2025}
}
@misc{hf-transformers,
   author = {Inc. Hugging Face},
   title = {Transformers},
   note = {\url{https://huggingface.co/docs/transformers/index} Accessed: 3-14-2025}
}
@inproceedings{vllm,
   abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2 - 4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
   author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph Gonzalez and Hao Zhang and Ion Stoica},
   doi = {10.1145/3600006.3613165},
   isbn = {9798400702297},
   booktitle = {SOSP 2023 - Proceedings of the 29th ACM Symposium on Operating Systems Principles},
   month = {10},
   pages = {611-626},
   publisher = {Association for Computing Machinery, Inc},
   title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
   year = {2023}
}

@misc{vllm-github,
  title        = {vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs},
  author       = {VLLM Project},
  year         = 2025,
  note         = {\url{https://github.com/vllm-project/vllm} Accessed: 8-1-2025}
}

@misc{,
   author = {Jan Drchal},
   title = {Timelines...},
   url = {https://fcheck.fel.cvut.cz/static/timelines/becva/}
}
@Book{stanfordNLP,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released January 12, 2025",
  edition =         "3rd",
}
@book{fundAI,
   abstract = {Fundamentals of Artificial Intelligence introduces the foundations of present day AI and provides coverage to recent developments in AI such as Constraint Satisfaction Problems, Adversarial Search and Game Theory, Statistical Learning Theory, Automated Planning, Intelligent Agents, Information Retrieval, Natural Language \& Speech Processing, and Machine Vision. The book features a wealth of examples and illustrations, and practical approaches along with the theoretical concepts. It covers all major areas of AI in the domain of recent developments. The book is intended primarily for students who major in computer science at undergraduate and graduate level but will also be of interest as a foundation to researchers in the area of AI.},
   author = {K. R. Chowdhary},
   doi = {10.1007/978-81-322-3972-7},
   isbn = {9788132239727},
   journal = {Fundamentals of Artificial Intelligence},
   keywords = {First-order Predicate Logic,Knowledge Representation,Non-monotonic Reasoning,Prolog,State-space Search},
   month = {1},
   pages = {1-716},
   publisher = {Springer India},
   title = {Fundamentals of artificial intelligence},
   year = {2020}
}
@article{genIEsurvey,
   abstract = {<p>Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).</p>},
   author = {Derong Xu and Wei Chen and Wenjun Peng and Chao Zhang and Tong Xu and Xiangyu Zhao and Xian Wu and Yefeng Zheng and Yang Wang and Enhong Chen},
   doi = {10.1007/s11704-024-40555-y},
   issn = {2095-2228},
   issue = {6},
   journal = {Frontiers of Computer Science},
   month = {12},
   pages = {186357},
   title = {Large language models for generative information extraction: a survey},
   volume = {18},
   url = {https://link.springer.com/10.1007/s11704-024-40555-y},
   year = {2024}
}
@inproceedings{Wang2023,
    title = "{GPT}-{NER}: Named Entity Recognition via Large Language Models",
    author = "Wang, Shuhe  and
      Sun, Xiaofei  and
      Li, Xiaoya  and
      Ouyang, Rongbin  and
      Wu, Fei  and
      Zhang, Tianwei  and
      Li, Jiwei  and
      Wang, Guoyin  and
      Guo, Chen",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.239/",
    pages = "4257--4275",
    ISBN = "979-8-89176-195-7",
    abstract = "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text {\textquotedblleft}Columbus is a city{\textquotedblright} is transformed to generate the text sequence ''@@Columbus{\#}{\#} is a city{\textquotedblright}, where special tokens @@{\#}{\#} marks the entity to extract. To efficiently address the \textit{hallucination} issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited."
}
@inproceedings{summarizationSurvey,
   abstract = {The size of data on the Internet has risen in an exponential manner over the past decade. Thus, the need for a solution emerges, that transforms this vast raw information into useful information which a human brain can understand. One such common technique in research that helps in dealing with enormous data is text summarization. Automatic summarization is a renowned approach which is used to reduce a document to its main ideas. It operates by preserving substantial information by creating a shortened version of the text. Text Summarization is categorized into Extractive and Abstractive methods. Extractive methods of summarization minimize the burden of summarization by choosing from the actual text a subset of sentences that are relevant. Although there are a ton of methods, researchers specializing in Natural Language Processing (NLP) are particularly drawn to extractive methods. Based on linguistic and statistical characteristics, the implications of sentences are calculated. A study of extractive and abstract methods for summarizing texts has been made in this paper. This paper also analyses above mentioned methods which yields a less repetitive and a more concentrated summary.},
   author = {Ishitva Awasthi and Kuntal Gupta and Prabjot Singh Bhogal and Sahejpreet Singh Anand and Piyush Kumar Soni},
   doi = {10.1109/ICICT50816.2021.9358703},
   isbn = {9781728185019},
   booktitle = {Proceedings of the 6th International Conference on Inventive Computation Technologies, ICICT 2021},
   keywords = {Text summarization,abstractive,extractive,reinforcement learning,supervised,unsupervised},
   month = {1},
   pages = {1310-1317},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Natural Language Processing (NLP) based Text Summarization-A Survey},
   year = {2021}
}
@inproceedings{summarizationSurvey2,
   abstract = {NLP (Natural Language Processing) is a subfield of artificial intelligence that examines the interactions between computers and human languages, specifically how to design computers to process and evaluate vast quantities of natural language data. The procedure of condensing long text into paragraphs or phrases is known as NLP text summarization. This technique retrieves essential information from a text while keeping its meaning. This decreases the time necessary to comprehend large elements, such as articles, without compromising the integrity of the content. Major difficulties in text summarizing include subject identification, interpretation, summary construction, and summary evaluation. Most real-world systems that summarize texts rely on extractive summarization. Hence, there must be a way to summarize lengthy assessments into concise statements with few words that convey the same information. The use of text summarization in this context can be helpful. Text Summarization is of interest to several researchers in natural language processing. This study provides an overview of the different text-summarization approaches used in Natural language processing.},
   author = {G. Senthil Kumar and Midhun Chakkaravarthy},
   doi = {10.1007/978-3-031-36402-0_46},
   isbn = {9783031364013},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Extractive,Natural Language Processing,Text summarization,abstractive and reinforcement learning},
   pages = {496-502},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Survey on Recent Text Summarization Techniques},
   volume = {14078 LNAI},
   year = {2023}
}
@misc{summarizationIBM,
   author = {Jacob Murel Ph.D. and Eda Kavlakoglu},
   journal = {IBM.com},
   month = {5},
   title = {What is text summarization?},
   note = {\url{https://www.ibm.com/think/topics/text-summarization} Accessed: 11-5-2024},
   year = {2024}
}
@inproceedings{pacsumm,
   author = {Hao Zheng and Mirella Lapata},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/P19-1628},
   booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   pages = {6236-6247},
   publisher = {Association for Computational Linguistics},
   title = {Sentence Centrality Revisited for Unsupervised Summarization},
   year = {2019}
}
@inproceedings{bert,
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/N19-1423},
   booktitle = {Proceedings of the 2019 Conference of the North},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding},
   year = {2019}
}
@article{edgeSum,
   author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
   doi = {10.1016/j.ipm.2020.102264},
   issn = {03064573},
   issue = {6},
   journal = {Information Processing \& Management},
   month = {11},
   pages = {102264},
   title = {EdgeSumm: Graph-based framework for automatic text summarization},
   volume = {57},
   year = {2020}
}
@article{summaryLSA,
   abstract = {<p>Text summarization solves the problem of presenting the information needed by a user in a compact form. There are different approaches to creating well-formed summaries. One of the newest methods is the Latent Semantic Analysis (LSA). In this paper, different LSA-based summarization algorithms are explained, two of which are proposed by the authors of this paper. The algorithms are evaluated on Turkish and English documents, and their performances are compared using their ROUGE scores. One of our algorithms produces the best scores and both algorithms perform equally well on Turkish and English document sets.</p>},
   author = {Makbule Gulcin Ozsoy and Ferda Nur Alpaslan and Ilyas Cicekli},
   doi = {10.1177/0165551511408848},
   issn = {0165-5515},
   issue = {4},
   journal = {Journal of Information Science},
   month = {8},
   pages = {405-417},
   title = {Text summarization using Latent Semantic Analysis},
   volume = {37},
   year = {2011}
}
@article{fuzzySum,
   author = {Wei Song and Lim Cheon Choi and Soon Cheol Park and Xiao Feng Ding},
   doi = {10.1016/j.eswa.2010.12.102},
   issn = {09574174},
   issue = {8},
   journal = {Expert Systems with Applications},
   month = {8},
   pages = {9112-9121},
   title = {Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization},
   volume = {38},
   year = {2011}
}
@inproceedings{supervisedsum,
    title = "Extractive Summarization Using Supervised and Semi-Supervised Learning",
    author = "Wong, Kam-Fai  and
      Wu, Mingli  and
      Li, Wenjie",
    editor = "Scott, Donia  and
      Uszkoreit, Hans",
    booktitle = "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)",
    month = aug,
    year = "2008",
    address = "Manchester, UK",
    publisher = "Coling 2008 Organizing Committee",
    url = "https://aclanthology.org/C08-1124/",
    pages = "985--992"
}
@article{bayessum,
   abstract = {<p>Text classification categorizes web documents in large collections into predefined classes based on their contents. Unfortunately, the classification process can be time-consuming and users are still required to spend considerable amount of time scanning through the classified web documents to identify the ones with contents that satisfy their information needs. In solving this problem, we first introduce CorSum, an extractive single-document summarization approach, which is simple and effective in performing the summarization task, since it only relies on word similarity to generate high-quality summaries. We further enhance CorSum by considering the significance factor of sentences in documents, in addition to using word-correlation factors, for document summarization. We denote the enhanced approach CorSum-SF and use the summaries generated by CorSum-SF to train a Multinomial Naïve Bayes classifier for categorizing web document summaries into predefined classes. Experimental results on the DUC-2002 and 20 Newsgroups datasets show that CorSum-SF outperforms other extractive summarization methods, and classification time (accuracy, respectively) is significantly reduced (compatible, respectively) using CorSum-SF generated summaries compared with using the entire documents. More importantly, browsing summaries, instead of entire documents, which are assigned to predefined categories, facilitates the information search process on the Web.</p>},
   author = {Maria Soleda Pera and Yiu-Kai Ng},
   doi = {10.1142/S0218213010000285},
   issn = {0218-2130},
   issue = {04},
   journal = {International Journal on Artificial Intelligence Tools},
   month = {8},
   pages = {465-486},
   title = {A na\"{\i}ve bayes classifier for web document summaries created by using word similarity and significant factors},
   volume = {19},
   year = {2010}
}
@article{eduSum,
   author = {Natalia Vanetik and Marina Litvak and Elena Churkin and Mark Last},
   doi = {10.1016/j.ins.2019.08.079},
   issn = {00200255},
   journal = {Information Sciences},
   month = {1},
   pages = {22-35},
   title = {An unsupervised constrained optimization approach to compressive summarization},
   volume = {509},
   year = {2020}
}
@inproceedings{Bhagchandani2019,
   author = {Gaurav Bhagchandani and Deep Bodra and Abhishek Gangan and Nikahat Mulla},
   doi = {10.1109/ICCS45141.2019.9065724},
   isbn = {978-1-5386-8113-8},
   booktitle = {2019 International Conference on Intelligent Computing and Control Systems (ICCS)},
   month = {5},
   pages = {566-570},
   publisher = {IEEE},
   title = {A Hybrid Solution To Abstractive Multi-Document Summarization Using Supervised and Unsupervised Learning},
   year = {2019}
}
@inproceedings{dohare,
   author = {Shibhansh Dohare and Vivek Gupta and Harish Karnick},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/P18-3011},
   booktitle = {Proceedings of ACL 2018, Student Research Workshop},
   pages = {74-83},
   publisher = {Association for Computational Linguistics},
   title = {Unsupervised Semantic Abstractive Summarization},
   url = {http://aclweb.org/anthology/P18-3011},
   year = {2018}
}
@misc{meansum,
      title={MeanSum: A Neural Model for Unsupervised Multi-document Abstractive Summarization}, 
      author={Eric Chu and Peter J. Liu},
      year={2019},
      eprint={1810.05739},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.05739}, 
}
@inproceedings{seq3,
   author = {Christos Baziotis and Ion Androutsopoulos and Ioannis Konstas and Alexandros Potamianos},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/N19-1071},
   booktitle = {Proceedings of the 2019 Conference of the North},
   pages = {673-681},
   publisher = {Association for Computational Linguistics},
   title = {SEQˆ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression},
   year = {2019}
}
@inproceedings{ted,
   author = {Ziyi Yang and Chenguang Zhu and Robert Gmyr and Michael Zeng and Xuedong Huang and Eric Darve},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/v1/2020.findings-emnlp.168},
   booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
   pages = {1865-1874},
   publisher = {Association for Computational Linguistics},
   title = {TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising},
   year = {2020}
}
@inproceedings{autoRL,
   abstract = {An automatic text summarization system can automatically generate a short and brief summary that contains a main concept of an original document. In this work, we explore the advantages of simple embedding features in Reinforcement leaning approach to automatic text summarization tasks. In addition, we propose a novel deep learning network for estimating Q-values used in Reinforcement learning. We evaluate our model by using ROUGE scores with DUC 2001, 2002, Wikipedia, ACL-ARC data. Evaluation results show that our model is competitive with the previous models.},
   author = {Gyoung Ho Lee and Kong Joo Lee},
   booktitle = {International Journal of Artificial Intelligence Tools},
   pages = {193-197},
   title = {Automatic Text Summarization Using Reinforcement Learning with Embedding Features},
   year = {2010}
}
@article{DQN,
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
   doi = {10.1038/nature14236},
   issn = {0028-0836},
   issue = {7540},
   journal = {Nature},
   month = {2},
   pages = {529-533},
   title = {Human-level control through deep reinforcement learning},
   volume = {518},
   year = {2015}
}
@inproceedings{SAAR,
   author = {Chandra Prakash and Anupam Shukla},
   doi = {10.1109/ISCMI.2014.22},
   isbn = {978-1-4673-6751-6},
   booktitle = {2014 International Conference on Soft Computing and Machine Intelligence},
   month = {9},
   pages = {83-87},
   publisher = {IEEE},
   title = {Human Aided Text Summarizer "SAAR" Using Reinforcement Learning},
   year = {2014}
}
@inproceedings{hybrid1,
   author = {Gaurav Bhagchandani and Deep Bodra and Abhishek Gangan and Nikahat Mulla},
   doi = {10.1109/ICCS45141.2019.9065724},
   isbn = {978-1-5386-8113-8},
   booktitle = {2019 International Conference on Intelligent Computing and Control Systems (ICCS)},
   month = {5},
   pages = {566-570},
   publisher = {IEEE},
   title = {A Hybrid Solution To Abstractive Multi-Document Summarization Using Supervised and Unsupervised Learning},
   year = {2019}
}
@inproceedings{hybrid2,
    title = "Extractive Summarization Using Supervised and Semi-Supervised Learning",
    author = "Wong, Kam-Fai  and
      Wu, Mingli  and
      Li, Wenjie",
    editor = "Scott, Donia  and
      Uszkoreit, Hans",
    booktitle = "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)",
    month = aug,
    year = "2008",
    address = "Manchester, UK",
    publisher = "Coling 2008 Organizing Committee",
    url = "https://aclanthology.org/C08-1124/",
    pages = "985--992"
}