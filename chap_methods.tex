\chap Methodology

The chapter describes the methodology for extracting structured data from media articles. First, the target data schema and its individual components, including entities, events, and their mutual relations, are defined. This is followed by a detailed description of the extraction pipeline, including the data preprocessing phase and the multi-level extraction of entities and events. Next, the construction of the evaluation dataset is introduced, along with the procedure for both manual and automated annotation. The chapter also presents the design of an evaluation metric to measure the quality of the output structure against the annotated data. The final part is devoted to the experimental methodology proposed to asses the performance of the proposed solution, including an overview of the models used and their configurations.

\sec Extraction data structure

The principle of the problem is structure extraction. It is a matter of extracting persons, organizations, and locations from mutually connected articles related to a single media case or topic. The individual entities further contain associated information. In the case of persons, it is the roles they occupy across the examined set of articles. In the case of organizations and locations, we try to extract their type for further potential categorization and filtering. Furthermore, it is necessary to extract the objective events that the articles describe and their temporal validity, which can be derived both from the text of the article from which the event is extracted and from all available information across the articles.

At the core of the first part of the extraction process is named entity recognition (NER), which is extended beyond standard entity detection by also extracting associated roles for persons and types for organizations and locations. This enriched NER approach enables more detailed categorization and filtering of entities within the dataset.

A fundamental part of the resulting structure are mutual relations. Therefore, each event additionally contains a list of relevant entities (actors). At the same time, the relation between the entities, events, and individual articles is recorded. It is necessary to capture in which articles the extracted events appear and which reference them. Furthermore, it is necessary to capture a list of persons, organizations, and locations that appear in the given article.

The list of entities and events is shared for the whole case. It is therefore necessary to identify duplicates across the dataset—entities and events that are referenced from multiple articles within the case. Individual entities and events are thus atomic and unique units. The mentioned relations between them and the articles are then recorded using references (usage of unique identifiers, keys). Each entity (person, organization) and event has a unique private key in the dataset. Events then consist, apart from their description, of foreign keys of relevant entities. The list of events consists of foreign keys of contained entities and also of events from which the given article is composed.

Another component of the dataset is also a list of event summaries. These are clusters of events that serve for easier orientation within the case. These higher-level events contain, apart from the identifier, also, just like atomic events, a textual description and temporal validity given by a starting and ending timestamp. Descriptions of these event summaries are usually longer than those of atomic events, which tend to be shorter precisely due to their atomicity. The descriptions of event summaries must capture greater variance among the subordinated atomic events. Furthermore, these summaries contain lists of foreign keys of the events from which they are composed.

The structure of the ground truth dataset for individual cases is as follows:

\begitems
* "people" (array of objects): This array serves as a comprehensive catalog of all unique individuals mentioned in the dataset.
    \begitems
  * "id" (integer): A unique identifier assigned to each individual.
  * "name" (string): The full name of the individual.
  * "roles" (array of strings): A list detailing the various roles or professions held by the individual.
    \enditems
    
* "organizations" (array of objects): This array lists all distinct organizations identified in the data.

    \begitems
  * "id" (integer): A unique identifier for an organization.
  * "name" (string): The full name of the organization.
  * "types" (array of strings): A list specifying the categories or functions of the organization.
    \enditems
    
* "locations" (array of objects): This array contains all unique geographical or political locations referenced.

    \begitems
  * "id" (integer): A unique identifier for a location.
  * "name" (string): The name of the location.
  * "types" (array of strings): A list describing the nature or significance of the location (e.g., city, country, region).
    \enditems

* "events" (array of objects): This array details specific occurrences or actions.

    \begitems
  * "id" (integer): A unique identifier for a specific event.
  * "text" (string): A descriptive sentence or phrase detailing the event.
  * "people" (array of integers): A list of IDs referencing individuals involved in this event from the people array.
  * "organizations" (array of integers): A list of IDs referencing organizations associated with this event from the organizations array.
  * "locations" (array of integers): A list of IDs referencing locations pertinent to this event from the locations array.
  * "time_start" (string): The commencement time or date of the event, formatted as "yyyy", "yyyy-mm", "yyyy-mm-dd", or "NA" if unknown.
  * "time_end" (string): The conclusion time or date of the event, formatted as "yyyy", "yyyy-mm", "yyyy-mm-dd", or "NA" if unknown.
    \enditems
    
* "articles" (array of objects): This object functions as a cross-reference between source articles and the entities and events they contain.

    \begitems
  * "id" (integer, optional): An identifier for the article.
  * "people" (array of integers): A list of IDs referencing individuals mentioned within this article.
  * "organizations" (array of integers): A list of IDs referencing organizations mentioned within this article.
  * "locations" (array of integers): A list of IDs referencing locations mentioned within this article.
  * "events" (array of integers): A list of IDs referencing events discussed within this article.
    \enditems
* "event_summaries" (array of objects): This array provides a higher level of abstraction by grouping related events into broader summaries.

    \begitems
  * "id" (integer): A unique identifier for an event summary.
  * "text" (string): A concise narrative summarizing a group of related events.
  * "sub_events" (array of integers): A list of IDs referencing the specific events included in this summary from the events array.
  * "time_start" (string): The commencement time or date of the period covered by the summary, formatted as "yyyy", "yyyy-mm", "yyyy-mm-dd", or "NA" if unknown.
  * "time_end" (string): The conclusion time or date of the period covered by the summary, formatted as "yyyy", "yyyy-mm", "yyyy-mm-dd", or "NA" if unknown.
    \enditems
\enditems

Example of the resulting structure is shown in the following json snippet:

\begtt
{
    "people": [
        {"id": 1, 
        "name": "David Rath", 
        "roles": ["bývalý hejtman", "bývalý poslanec ČSSD"]}
    ],
    "organizations": [
        {"id": 1, "name": "ČSSD", "types": ["politická strana"]}
    ],
    "locations": [
        {"id": 1, "name": "Litoměřice", "types": ["město", "obec"]}
    ],
    "events": [
        {
            "id": 1,
            "text": "Exhejtman David Rath získal v prvním kole senátních 
                     voleb v Litoměřicích necelých pět procent hlasů.",
            "people": [1], "organizations": [], "locations": [1],
            "time_start": "2018-10-07", "time_end": "2018-10-07"
        }
    ],
    "articles": [
        {
            "id": 7634574,
            "people": [1], "organizations": [1], "locations": [1], 
            "events": [1]
        }
    ],
    "event_summaries": [
        {
            "id": 1,
            "text": "Proběhlo první kolo senátních voleb.",
            "sub_events": [1],
            "time_start": "2018-10-07", "time_end": "2018-10-07"
        }
    ]
}
\endtt


\sec Pipeline

The extraction pipeline produces the desired structure. It consists of a series of components that sequentially analyze articles related to a single media case and transform textual data into structured lists of persons, organizations, locations, and events. The first step is preprocessing, during which disruptive elements in the continuous text are removed. This is followed by extraction, which consists of several steps that incrementally structure the unstructured input.

\label[preprocessing]
\secc Preprocessing

Data should be preprocessed before analysis.
Before conducting any natural language analysis, it is advisable to apply certain filters and transformations to the data that facilitate later machine processing and reduce potential biases arising from the syntax of the original language used\cite[preprocessing]. Traditional natural language preprocessing typically includes techniques such as lemmatization and stemming, which are employed and discussed in greater detail in the chapter \ref[dataset-analysis]. However, these techniques substantially alter the form of the text and could distort the outputs of the large language models.

Dataset-specific preprocessing methods were applied.
The applied preprocessing methods are tailored to the specific characteristics of the dataset. Individual newspaper articles contain extraneous blocks intended to attract readers’ attention but which interrupt the text’s coherence. One such phenomenon involves image captions, which are included in the dataset in the following format:
\begtt
{title} | Foto: {author} | Zdroj: {source} \n\n
\endtt

Articles contain hyperlinks to related content.
Another disruptive feature consists of embedded hyperlinks to related articles. These links follow a typical syntax:
\begtt
{title} \n\nČíst článek \n\n
\endtt

\medskip  \clabel[embedded-article]{Embedded article}
\picw=12cm \cinspic figs/embeded-article.png
\caption/f An example of such an inserted hyperlink in the article might appear as: "Zápas Česko - Slovensko jsme sledovali ONLINE \n\nČíst článek \n\n".
\medskip

Tweets are also embedded in the text.
A further form of distraction comes from embedded tweets. Authors often include tweet panels as quotations or multimedia supplements. The structure of such tweet references is more complex:
\begtt
{tweet content} pic.twitter.com/{url}\n\n
- {tweet author} (@{author’s slug}) {date} \n\n
\endtt

Disruptive blocks were removed using regular expressions.
To maximize textual coherence, such disruptive elements were eliminated during preprocessing by detecting regular expressions matching the identified structural patterns. Additionally, redundant new-line characters, those that did not contribute semantic information, were removed.

Meaningful components were merged into a single string named "ftext".
To simplify further processing, the individual article components were concatenated into a single textual string. The final string takes the following form:
\begtt
{date}
{title}
{abstract}
{text}
\endtt
The original date, formatted as "YYYY-MM-DDTHH:MM:SS", is converted to the Czech date format to prevent ambiguity between days and months and to align with the language of the text. Article titles and abstracts are included as they may contain supplementary semantic information.

\secc Extraction 
The main component of the pipeline is extraction. After data preprocessing, the extraction of the desired structure follows. This process consists of the sequential extraction of persons, organizations, locations, and entities. These stages of the pipeline are separated to allow each type of entity or event to be targeted with an individual, specialized prompt. At the same time, this approach supports the scalability of the solution, which—thanks to its decomposition into subproblems—is less dependent on the context length of the current model and on the variable quality depending on the context length, including phenomena such as positional bias, which become more prominent with longer context in many models. \cite[positional-bias]

An extraction script was created. Following experimentation in a Python notebook, the script "pipeline.py" was developed. Its main method is "predict_structure", which takes the parameters "articles" and "create_completion". The "articles" parameter is a list of articles produced by preprocessing. Each object must contain the required attributes "id" and "ftext", which consists of the formatted text of the article, according to the preprocessing structure described in \ref[preprocessing]. The "create_completion" parameter is a function that accepts two arguments: the resulting prompt string with substituted variables, and a dictionary representing the JSON definition of the output schema. The output of "create_completion" is a populated dictionary containing the extracted data in accordance with the provided JSON schema. Since the function is passed as a parameter, the prediction script is completely independent of the settings of the employed model and can therefore be used generically across all experiments.

The prediction function calls subsequent parts of the pipeline. A separate function is prepared for extracting each type of entity and event structure. The prediction function first extracts structures for people, organizations, and locations. Subsequently, a new structure is assembled as the input parameter for event extraction. The resulting dictionary has the following schema:

\begitems
* "people": global list of extracted people

* "organizations": global list of extracted organizations

* "locations": global list of extracted locations

* "articles": list of processed articles

\begitems

* "id": article identifier

* "text": preprocessed text of the article, including date, headline, abstract and text

* "people": local list of people contained in the article

* "organizations": local list of organizations contained in the article

* "locations": local list of locations contained in the article
\enditems
\enditems

The resulting script is independent of the number of processed articles.
Articles can be processed either individually or in batches of a specified size. The principle of extraction for individual entities and events is as follows:

\begitems \style I
* Initialize "entities" = [] and "articles" = [].
* As long as there are unprocessed articles:
    \begitems
    * A batch of unprocessed articles of the given size is created.
    * Inference is performed, with the current list of extracted entities inserted into the prompt.
    * The new list of entities is merged with the "entities" list.
    * Relations between the article and the persons are added to the "articles" list.
    \enditems
\enditems

The model is instructed to reuse entities from the global list. If it is necessary to modify the list of roles/types of an entity, a new entity is returned, but with the original ID. If a new entity is discovered, it is returned with a unique ID relative to both the global and newly created entity list.

Entity lists are merged as follows.
The final list contains the original entities. Only if an entity with the same ID also exists in the new list is the old entity replaced with the updated version. The remaining entities from the new list that have no counterpart in the old list are appended to the end of the final list.

\label[pipeline-versions]
\sec Versions

Three configurations of the extraction pipeline emerged. The extraction pipeline is parameterized by the model used, batch size (the number of articles processed simultaneously), and instructional prompts. During development, three distinct configurations of the pipeline arose, differing specifically in batch size or the form of prompts. Individual versions of the extraction module are referenced in the following text according to the nomenclature established here.

\begitems
* {\bf V1}: Baseline implementation of instructional prompts and the maximum batch size, where all articles in the dataset are processed at once.

* {\bf V2}: The next phase aimed to maximize the observed metrics through prompt engineering. The second version retains the maximum batch size but applies improved prompts.

* {\bf V3}: Improved prompts from V2 are used in this configuration, while the batch size is reduced to one. Only a single article is processed at any given moment.
\enditems

\label[data-schema]
\sec Evaluation Dataset 

It was necessary to construct an evaluation dataset. In order to evaluate the quality of the implemented solution and at the same time obtain an objective metric for iterating over the individual principal decisions of the data extraction and summarization process, it was necessary to construct an evaluation dataset. Given that the entire provided original dataset is not the subject of the final analysis, it is possible to use the unused articles to create ground truth data.

The dataset consists of five cases. The cases were selected to cover the widest possible range of different types of articles. The extracted topic hierarchy was used for this purpose. The sections of different main sections were randomly selected. The final selection includes the following cases/topics. A table of counts of unique entities within the dataset is given.

\begitems
* {\bf Český lev Film Award}
\begitems
  * Section: Culture
  * The set of articles focuses on contemporary Czech film production, dominated by the topic of the Český lev film awards, director and actor portraits, analysis of selected films, and reflections on social themes in film. The texts combine historical and cultural memory, film aesthetics, and critical responses.
  * The dataset consists of five randomly selected articles out of a total of 62, from the period between 2017 and 2022.\smallskip
  * \table{ccccc}{
    \crl
    \hfil People & \hfil Organizations & Locations & Events & Event summaries \crl \tskip4pt
    45 & 10 & 6 & 74 & 8 \crl
    }
\enditems
* {\bf Senate Elections 2018}
\begitems
  * Section: Domestic News
  * The articles address topics related to the Senate elections of the Czech Republic held in 2018. The articles include pre-election campaign coverage, forecasts, election proceedings with real-time commentary, and post-election analyses. A minority of the texts also analyze the 2018 municipal elections, which were held alongside the first round of Senate elections.
  * The dataset consists of five randomly selected articles out of 113, from the period between August 3, 2018, and November 11, 2018.\smallskip
  * \table{ccccc}{
    \crl
    \hfil People & \hfil Organizations & Locations & Events & Event summaries \crl \tskip4pt
    42 & 20 & 19 & 83 & 6 \crl
    }
\enditems
* {\bf Olympic Games Beijing 2022}
\begitems
  * Section: Sports
  * The articles analyze the preparations of the Czech Olympic Committee and individual athletes for the 2022 Winter Olympics in Beijing. They also review the athletes’ performance during the games. A substantial portion discusses anti-COVID measures and other complications tied to the global pandemic, which was still a major issue at the time.
  * The dataset consists of five randomly selected articles out of a total of 109, from the period between July 2018 and March 2022, with most articles falling in February 2022, when the Games were held.\smallskip
  * \table{ccccc}{
    \crl
    \hfil People & \hfil Organizations & Locations & Events & Event summaries \crl \tskip4pt
    21 & 11 & 18 & 51 & 14 \crl
    }
\enditems
* {\bf COVID-19}
\begitems
  * Section: Society
  * The articles reflect on the multifaceted impacts of the COVID-19 pandemic on society, healthcare, education, and interpersonal relations, emphasizing both crisis moments and expressions of solidarity and adaptation. The texts also reveal deeper social issues, such as the spread of misinformation, loss of trust in the state, and public polarization.
  * The dataset consists of five selected articles out of a total of 72, from the period between March 17, 2020, and April 17, 2022.\smallskip
  * \table{ccccc}{
    \crl
    \hfil People & \hfil Organizations & Locations & Events & Event summaries \crl \tskip4pt
    12 & 24 & 26 & 66 & 10 \crl
    }
\enditems
* {\bf Trump-Kim Summit}
\begitems
  * Section: World News
  * The articles map diplomatic efforts toward denuclearization of the Korean Peninsula through summits between Donald Trump and Kim Jong-un, including preparations, proceedings, and reactions. The texts reflect the tension between the effort for a peaceful resolution and repeated failures or escalations, while both the DPRK and USA used the meetings to strengthen their international image.
  * The dataset consists of five randomly selected articles out of a total of 87, from the period between April 28, 2018, and September 24, 2019.\smallskip
  * \table{ccccc}{
    \crl
    \hfil People & \hfil Organizations & Locations & Events & Event summaries \crl \tskip4pt
    17 & 19 & 15 & 61 & 6 \crl
    }
\enditems
\enditems

A script was created to access the dataset. The script "eval.eval_dataset.py" contains definitions of the names of media cases and paths to the source and ground truth data within the repository. The script also includes preprocessing of the data. The return values are a list of case names, a list of source articles with the properties "id" and "ftext", and also a list of ground-truth structures.

The data was manually annotated. For easier orientation in the annotated data, entities and events were first manually detected for individual articles. To assist in subsequently linking entities across articles, it is necessary to detect which ones correspond to each other. The relevant identifiers must then be remapped. For this purpose, a helper script was created.

\secc Deduplication script

This section introduces a supporting script designed to facilitate the deduplication and linking of entities and events within the dataset. The script assists human annotators by automatically identifying and mapping corresponding entities and events across articles, streamlining the process of constructing a consistent and accurate relational structure. After the script's application, a human annotator reviews and refines the resulting structure, ensuring that all links and references accurately reflect the underlying data and correcting any errors, omissions, or inconsistencies as needed.

The script is based on semantic and approximate string matching. Individual entity names are compared with the ground truth entities step by step. For name comparison, Jaro-Winkler similarity is used\cite[jaro-winkler]. Only entities whose names match sufficiently—i.e., their Jaro-Winkler similarity exceeds a given threshold—are considered corresponding. If no entity with sufficient similarity is found, the entity is added to the global entity list with a new free identifier. The mapping between local and global identifiers is recorded.

Events are matched semantically. Descriptions of atomic events are converted into semantic embeddings using the SentenceTransformers library\cite[sentence-transformer], employing the embedding model "paraphrase-multilingual-MiniLM-L12-v2"\cite[multilingual-sentence-bert]. Events are considered identical when the cosine similarity of their textual representation exceeds a configurable threshold. If no corresponding event is found in the global list of events, the tested event is added with a new identifier. The mapping between identifiers is again recorded. At the same time, the foreign keys of entities associated with the event are remapped according to the previously inferred entity mapping. Finally, a relational object for articles is constructed using the identifier mapping.

The algorithm operates on a corpus of discrete articles, $\cal A$, to generate a canonicalized and consolidated knowledge graph. This graph comprises distinct sets of entities and events, effectively resolving inter-article redundancy.

Global Variables and Constants:
\begitems
* {\cal P}: Canonical register for Persons.
* {\cal O}: Canonical register for Organizations.
* {\cal L}: Canonical register for Locations.
* {\cal V}: Canonical register for Events.
* $\tau_E$: Threshold for entity fuzzy string similarity.
* $\tau_V$: Threshold for event semantic similarity.
\enditems

\begitems \style I
* Initialization Phase
    \begitems
    * Initialize canonical registers ({\cal P}, {\cal O}, {\cal L}, {\cal V}) with entities and events from the first article, $A_1$, assigning unique IDs to events.
    * Establish a mapping structure, ${\cal M}_i$, for each article $A_i$, and initialize a deduplicated representation, $\hat{A}_i$, for each article.
    \enditems
* Entity Deduplication (Iterative Processing for $A_i$ where $i > 1$)
    For each subsequent article $A_i$, entities (persons, organizations, locations) are deduplicated against their respective global canonical registers.
    \begitems
    * Similarity Assessment: Employ fuzzy string similarity (e.g., Jaro-Winkler) for name matching between entities from $A_i$ and existing canonical entries. A match is determined if the similarity exceeds $\tau_E$.
    * Consolidation:
        \begitems
        * If a match is found, the article's entity ID is mapped to the canonical entity's ID, and any new descriptive attributes (roles, types) are merged into the canonical record.
        * If no match, the article's entity is added as a new canonical entry with a unique ID, and its original ID is mapped to this new canonical ID.
        \enditems
    \enditems
* Event Deduplication (Iterative Processing for $A_i$ where $i > 1$)
    For each event in article $A_i$:
    \begitems
    * Semantic Embedding: Convert event textual descriptions into vector embeddings using a sentence transformer model.
    * Cosine Similarity Matching: Compute cosine similarity between the event's embedding and all existing canonical event embeddings. Identify the best match with score $s_{max}$.
    * Consolidation:
        \begitems
        * If $s_{max} \ge \tau_V$, the article's event ID is mapped to the canonical event's ID.
        * If $s_{max} < \tau_V$, the article's event is added as a new canonical event with a unique ID, its original ID is mapped, and its associated entity IDs are re-mapped to their canonical forms.
        \enditems
    \enditems
* Final Output
    The algorithm outputs a consolidated knowledge graph comprising the final, deduplicated sets of canonical entities ({\cal P}, {\cal O}, {\cal L}) and events ({\cal V}), along with canonicalized article representations ($\hat{A}_i$) linking to these global elements.
\enditems

\sec Evaluation
Evaluation is conducted through a custom evaluation pipeline.
An evaluation pipeline was developed to assess the quality of the proposed solution. The purpose of the evaluation process is to compare the similarity of the extracted structure to the ground truth data using evaluation metrics. Given the uncommon nature of the problem, the thesis proposes a custom evaluation process to quantify similarity. The data structures being compared are composed as described in Chapter \ref[data-schema].

Entity correctness is assessed using information retrieval metrics.
Correct inference of entities (persons, organizations, and events) is reduced to an information retrieval problem. Therefore, standard metrics such as precision and recall can be applied. All entity types include additional information — roles in the case of persons, and types for other entities. The atomic element is defined as a pair consisting of the entity name and its role. If a predicted pair is successfully matched with a ground truth name-role pair, it is considered a true positive (TP). If the pair cannot be matched with any ground truth pair, it is classified as a false positive (FP). Ground truth pairs for which no prediction exists are counted as false negatives (FN).

Entity matching is performed using fuzzy string comparison.
Fuzzy matching is employed for pair matching, as in the auxiliary deduplication script, using approximate (fuzzy) string matching to determine correspondence between pairs. A minimal change in entity names is assumed, and even word order changes are considered undesirable. Likewise, role or type names must match literally, though variation in affixes (prefixes and suffixes) is permitted. Deterministic similarity metrics based on character-level or edit distance comparison are used. The Jaro-Winkler similarity\cite[jaro-winkler] method was selected for the final approach, due to its significantly higher comparison speed compared to current semantically-based methods built on large neural models. Whether a predicted pair and a ground truth pair correspond is decided by thresholding the final similarity. The thresholds were empirically set to 0.9 for names, requiring stricter similarity, and to 0.8 for roles or types, allowing more flexibility.

\midinsert \clabel[jaro-winkler-similarity]{Jaro-Winkler similarity demonstration}
\ctable{ll|c}{
\hfil String 1 & \hfil String 2 & Jaro-Winkler similarity \crl \tskip4pt
Petr Pavel & Petr Pave & 0.98 \cr
Petr Pavel & Petra Pavlová & 0.89 \cr
herec & herečka & 0.87 \cr
státní úřad & statistický úřad & 0.66 \cr
}
\caption/t The table demonstrates similarity values for various strings. For names, a threshold of 0.9 was chosen to allow minor typographical errors while minimizing incorrect assignments to different individuals. For roles and types, a more lenient threshold of 0.8 was selected. For example, the similarity between herec and herečka illustrates a case we aim to accept (misidentified gender), while still minimizing incorrect mappings between different entities as in the last example.
\endinsert

The next step evaluates entity assignment to articles.
Each article contains a list of persons, organizations, and locations mentioned. During entity evaluation, a mapping is established between predicted entity identifiers and their ground truth counterparts. This mapping is then used to assess the accuracy of entity-to-article assignments. For each article and each entity type (person, organization, event), precision, recall, and F1-score can be computed. These precision and recall values are then averaged across entity types, yielding overall precision, recall, and F1-score for the assignment of entities to articles.

Event detection is evaluated similarly to entity detection.
The same principles applied to entity detection are used to evaluate event detection. Detected events are sequentially matched to ground truth events, recording the counts of TP, FP, and FN events. Since textual descriptions of events can vary significantly, the evaluation prioritizes semantic similarity of the compared texts. The "paraphrase-multilingual-MiniLM-L12-v2" model\cite[multilingual-sentence-bert] is used for this purpose, offering relatively fast, language-agnostic semantic embedding extraction. The resulting embeddings are then compared using cosine similarity.

\midinsert \clabel[semantic-similarity]{Semantic similarity demonstration}
\ctable{p{5cm}p{5.2cm}|c}{
\hfil String 1 & \hfil String 2 & Similarity \crl \tskip5pt
Jiří Drahoš získal 52,65 procenta hlasů. & Jiří Drahoše volilo 52,65 procenta voličů. & 0.97 \crl \tskip5pt
Václav Kopta moderoval ve\-černí u\-dílení cen Čes\-kého lva.
 & Konal se slavnostní ve\-čer udíle\-ní cen Českého lva. & 0.81 \crl \tskip5pt
ODS pro letošní volby dala přednost právníkovi Milanovi Golasovi. &  Komunální volby se konaly minulý víkend. & 0.41 \cr
}
\caption/t The table presents two event descriptions and their cosine similarity score based on embeddings from the "paraphrase-multilingual-MiniLM-L12-v2" model\cite[multilingual-sentence-bert]. Shows that semantically equivalent strings yield high similarity scores, while texts with lower semantic overlap yield a significantly lower similarity value.
\endinsert

Subsequent evaluation assesses event-entity and event-article relations.
As with articles, precision and recall are now calculated for the assignment of entities to events in which they appear. Additionally, precision and recall are computed for the assignment of events to the articles in which they are mentioned. The evaluation process thus yields precision and recall from the following stages: entity detection, event detection, entity-to-event assignment, entity-to-article assignment, and event-to-article assignment. All metrics are logged together with the experiment in MLflow. Since these problem aspects are considered equally important, the final metric for a given case is computed by averaging precision and recall across all tasks. This result is also attached to the experiment log. The final evaluation score is obtained by averaging all precision and recall values across all evaluation media cases, followed by computing the harmonic mean—i.e., the final F1 score.

\label[models]
\sec Used models
A wide range of language models was used for the experiments. The selection aimed to include both open-source and commercial models. Likewise, the models represent both large language models with direct inference and those with an integrated reasoning phase.

Meta {\em Llama-4-Scout-17B-16E-Instruct}\cite[llama4] is an open-source multimodal language model developed by Meta, based on the Mixture-of-Experts (MoE) architecture. It includes 17 billion active parameters out of a total of 109 billion and is capable of processing both textual and visual inputs. The model was trained on a large corpus of public and licensed data and is optimized for efficiently handling very long contexts, up to 10 million tokens. It is suitable for tasks such as multilingual document analysis and multimodal querying. The model is released under the community license Llama 4.\cite[llama4] The version used was hosted via e-INFRA CZ\cite[e-infra].

{\em Qwen3-14B-AWQ}\cite[qwen3] is an open large language model of the third generation, developed by the research team at Alibaba Group. It comprises approximately 14.8 billion parameters and uses 4-bit quantization (AWQ – Activation-aware Weight Quantization), enabling efficient deployment on hardware with limited computational capacity. The model is designed for two processing modes: a standard mode for common conversational tasks, and a so-called “thinking mode” focused on tasks requiring complex reasoning, including mathematics and programming. It natively supports sequences of up to 32,768 tokens, with the YaRN extension allowing up to 131,072 tokens. Qwen3-14B-AWQ supports over 100 languages and demonstrates strong performance in text generation, instruction following, and agent-based tasks.\cite[qwen3] A self-hosted version was used via the RCI computing cluster\cite[rci] using the vllm tool\cite[vllm] using following command:
\begtt
vllm serve Qwen/Qwen3-14B-AWQ --enable-reasoning \ 
    --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.98 
\endtt

{\em DeepSeek-R1-Distill-Qwen-32B}\cite[deepseek] is an open large language model with approximately 32 billion parameters, developed by the DeepSeek research team. The model is a distilled version of the larger DeepSeek-R1, which was trained using large-scale reinforcement learning without prior supervision (SFT). This approach enabled the model to develop capabilities such as self-verification, reflection, and the generation of extended chains of thought, representing a significant advancement in language model research. The distilled Qwen-32B model was further optimized for efficient deployment and shows high performance in text generation, dialogue optimization, and logical reasoning tasks.\cite[deepseek] In comparative evaluations, it outperforms models such as OpenAI-o1-mini. The version used was hosted via e-INFRA CZ\cite[e-infra].

{\em Gemini 2.5 Pro} and {\em Gemini 2.5 Flash} are advanced closed-source multimodal language models of the second generation developed by Google, supporting the processing of various input modalities (text, image, audio, video) and demonstrating high performance in tasks requiring inferential reasoning and language understanding. Gemini 2.5 Pro is optimized for complex, computationally intensive scenarios such as programming, mathematical tasks, and large-scale data analysis, and offers a context window of up to 1 million tokens. In contrast, Gemini 2.5 Flash is designed with a focus on low latency and efficient real-time deployment, typically for summarization, information extraction, and conversational systems; it allows the definition of a so-called “thinking budget,” i.e., a computational constraint to balance quality, speed, and cost. Both models share a common architecture and represent two complementary optimization strategies for different application scenarios.\cite[gemini] Access to the model was provided via Google AI Studio\cite[ai-studio].

{\em GPT-4.1}\cite[gpt4.1] is a closed-source multimodal language model developed by OpenAI in 2025, designed for advanced natural language understanding, code generation, and processing of very long contexts of up to 1 million tokens. Compared to previous versions, it achieves significantly better results in logical reasoning, following complex instructions, and multimodal analysis, including text, image, audio, and video. The model shows robust performance across standard benchmarks and represents a significant improvement in accuracy, efficiency, and generalization capabilities.\cite[gpt4.1] Access to the model was provided via the OpenAI Platform\cite[openai-platform].

\sec Experiments

Experimental methodology defines the objectives and procedures of experiments. The objective of the experimental section is to measure the performance of individual extraction pipeline configurations (v1, v2, and v3) defined in section \ref[pipeline-versions]. For the purposes of the experiments, the dataset was divided into validation and test splits. The experiments can be categorized into the following groups:

Performance measurement of the baseline configuration (v1) on the test dataset using selected language models.
Iterative improvement of the selected model's (Llama4 Scout) performance on the validation dataset through prompt engineering.
Performance measurement of the improved configuration (v2) on the test dataset using selected language models.
Performance measurement of the configuration with minimal batch size (v3) on the test dataset using selected language models.

A number of models were used for testing. For incremental improvement on the validation dataset, only one model was used, specifically the relatively fast Llama 4 Scout. However, for evaluation on the test dataset, several models were used. The reason for this was both the effort to compare the models on this specific problem and to ensure greater objectivity of the results, which would not depend solely on a single model. The list of models can be found in chapter \ref[models]. Some models were not used for all versions of the pipeline due to insufficient context or financial reasons. The intention was to include both open and commercial models.
