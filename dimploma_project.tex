% Lokální makra patří do hlavního souboru, ne sem.
% Tady je mám výjimečně proto, že chci nechat hlavní soubor bez maker,
% která jsou jen pro tento dokument. Uživatelé si pravděpodobně budou
% hlavní soubor kopírovat do svého dokumentu.

\def\ctustyle{{\ssr CTUstyle}}
\def\ttb{\tt\char`\\} % pro tisk kontrolních sekvencí v tabulkách

\chap Introduction

This semester project and future thesis deals with the creation and improvement of an application summarizing media cases. The app is intended to be used to introduce new journalists to the context of long-term media cases. Causes are tracked over time through a series of articles. The purpose of the tool is to abstract over this information and present a concise summary for each event on the timeline. The first version of the app already exists. Currently, improvements need to be designed and tested.

The application contains many natural language processing tasks. First of all, timeline summarization and event extraction problems are related to extracting individual events and their temporal validity. Furthermore, individual entities such as people, places, companies need to be detected. This preblematics is dealt with by named entity recognition. The task is extended in this by the need to detect other properties of entities such as roles of individuals. These properties must be unique. Also, the entity identification should be unique across all analyzed cases in the best case. The resulting passages, events and articles are appropriately clustered semantically so that the assigned clusters help the user to navigate within the case.

This project serves as preparation for the thesis. It deals with the analysis of the state of the art solutions available for each sub-problem. It also establishes the theoretical basis for the technologies that will be used in the thesis and places them in the context of the development of the Timelines application. 



\medskip  \clabel[timelines-ui]{User interface of Timelines web application.}
\picw=16cm \cinspic figs/timelines.png
\caption/f User interface of Timelines web application. \cite[timelines]{}
\medskip


\chap State of the art

The goal system consists of multiple tasks. Most of the tasks fall under the field of {\bf natural language processing}. First of all, it is necessary to define the individual components that make up the Timelines computation module. In general, the whole process can be classified under the multi-document summarization problem. Problems of this type deal with the extraction of important information and the abstraction of this information so that the information value of the text is condensed as much as possible. The multidocumentation of input data then presents a number of challenges, but also benefits. Given a large amount of data, it is possible to record multiple possible perspectives and the temporal development of events. However, at the same time, the volume to be processed increases, which prohibits the use of various established techniques with limited context. If sufficient context is available, other potential problems of variable processing quality with long inputs stand out. At the same time, multi-document summarization systems must contend with possible duplicate and mutually contradictory claims that are likely to be found in a multi-document dataset. \cite[multidocumentSummarization]{}

The problem presented consists of many sub-problems. Since the Timelines application solves a complex problem, we can further subdivide it into various known natural language processing problems. First of all, it deals with {\bf timeline summarization}. A multidocument summarization problem where the presence of atomic events is taken into account and they need to be concisely described and assigned to a timeline. Papers such as \cite[timeline-sum]{} and \cite[towards-timeline-sum]{} approach the problem in an abstract manner. They try to create a general description of the event by aggregating individual mentions. The MTS\cite[towards-timeline-sum]{} system achieves a value of 39.78 ROUGE-1 on the CNN/DailyMail dataset\cite[cnn-dailymail]{}. On the Papers With Code website \cite[paperswithcode-timeline-sum]{}, it is possible to follow the development of solutions in this problem category.

The next task is {\bf event extraction}. This involves a kind of relaxation of the timeline extraction task, which extracts structured information from often unstructured text. The system is responsible for detecting sections that are semantically and temporally related. Often questions such as who, when, where, what, why and how are answered. \cite[event-extraction]{} The problem is further divided into open-domain and close-domain. Open-domain understands an event as a set of related mentions over which clustering takes place. Close-domain is concerned with detecting events that correspond to a particular schema (e.g., a temporal event). 

Several solutions can be found in the literature. Based on a comparison of the quality \cite[pwc-event-extraction]{} of the output, the solutions DeepEventMine \cite[DeepEventMine]{} and DeepStruct \cite[DeepStruct]{} stand out from the recent ones. The older of the two, DeepEventMine, is based on the bioinformatics domain. Directed acyclic graphs are generated over the textual input, representing the sematic hierarchy. These hierarchies can be interleaved and further hierarchized in the text. The second method focuses on the possibility of general applicability to different types of event structures. The transferable problem is data event detection from twitter posts. A representative solution is for example \cite[twitter-events]{}. These solutions rely on the short document format; however, they also involve external knowledge in the detection process. 

An important part of the system is {\bf Named Entity Recognition} (NER). It deals with extracting mentions of entities falling into predefined semantic types such as person, location, organization. Many challenges are associated with this problem such as uniqueness of entities, duplicate mentions etc. \cite[NER]{} NER serves as an important pre-processing step for many nlp applications. In Timelines, it is an important component defining the information coherence of events. It also offers the possibility of intelligent filtering of events according to the people involved.

NER is a hot research topic. According to \cite[pwc-ner]{} the community provides several datasets and benchmarks, intended for solutions comparison. Among the current solutions, the following arise in particular. Automated Concatenation of Embeddings for Structured Prediction \cite[concat-embed]{} seeks the optimal way to represent entities numerically. Thus, it combines different kinds of embeddings, depending on the type of the current task. Another approach is models based on the BERT \cite[bert]{} architecture such as \cite[dice-loss]{}. It introduces a new loss function to compensate for the disproportion of easy and hard examples in the training dataset.

{\bf Clustering} can help navigate the events. The number of atomic features in the analyzed events can be sorted into a number of clusters based on sematics. Clustering in NLP in an extensive topic. Clustering methods usually depend on the ability to compare text elements against each other. For this reason, various numerical representations of text and distance measures and similarity functions are introduced. Clustering methods are divided into: hierarchical, partitioning, density-based, model-based, grid-based and soft-computing categories. \cite[clustering]{} 


Hierarchical methods recursively partition or merge data points in a top-down or bottom-up fashion, creating a dendrogram that shows nested groupings based on similarity. Agglomerative (bottom-up) merges clusters, while divisive (top-down) splits a single cluster into smaller sub-clusters.

Partitioning methods divide the data into a set number of clusters $K$ by relocating points between clusters, optimizing an error criterion such as the Sum of Squared Errors (SSE). K-means\cite[clustering]{}  is a popular partitioning method, where each cluster is represented by its mean and the algorithm iteratively minimizes the SSE.

Density-based methods focus on discovering clusters by identifying regions of high data point density, using algorithms like DBSCAN, which can detect clusters of arbitrary shapes by evaluating local density maxima. These methods assume the data is drawn from a mixture of probability distributions, often using the maximum likelihood principle. 

Model-based methods aim to optimize the fit between the data and a mathematical model, such as Gaussian or Poisson distributions, to describe each cluster. Algorithms like EM (Expectation-Maximization)\cite[clustering]{}  and AUTOCLASS\cite[clustering]{}  use statistical models to assign data points to clusters based on probabilistic distributions.

Grid-based methods partition the data space into a grid structure and perform clustering operations within the cells, offering fast processing times. These methods are efficient for large datasets but can struggle with high-dimensional data.

Soft-computing methods incorporate approaches like fuzzy clustering and genetic algorithms, which deal with uncertainty and optimization in clustering. These methods, including fuzzy C-means and evolutionary techniques, allow for flexible, probabilistic assignments of data points to clusters, adapting to complex data patterns.

Just as important as the clustering algorithm is the method of extracting the numerical representation of the text elements, so called {\bf embeddings}. Based on the benchmark\cite[clustering-benchmark]{} performed in 2023, the ST5-XXL\cite[ST5-XXL]{}\cite[hf-ST5-XXL]{} model appears to be the most promising, but the quality of the resulting clustering changes significantly with the change of the model depending on the actual task.

In order to enable interactive searches it will be possible to use the {\bf RAG} system. In the future, it is planned to extend the functionality of the Timelines application by the possibility of semantic search within cases and also possible improvement of filtering of input articles based on semantic information (not only metadata). The Retrieval-Augmented Generation system is suitable for this purpose. This is a two-step algorithm that first retrieves related documents based on various embedding techniques. The search result serves as input to the generative language model. Thus, it is possible to create a conversational tool with domain knowledge or to analyze a specific subset of data based on unstructured textual input. \cite[RAG]{}

The result depends on the exact {\bf prompts}. Since the current solution is based on queries to general large language models, the quality of the output is largely dependent on the quality of the instructions for the specific model. It is therefore worthwhile to consider an approach for improving the quality of these queries. Various automated approaches can be found in the literature. For example, \cite[black-box-prompt]{}, where the authors cyclically query the language model for feedback and then propose a query modification. Another approach is evolution optimization \cite[evolution-prompt]{}. 

\chap Materials \& Methods

The following list of methods and materials has been used and will be used in the preparation of the thesis. These include computational infrastructures, suitable perspective language models, libraries and software required for running the experiments and commissioning the application.

The experiments run on a computing cluster. The {\bf RCI compute cluster}\cite[rci]{} is used to provide access to opensource large language models. The cluster is an HPC (High Performance Computing) infrastructure designed for scientific purposes in the field of computationally intensive tasks such as large language models. The cluster consists of two subclusters, one based on Intel processors, the other on AMD processors. Each cluster consists of compute and management nodes. At the same time, a common data store takes care of persistence across nodes. The compute nodes of Intel clusters are divided into CPU, GPU and multi-CPU. GPU nodes have access to up to four NVIDIA Tesla V100 GPUs. In total, therefore, 48 Tesla V100 graphics cards are available. AMD subcluster nodes are divided into CPU, 4GPU, 8GPU and one node with access to a Graphcore M2000 IPU (Intelligence Processing Unit). Graphics nodes use graphics cards NVIDIA Tesla A100 with 40 GB of graphics operating memory. Thus, a total of 56 Tesla A100 GPUs available. Job scheduling is taken care of by the Slurm workload manager\cite[slurm]{}.

Most modern NLP solutions are based on {\bf transformers}. The Transformer architecture, introduced in the paper {\it Attention Is All You Need}\cite[transformer]{}, revolutionized sequence transduction tasks by eliminating recurrence and convolutions, relying instead entirely on self-attention mechanisms. Its encoder-decoder structure leveraged multi-head self-attention layers combined with position-wise feed-forward networks, enhanced by residual connections and layer normalization. The self-attention mechanism efficiently modeled dependencies across all positions in a sequence, while positional encodings preserved order information. This design allowed the Transformer to process sequences with unparalleled parallelism, significantly reducing training time compared to recurrent or convolutional models.

The architecture introduced innovations such as scaled dot-product attention and multi-head attention, enabling the model to attend to multiple representation subspaces simultaneously. In the encoder, all positions could influence each other globally, while the decoder incorporated masked self-attention to ensure autoregressive sequence generation. These design choices allowed the Transformer to achieve state-of-the-art results in tasks like machine translation and demonstrate exceptional adaptability to other domains. Its focus on attention mechanisms and computational efficiency has since become a foundation for modern deep learning systems.

\medskip  \clabel[transformer-architecture]{Transformer model architecture}
\picw=6cm \cinspic figs/transformer.png
\caption/f Transformer model architecture.
\medskip

{\bf Hugging Face} helps with the development of generative language models. Hugging Face offers a platform \cite[huggingface]{} for sharing source code, trained weights of individual deep learning models, and datasets for performance testing and comparison of trained models.

The Transformers library \cite[hf-transformers]{} is part of the Hugging Face platform. It is an interface for a unified approach to inference, training and retraining pre-trained models. The library is based on Python and supports deep learning libraries such as PyTorch, TensorFlow and Flax.

The {\bf vLLM}\cite[vllm]{}\cite[vllm-github]{} library helps with inference. The vLLM library takes care of optimizing inference and serving inference http endpoints for large language models. It introduces Pipelined Execution with Distributed Shared Memory (PDS) memory management paradigm, which enables the concurrent processing of multiple requests by sharing memory across different inference tasks. This design minimizes redundant computation and memory overhead, enhancing throughput and reducing latency. Compatible with various model repositories like Hugging Face Transformers, vLLM supports diverse architectures.

\chap Conclusion

The diploma project had several objectives. First of all, to describe the motivation and set the goals of the future thesis. The problem of the target application was decomposed into sub-problems, upon which an analysis of currently available state of the art solutions that address these problems was performed and for some of them an approach to their solution in the context of the application under development was proposed. Subsequently, the set of methods and resources needed to develop the resulting solution was described.

