\chap Introduction

% Generative methods generally outperform discriminative. \cite[genIEsurvey]{}

This thesis investigates the problem of multi-document structured summarization. Summarization is a central task in natural language processing (NLP) that addresses the challenge of information overload by distilling essential content from voluminous and redundant textual data. Existing approaches for text summarization include both extractive and abstractive methods, yet structured summarization remains a distinct subproblem requiring not only the condensation of information but also the extraction and organization of entities and events.

Modern advances in large language models (LLMs) have transformed the field. Deep learning techniques, particularly those based on transformer architectures, have achieved state-of-the-art results in text summarization tasks. However, multi-document summarization (MDS) presents unique challenges, including semantic conflicts, duplication, and the need for temporal alignment of information across heterogeneous sources. Addressing these issues demands not only robust models but also methodological innovations in knowledge extraction, prompt engineering, and evaluation metrics.

The primary objective of this work is to design, implement, and evaluate a pipeline for structured knowledge extraction from a set of related documents. The core of the proposed system lies in the integration of advanced named entity recognition, event extraction, and clustering techniques, leveraging both open-source and commercial large language models. The pipeline is developed to extract and deduplicate structured elements—such as people, organizations, locations, and events—across document collections, supporting timeline construction.

A substantial part of this research is devoted to empirical analysis and reproducible experimentation. The system is benchmarked on a news corpus dataset, with methodological rigor ensured by manual annotation and the development of a dedicated evaluation dataset. Performance is measured using both classical information retrieval metrics (precision, recall, F1-score) and custom metrics tailored for the structured output format. Experimentation includes systematic prompt engineering, model comparison, and ablation studies to assess the impact of design choices, batch processing, and temperature settings.

The contribution of this thesis is a comprehensive and modular extraction pipeline, robust experimental methodology, and an open evaluation framework for structured multi-document summarization. The thesis demonstrates the feasibility of using state-of-the-art language models for structured knowledge extraction, highlights performance trade-offs between open-source and commercial models, and identifies current limitations and avenues for further improvement. The subsequent chapters provide background, describe the methodology, present empirical results, and discuss broader implications and future directions.