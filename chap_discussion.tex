\chap Discussion

This chapter provides a focused discussion based solely on a comparative analysis of results introduced in the previous chapter. The analysis centers on the relative performance of different models and pipeline versions, examining how evaluation metrics such as precision, recall, and F1 score are affected by model choice and iterative changes to the extraction process. Special attention is given to trends highlighted by statistical analysis and visual summaries, allowing for a critical assessment of absolute and relative strengths among the evaluated approaches.

Additionally, the chapter systematically compares outcomes from baseline and improved pipeline variants, as well as the effect of processing articles in batches versus individually. This assessment quantifies whether prompt refinements or changes in inference strategy yield meaningful gains, relying on statistical significance testing and visual inspection of metric distributions. The final part of the discussion reflects on the limits of current improvements and outlines further research directions, providing context for understanding the present state and future prospects of the extraction system.

\sec Model Performance

Model performance is compared across all pipeline versions using results from the previous chapter. The main trends are visible in the heatmap (Figure \ref[models-heatmap]), which displays Precision, Recall, and F1 scores for each model and pipeline version, with color intensity indicating performance from light (low) to dark blue (high).

\medskip  \clabel[models-heatmap]{Heatmap of model performances}
\picw=14cm \cinspic figs/discussion/model-performance-heatmaps.png
~\caption/f Heatmap of model performances across pipeline versions for Precision, Recall, and F1. Each cell displays the score achieved by a given model (row) for a specific pipeline version (column), with color intensity ranging from light blue (lower performance) to dark blue (higher performance). The three panels represent the different evaluation metrics. Darker blue cells correspond to stronger performance. The heatmap highlights that commercial models (e.g., Gemini 2.5 Flash, GPT 4.1) consistently achieve the highest scores across all metrics and pipeline versions, while open-source models tend to perform lower, especially in more complex extraction phases. Comparing columns shows the effect of pipeline improvements for each model; reading across rows allows quick assessment of relative model strength.
\medskip


Performance differences are primarily driven by model choice. Commercial models, especially Gemini 2.5 Flash and GPT 4.1, consistently outperform open-source models such as Llama 4 Scout and Qwen3 across all evaluation metrics. This is clearly visible in the heatmap, where the darkest blue cells cluster around commercial models in each metric panel, while open-source models remain in lighter blue regions, indicating weaker performance.

Pipeline improvements generally increase performance. Most models show a trend of increasing precision and recall with each new pipeline version, resulting in higher F1 scores. The effect of prompt engineering and more structured extraction is most apparent for open-source models, where F1 scores improve between versions, confirming the positive impact of iterative pipeline refinement.

However, Gemini 2.5 Flash demonstrates a different behavior. For Gemini, recall improves slightly with each pipeline update, but this is offset by a corresponding decrease in precision. As a result, the F1 score remains nearly constant across all pipeline versions, as shown by the stable, dark blue F1 cells for Gemini in Figure \ref[models-heatmap].

Reducing batch size has minimal effect on the performance hierarchy. When the pipeline processes single articles rather than batches, the relative ranking of models does not change, and commercial models remain dominant.

In summary, commercial models are consistently superior across all pipeline versions, and most models benefit from pipeline optimization. Gemini 2.5 Flash is an exception, with F1 score stagnating despite small shifts in recall and precision. The heatmap in Figure \ref[models-heatmap] offers a clear, visual summary of these trends, allowing quick assessment of both absolute performance and the effects of pipeline improvements for each model.

In addition to model and pipeline effects, the impact of temperature settings was examined for the Llama model. Results showed that while precision and F1 score were largely unaffected by changes in temperature, recall varied significantly. This was confirmed by ANOVA, which found a statistically significant effect only for recall. Notably, recall was maximized at a temperature value of 0, which was therefore used in most experiments. Thus, for Llama, temperature tuning primarily influences recall, making it a useful tool when maximizing retrieval is important, without compromising overall accuracy.

\sec Assessment of System Improvements

This section summarizes the key findings from both the baseline improvement (v1 vs v2) and the single-batch approach (v2 vs v3) experiments.

Baseline improvements produced modest but non-significant performance gains.
Comparing the initial extraction pipeline (v1) with the prompt-refined version (v2), the results show a small average increase in F1 score and recall, while precision remained nearly unchanged. Specifically, F1 score improved from 0.450 to 0.476 and recall from 0.494 to 0.509, with standard deviations indicating high variability across different model-dataset pairs. Both the paired t-test and the Wilcoxon signed-rank test yielded p-values far above the 0.05 threshold, indicating that the observed improvements are not statistically significant. Visualizations, including grouped bar plots and scatter plots, reinforce this conclusion by revealing that the majority of performance differences are minor and not systematically in favor of the improved version. Therefore, although iterative prompt engineering slightly improved average performance, it did not deliver robust or consistent gains across all tested scenarios.

The single-batch (article-by-article) approach does not improve extraction effectiveness.
When comparing the standard batch inference (v2) to the minimum batch (v3), where only one article is processed at a time, results show negligible differences. The mean F1 score rose slightly from 0.459 to 0.465, but this change is far smaller than the observed standard deviation. Precision marginally decreased while recall slightly increased, but none of these differences approached statistical significance according to either the t-test or Wilcoxon signed-rank test. Figures summarizing these comparisons visually confirm the lack of consistent benefit: most bars in difference plots are close to zero, and scatter plots show points tightly clustered along the identity line. Thus, reducing context to a single article per inference does not yield measurable improvements in extraction accuracy.

Overall, both approaches yield stable but limited results.
Across both experimental comparisons, the system demonstrates stable extraction performance, with only minimal and statistically insignificant differences arising from prompt engineering or batch size manipulation. The similarity of results across all evaluation metrics, statistical tests, and visualization techniques suggests that the current extraction pipeline may be approaching a performance plateau on the available datasets. Incremental changes to prompt design or batching strategy appear insufficient to drive further substantial improvements.

\sec Potential Future Improvements

Several promising directions remain for further development of multi-document structured summarization systems. The current work establishes a strong foundation, but further research can address limitations related to evaluation methods, event schema design, domain generalization, and prompt engineering. The following paragraphs outline key areas for future improvement based on findings and open questions from this thesis.

Evaluation methods can be improved. Traditional metrics like precision and recall only partially capture extraction quality. Large language models can act as sophisticated judges, evaluating structured summaries with a deeper understanding of context, relevance, and factuality. Using LLMs as evaluators can highlight discrepancies and subtle errors missed by automated metrics. Comparing LLM-based assessments with current quantitative metrics may reveal gaps and lead to more reliable evaluation strategies.

Event schema should be more detailed. The current approach extracts mainly atomic events, which misses nested events and complex relationships. Future systems should support richer event representations, such as hierarchies, causal links, and structured arguments for each event. This would provide more informative timelines and enable advanced analysis of event sequences in the data.

Expanding to new domains is important. The present solution is designed for news articles, but other domains—like legal, biomedical, or scientific texts—have different structures and vocabularies. Adapting the system will require domain-specific models, prompts, and possibly new annotation standards. Generalizing the approach to various domains will demonstrate its true versatility and value.

Prompt design should be more adaptive. Manual prompt engineering is slow and often suboptimal. Automated prompt optimization techniques, such as large-scale few-shot libraries, can discover and select effective instructions more efficiently. These methods can increase extraction quality and make the system easier to adapt to new tasks, datasets, or domains in the future.