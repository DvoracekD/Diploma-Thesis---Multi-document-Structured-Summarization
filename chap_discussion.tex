\chap Discussion

\sec Assessment of baseline improvement

We want to verify whether there has been an improvement in the extraction pipeline. In this stage, we compare the baseline implementation and prompts with the improved version obtained by iterating on the prompt design. The iterations were guided solely by the results of a single model (Llama 4) on the validation dataset. However, for the verification, we use a test dataset and a larger number of models. We designate the baseline solution as v1. We designate the improved solution as v2. Both versions were evaluated using four models across several datasets (media cases), measuring precision, recall, and F1-score for each (model, dataset) pair. The goal is to determine if the new version offers statistically significant improvements.

Data is loaded and merged. Evaluation results from both versions are exported as CSV files, loaded, and standardized by renaming columns for clarity. Both datasets are merged on model and dataset, enabling direct comparison. For each metric, the difference between the new and old version is calculated for every (model, dataset) pair.

Descriptive statistics give overview. The mean and standard deviation are calculated for each metric and version, along with the mean difference. This step provides an initial impression of potential improvements in precision, recall, and F1-score, indicating the central tendency and spread for both system versions.

For each metric (precision, recall, F1-score), we formally test the following hypotheses:
\begitems
* Null Hypothesis ($H_0$): There is no difference in the metric between the two system versions.
* Alternative Hypothesis ($H_1$): There is a difference in the metric between the two versions (two-sided test).
\enditems

Mathematically, for each metric $M$:

$$
H_0: {\bbchar E}[M_{v2} - M_{v1}] = 0
$$
$$
H_1: {\bbchar E}[M_{v2} - M_{v1}] \neq 0
$$

Test methodology is robust. The paired t-test assesses whether the mean difference is significantly different from zero, assuming approximate normality. The Wilcoxon signed-rank test provides a non-parametric alternative that does not require normality. A p-value below 0.05 is considered statistically significant.

\midinsert \clabel[descriptive-v1-v2]{Descriptive statistics for system versions 1 and 2}
\ctable{l|cccccc}{
Metric & $\mu_1$ & $\sigma_1$ & $\mu_2$ & $\sigma_2$ & $\mu_{2-1}$ & $\sigma_{2-1}$ \crl \tskip4pt
Precision & 0.629 & 0.117 & 0.633 & 0.091 & 0.004 & 0.082 \cr
Recall    & 0.494 & 0.085 & 0.509 & 0.083 & 0.014 & 0.042 \cr
F1-score  & 0.450 & 0.123 & 0.476 & 0.103 & 0.026 & 0.067 \cr
}
\caption/t This table provides a summary of the central tendency ($\mu$) and spread ($\sigma$) for each metric (precision, recall, F1-score) in both system versions. $\mu_1$, $\sigma_1$ refer to version 1, $\mu_2$, $\sigma_2$ to version 2, and $\mu_{2-1}$, $\sigma_{2-1}$ to the difference. Read horizontally to compare old and new versions. The final columns show the average change, indicating if, on average, the new system increases or decreases performance.
\endinsert


\midinsert \clabel[statistics-v1-v2]{Statistical test results for metric differences}
\ctable{l|cccc}{
Metric & $\mu_{2-1}$ & $\sigma_{2-1}$ & t-test p-value & Wilcoxon p-value \crl \tskip4pt
Precision & 0.004 & 0.082 & 0.84  & 0.70 \cr
Recall    & 0.014 & 0.042 & 0.14  & 0.18 \cr
F1-score  & 0.026 & 0.067 & 0.098 & 0.29 \cr
}
\caption/t This table shows the results of both statistical tests for every metric. The Mean Diff ($\mu_{2-1}$) and Std Diff ($\sigma_{2-1}$) reflect the average and standard deviation of observed improvement. The p-values (t-test and Wilcoxon) indicate whether observed changes are statistically significant. To interpret, look for p-values below 0.05, which indicate strong evidence against the null hypothesis.
\endinsert

\medskip  \clabel[v1-v2-bar-plot]{Comparison v1-v2: Grouped bar plot}
\picw=14cm \cinspic figs/discussion/v1-v2-barplot.pdf
\caption/f Grouped bar plot displays F1 scores for each (model, dataset) combination for both system versions. Each group contains two bars: one for the old version and one for the new version. To interpret, locate the (model, dataset) of interest along the axes, then compare the height of the bars within each group. Taller bars indicate higher F1 scores. If the bar for the new version is taller than that of the old, the new system performed better for that specific case. This figure allows a quick visual scan for systematic improvements or regressions across all groups.
\medskip


\medskip  \clabel[v1-v2-diff-bar-plot]{Comparison v1-v2: Difference bar plot}
\picw=10cm \cinspic figs/discussion/v1-v2-f-diff.pdf
\caption/f Bar plot visualizes the difference in F1 score (v2 - v1) for each (model, dataset) pair. The x-axis shows different models, grouped or colored by dataset. Each bar’s height indicates how much the F1 score changed: positive values (bars above zero) represent improvements, and negative values (bars below zero) represent declines. The horizontal dotted line at zero provides a clear visual threshold—bars crossing above this line signify improvement, those below indicate regression. To interpret, examine the position and height of each bar relative to zero.
\medskip

\medskip  \clabel[v1-v2-data-points]{Comparison v1-v2: Scatter plot}
\picw=10cm \cinspic figs/discussion/v1-v2-data-points.pdf
\caption/f Scatter plot compares F1 scores of the two versions for each (model, dataset) pair. The shape represents a dataset, color of the point represents a model. Each point represents one (model, dataset) instance, with the x-coordinate showing the old version’s F1 and the y-coordinate showing the new version’s F1. The diagonal dashed line marks equality ($y = x$): points above this line indicate that the new version outperformed the old for that pair, while points below indicate a decrease. The distance from the line reveals the magnitude of the difference. This figure allows the reader to quickly assess whether improvements are systematic (most points above the line) or inconsistent.
\medskip

Conclusion is based on statistical evidence. Statistical tests and visualizations indicate a trend toward improved F1 scores, but these improvements do not reach statistical significance at the 0.05 level. Mean values are generally higher for the new version, but standard deviations indicate considerable overlap.

\sec Assessment of single batch approach

We want to verify whether the minimum batch approach improves the solution. This is an approach where each phase of the extraction pipeline works with only one article. The idea is that if the model uses a smaller context, it can focus more effectively on details in the text. We will refer to this approach as v3. This approach is compared with the extraction method that uses the maximum batch size, meaning that all five articles are processed in a single inference. We refer to this approach as v2, the same as in the previous section.

Statistical comparison continues for system versions two and three. Both versions are evaluated on the same set of models and datasets (media cases), with precision, recall, and F1-score measured for each (model, dataset) pair. The purpose is to identify any statistically significant improvement in the third version over the second.

Data handling is consistent. Results from both versions are loaded, standardized, and merged by model and dataset. For each metric, the difference between the new and previous version is computed for every (model, dataset) pair.

Descriptive statistics reveal distributions. The mean ($\mu$) and standard deviation ($\sigma$) for each metric and version, together with the mean and standard deviation of their differences, provide a summary of central tendency and variability. This statistical overview indicates the typical value and spread for each system version and the magnitude of observed changes.

\midinsert \clabel[descriptive-v2-v3]{Descriptive statistics for system versions 2 and 3}
\ctable{l|cccccc}{
Metric & $\mu_2$&	$\sigma_2$&	$\mu_3$&	$\sigma_3$&	$\mu_{3-2}$&	$\sigma_{3-2}$ \crl \tskip4pt
Precision&	0.622&	0.097&	0.610&	0.094&	-0.012&	0.049 \cr
Recall&	0.497&	0.087&	0.503&	0.094&	0.005&	0.059 \cr
F1-score&	0.459&	0.106&	0.465&	0.098	&0.006	&0.053 \cr
}
\caption/t Descriptive statistics for precision, recall, and F1-score for system versions 2 and 3. $\mu_2$ and $\sigma_2$ are the mean and standard deviation for version 2, $\mu_3$ and $\sigma_3$ for version 3, and $\mu_{3-2}$, $\sigma_{3-2}$ are the mean and standard deviation of the difference (version 3 minus version 2).
\endinsert

Statistical hypotheses are formally tested. The null hypothesis for each metric asserts no difference between version 3 and version 2; the alternative hypothesis asserts a difference. The tests used are the paired t-test (which assumes normally distributed differences) and the Wilcoxon signed-rank test (which is non-parametric). A p-value less than 0.05 is interpreted as statistically significant evidence of a difference.

\midinsert \clabel[test-v2-v3]{Statistical test results for difference between versions 3 and 2}
\ctable{l|cccc}{
Metric&	$\mu_{3-2}$&	$\sigma_{3-2}$&	t-test p-value&	Wilcoxon p-value \crl \tskip4pt
Precision & -0.012 & 0.049 & 0.362 & 0.389 \cr
Recall    & 0.005  & 0.059 & 0.727 & 0.489 \cr
F1-score  & 0.006  & 0.053 & 0.667 & 0.421 \cr
}
\caption/t Statistical test results for metric differences (version 3 minus version 2). $\mu_{3-2}$ and $\sigma_{3-2}$ denote the mean and standard deviation of the difference. p-values from the paired t-test and Wilcoxon signed-rank test are shown; p-values below 0.05 indicate statistical significance.
\endinsert

\medskip  \clabel[v2-v3-bar-plot]{Comparison v2-v3: Grouped bar plot}
\picw=14cm \cinspic figs/discussion/v2-v3-barplot.pdf
\caption/f Grouped bar plot showing F1 scores for each (model, dataset) combination for both system versions. Each group contains two bars: one for version 2 and one for version 3. The notation for each axis corresponds to the models and datasets. Taller bars for version 3 indicate local improvement. The plot facilitates systematic visual assessment across groups.
\medskip

\medskip  \clabel[v2-v3-diff-bar-plot]{Difference bar plot emphasizes magnitude and direction}
\picw=10cm \cinspic figs/discussion/v2-v3-f-diff.pdf
\caption/f Bar plot displaying the difference in F1 score ($\Delta$F1 = F1$_3$ – F1$_2$) for each (model, dataset) pair. The x-axis categorizes by model, colors distinguish datasets. The horizontal dashed line at zero highlights whether the difference is positive or negative. The magnitude and direction of each bar directly visualizes the local effect of the system change.
\medskip


\medskip  \clabel[v2-v3-data-points]{Scatter plot details pairwise relationships.}
\picw=10cm \cinspic figs/discussion/v2-v3-data-points.pdf
\caption/f Scatter plot comparing F1 scores for version 2 (x-axis) and version 3 (y-axis) for each (model, dataset) pair. Each point represents a single instance. The diagonal dashed line ($y=x$) is the reference for equality: points above the line indicate higher F1 in version 3, points below indicate higher F1 in version 2. This plot summarizes whether improvements are systematic or sporadic.
\medskip

Performance differences are minimal between system versions two and three.
Descriptive statistics show only a negligible increase in mean F1-score from 0.459 to 0.465, with the mean difference (0.006) far smaller than the standard deviation (0.053). Mean precision actually declined slightly, while mean recall improved by a similarly small margin.

Statistical tests support the absence of significant change.
The paired t-test p-values for all metrics are substantially greater than 0.05 (precision: 0.36; recall: 0.73; F1: 0.67), as are the Wilcoxon signed-rank test p-values. This statistical evidence indicates that the observed differences could easily be due to random variation rather than any systematic effect of the new system version.

Figure analysis confirms the statistical result.
In the grouped bar plots (\ref[v2-v3-bar-plot]), the bars representing system versions two and three are generally similar in height for all model-dataset pairs. In the difference bar plot (\ref[v2-v3-diff-bar-plot]), most bars cluster close to zero, with a nearly even balance of positive and negative values, further indicating the lack of consistent improvement or decline. The scatter plot (\ref[v2-v3-data-points]) shows that most points lie close to the diagonal, reinforcing that performance between the two versions is nearly equivalent for every tested combination.

Overall, the empirical evidence from all metrics, tests, and visualizations demonstrates that system version three does not yield any substantial or statistically significant improvement over version two. The approach of extracting data article by article is not more efficient than batch inference.