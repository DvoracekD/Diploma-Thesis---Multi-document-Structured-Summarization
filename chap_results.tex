\chap Results

This chapter presents the evaluation results of the proposed information extraction pipeline. The chapter is structured into several key sections, each addressing a distinct aspect of the experimental assessment.

The first section, Baseline evaluation, establishes reference performance levels by applying the initial version of the extraction pipeline to the test dataset using several state-of-the-art language models. Next, the Pipeline optimization section details iterative improvements to the pipeline, focusing on prompt engineering and targeted adjustments, with performance tracked across iterations. The third section, Optimized pipeline, summarizes the evaluation of the final, enhanced extraction pipeline using selected models, facilitating a direct comparison with baseline results. In the Minimal batch evaluation section, the impact of reducing the batch size to one—forcing per-article inference—is investigated to assess its effect on model performance and output consistency. Finally, the Temperature influence section systematically analyzes the effect of the inference temperature parameter on key evaluation metrics, utilizing both visual and statistical methods.

\sec Baseline Evaluation

The baseline quality was assessed. The baseline implementation of the extraction pipeline, together with the baseline instructional prompts, was evaluated using the evaluation process described earlier. The initial instructional prompts for the pipeline were tuned on the Llama 4 model.

The only controlled inference parameter for most models was temperature. The parameter was set to 0 to maximize the determinism of the experiments and to support structured output generation in JSON format. Based on the experiment (see \ref[temperature-influence]), it was also found that a value of 0 provides the best performance in terms of the overall recall of the process and yields comparable results in terms of precision and F1 score. Only for the Qwen3 model were the parameters recommended by the model authors used, namely "temperature": 0.6, "top_p": 0.95, and "top_k": 20.

Experiments were conducted using models Qwen3 (14B), GPT 4.1, Gemini 2.5 Pro, Gemini 2.5 Flash, DeepSeek R1 (32B), and Llama 4 Scout (17B). For a detailed description, see Section \ref[models]. For each model, one evaluation of a highly deterministic extraction pipeline was carried out, and the results can be observed in a series of visualizations.

\medskip  \clabel[v1-barplot]{Baseline metrics}
\picw=10cm \cinspic figs/results/pipeline-v1-precision-recall-bar.pdf
~\caption/f The bar plot offers a visualization of aggregated overall metrics across all test cases. For each model, the values of overall precision (blue), recall (red), and F1 score (green) are available.
\medskip

\medskip  \clabel[v1-metric]{Baseline model comparison by metric}
\picw=12cm \cinspic figs/results/pipeline-v1-coord-per-metric.pdf
~\caption/f In the parallel coordinates graph, it is possible to observe changes in the ranking of individual models depending on the observed metric. The horizontal axis represents the model used and then the individual metrics (overall precision, overall recall, and overall F1 score). The vertical axis then describes the values of these parameters. For maximum information value within limited space, the individual axes do not have the same range and scale, even though all three metrics share the same range of 0 to 1. The colored lines in the graph represent individual experiments. The lines connect the vertical axes at the metric values measured in the corresponding experiment. The color of the line corresponds to the value of the F1 score in the given experiment.
\medskip

\medskip  \clabel[v1-stage]{Baseline model comparison by evaluation aspect}
\picw=12cm \cinspic figs/results/pipeline-v1-coord-per-stage.pdf
~\caption/f The parallel coordinates diagram shows changes in the ranking of models depending on the F1 score value for individual aspects of the evaluation. These include the model's ability to detect entities in text (Entity Detection), the correct assignment of detected entities to the articles in which they appear (Entity-article Assignment), and the ability to detect entities (Entity Detection) and correctly assign these to events in the articles where they are mentioned. The color of the experiments is the overall value of the F1 score measured in the experiment.
\medskip

\medskip  \clabel[v1-dataset]{Baseline model comparison by dataset}
\picw=12cm \cinspic figs/results/pipeline-v1-coord-per-dataset.pdf
~\caption/f The parallel coordinates diagram for F1 score values in individual media cases that make up the test dataset. The color of the experiment corresponds to the overall measured value of the F1 score.
\medskip


In \ref[v1-barplot], the dominance of the Gemini 2.5 model can be observed across all monitored metrics. For easier comparison of the individual models, we can use parallel coordinates graphs, which can help us better visualize the ranking of models based on different aspects. In \ref[v1-metric], the dominance of Gemini is observed across all global metrics. It is followed by Gemini 2.5 Pro and GPT 4.1. The results between DeepSeek R1 and Llama 4 Scout are then less conclusive. DeepSeek stands out more in precision, while Llama stands out more in recall. The ranking of the models is concluded by Qwen3.

Similar trends can be observed in the graphs \ref[v1-stage] and \ref[v1-dataset]. In the diagram \ref[v1-stage], the movement of the F1 score for individual models and individual evaluation phases can be tracked. In the diagram \ref[v1-dataset], on the other hand, the F1 scores of the models can be tracked across the individual datasets. It can be observed that in both graphs, the Gemini models show very similar results. They are followed by the GPT 4.1 model, which surpasses the Gemini Pro version only on the {\it Senátní Volby 2018} dataset. The remaining open-source models perform quite similarly both across evaluation aspects and datasets. On all parallel coordinates diagrams, it is possible to observe a separation of the group of commercial models from the group of open-source models, with the commercial models being superior.

\sec Pipeline Optimization

The baseline pipeline was improved. For iterations over the solution, the evaluation pipeline was applied to the validation dataset. Only one model, namely Llama 4 Scout, was used for the prompt tuning. The improvement workflow was as follows:

\begitems \style N
* Evaluation of the pipeline on the validation dataset.
* The weakness of the current version of the extraction pipeline was identified, either by inspecting the extracted structure or by comparing individual metrics. 
* Modification of individual instruction prompts (addressing the identified weakness).
* Repeat the process until a significant improvement is achieved.
\enditems

In the initial iterations, it was found that the model had, compared to precision, a very weak performance in terms of recall on entities and events. This means that it detected only a small proportion of the required entities or events. This finding was confirmed by examining the predicted structure. The model, it appears, tends to abstract the results and extract only those results that are globally more important. For example, in the case of the prompt for extracting persons, the following note was added:

\begtt
IMPORTANT:
- Extract ALL people mentioned in the text, regardless 
  of their importance or how briefly they are mentioned.
- Do not skip any person mentioned in the articles, even if 
  they seem minor or are only mentioned once.
- Every individual mentioned by name must be included in the output.
\endtt

Further, examples were added. Given the proven effectiveness of using few-shot prompting~\cite[prompt-engineering], fully developed examples were added to the original prompts, containing both the complete form of the input structures and texts as well as the exact form of the expected output. Some baseline prompts had already used examples, but these were more fragments of edge-case examples rather than a complete definition of input and output.

\medskip  \clabel[v1-iter]{Progression of baseline optimization}
\picw=14cm \cinspic figs/results/v1-iter-all.pdf
~\caption/f The figure shows changes in the individual metrics. In all charts, the horizontal axis shows the number of the improvement iteration, and the vertical axis shows the value of the metric. All the charts share the same scale. In the upper left corner, the overall aggregated values of the tracked metrics are located. In the rest of the top row, the metrics calculated for individual evaluation phases are presented. The bottom row presents the metrics calculated on individual datasets (media cases). In the lower left corner, the overall aggregated values of the tracked metrics are located. In the rest of the bottom row, the metrics calculated for individual evaluation phases are presented.
\medskip


We observe Figure \ref[v1-iter]. In the chart of overall values in the lower left corner, it is possible to see improvement in precision and F1 score, while recall remains more or less constant. Improvement in precision can be observed in all charts except for event detection. Recall increased only for Event Detection, the {\it Volby 2018} dataset, and the {\it Peking 2022} dataset, and in both cases only slightly. Due to the significant contribution of precision, the F1 score also increases in most charts.

In the final system, unfortunately, there was no improvement in average recall, but it was possible to significantly improve the overall precision on the validation dataset (from 0.55 to 0.68), while recall remained nearly constant. Thus, it was also possible to improve the overall F1 score.

\sec Optimized Pipeline Evaluation

The optimized pipeline was then tested on the test dataset. Only models Qwen3, Llama 4 Scout, GPT 4.1, and Gemini 2.5 Flash were included. The evaluation procedure remained as previously described to maintain comparability. Deterministic inference (temperature set to 0) was used except where otherwise recommended. Gemini 2.5 Pro was not used for financial reasons. DeepSeek R1 was not used due to the insufficient context window of the version used. The generated responses in this version of the extraction pipeline are on average longer than the baseline due to more detailed analysis enforced by updated prompts.

\medskip \clabel[v2-barplot]{Enhanced pipeline metrics}
\picw=10cm \cinspic figs/results/pipeline-v2-precision-recall-bar.pdf
~\caption/f The bar plot summarizes overall precision, recall, and F1 score across all models and test cases. Gemini 2.5 Flash shows leading performance in all metrics.

\medskip

The Gemini 2.5 Flash model reached the highest overall precision (0.68), recall (0.59), and F1 score (0.56). GPT 4.1 followed with 0.54 precision, 0.44 recall, and 0.38 F1. The open-source models, Llama 4 Scout and Qwen3, showed lower scores, with Qwen3 at the bottom for all metrics.

Parallel coordinates visualization enables detailed comparison by metric. In \ref[v2-metric], Gemini 2.5 Flash clearly dominates precision, recall, and F1. The largest margin is observed in precision, where Gemini reaches 0.68, compared to GPT 4.1's 0.54. Model ranking remains consistent across all metrics.

\medskip \clabel[v2-metric]{Enhanced pipeline comparison by metric}
\picw=12cm \cinspic figs/results/pipeline-v2-coord-per-metric.pdf
~\caption/f Parallel coordinates graph for model performance by metric. Each colored line represents an experiment, colored by F1 score.

\medskip

Stage-wise evaluation examines F1 scores for four key extraction phases: Entity Detection, Entity-Article Assignment, Event Detection, and Event-Article Assignment. Gemini 2.5 Flash outperforms others at all stages. For example, event-article assignment reaches an F1 of 0.77. Open-source models, particularly Qwen3, are notably weaker in complex assignment phases, where F1 often falls below 0.30.

\medskip \clabel[v2-stage]{Enhanced pipeline comparison by evaluation aspect}
\picw=12cm \cinspic figs/results/pipeline-v2-coord-per-stage.pdf
~\caption/f Parallel coordinates for F1 scores by evaluation stage. Gemini consistently achieves the highest values.

\medskip

Dataset-level analysis reveals that Gemini 2.5 Flash leads in F1 score for every dataset. Its results range from 0.51 ({\it Český lev}) to 0.72 ({\it Volby 2018}). Llama 4 Scout and Qwen3 score considerably lower, with gaps exceeding 0.15 F1 on all datasets. The largest differences appear for {\it Summit Kim-Trump} and {\it Volby 2018}.

\medskip \clabel[v2-dataset]{Enhanced pipeline comparison by dataset}
\picw=12cm \cinspic figs/results/pipeline-v2-coord-per-dataset.pdf
~\caption/f Parallel coordinates plot for F1 scores by dataset. Performance advantage of commercial models is most evident for challenging cases.

\medskip

These results confirm previous findings. The performance gap persists across aggregate metrics, evaluation stages, and all datasets considered.

\sec Minimal Batch Evaluation

The pipeline from the previous chapter was re-evaluated with a key modification: batch size was reduced to one, so each model processed only a single article at a time. This experiment was designed to test the impact of sequential, article-by-article extraction, while keeping all other settings consistent with the previous evaluation. Only models Qwen3, Llama 4 Scout, and Gemini 2.5 Flash were included in this experiment. This extraction pipeline version consists of many inference requests. Therefore, GPT 4.1 was excluded from the experiment for financial reasons.

\medskip \clabel[v3-barplot]{Batch size 1 pipeline metrics}
\picw=8cm \cinspic figs/results/pipeline-v3-precision-recall-bar.pdf
~\caption/f The bar plot summarizes overall precision, recall, and F1 score for all included models when processing single articles.

\medskip

The results confirm the leading position of Gemini 2.5 Flash, which achieved the highest overall scores in all metrics. Precision was 0.67, recall 0.60, and F1 score 0.56. Llama 4 Scout reached 0.53 precision, 0.44 recall, and 0.40 F1. Qwen3 trailed behind, showing the lowest values in every metric.

A parallel coordinates plot was used to compare model performance by metric. In \ref[v3-metric], Gemini 2.5 Flash maintained its clear advantage across all measured metrics. The margin over Llama 4 Scout and Qwen3 remained stable compared to the previous evaluation, particularly in precision and recall.

\medskip \clabel[v3-metric]{Batch size 1 pipeline comparison by metric}
\picw=12cm \cinspic figs/results/pipeline-v3-coord-per-metric.pdf
~\caption/f Parallel coordinates plot for model performance by metric when using batch size 1. The Gemini model consistently leads in all metrics.

\medskip

Stage-wise analysis of F1 scores for entity detection, entity-article assignment, event detection, and event-article assignment reveals a consistent trend. Gemini 2.5 Flash outperforms the open-source models at each stage. For event-article assignment, Gemini 2.5 Flash reached an F1 score of 0.78, with Llama 4 Scout following at 0.57 and Qwen3 at 0.52. Open-source models are particularly weaker in entity-article assignment and entity detection, where their F1 scores are frequently below 0.30.

\medskip \clabel[v3-stage]{Batch size 1 pipeline comparison by evaluation aspect}
\picw=12cm \cinspic figs/results/pipeline-v3-coord-per-stage.pdf
~\caption/f Parallel coordinates plot for F1 scores by evaluation stage. Gemini 2.5 Flash achieves the highest F1 at all stages.

\medskip

Dataset-level analysis again shows Gemini 2.5 Flash ahead in F1 score on every dataset, with results ranging from 0.53 on {\it Český lev} and 0.56 on {\it Covid} to 0.72 on {\it Volby 2018}. The advantage of Gemini 2.5 Flash is most pronounced on {\it Summit Kim-Trump} and {\it Volby 2018} datasets. Llama 4 Scout and Qwen3 continue to show lower performance across the board, with F1 gaps lower then 0.15.

\medskip \clabel[v3-dataset]{Batch size 1 pipeline comparison by dataset}
\picw=12cm \cinspic figs/results/pipeline-v3-coord-per-dataset.pdf
~\caption/f Parallel coordinates plot for F1 scores by dataset. Gemini 2.5 Flash again demonstrates clear superiority.

\medskip

These results demonstrate that reducing batch size to one does not fundamentally change the ranking or performance gap between models. Gemini 2.5 Flash remains the best-performing model under this more granular, single-article regime, and open-source models continue to lag behind across all metrics, stages, and datasets.


\sec Assessment of Baseline Improvement

We want to verify whether there has been an improvement in the extraction pipeline. In this stage, we compare the baseline implementation and prompts with the improved version obtained by iterating on the prompt design. The iterations were guided solely by the results of a single model (Llama 4) on the validation dataset. However, for the verification, we use a test dataset and a larger number of models. We designate the baseline solution as v1. We designate the improved solution as v2. Both versions were evaluated using four models across several datasets (media cases), measuring precision, recall, and F1 score for each (model, dataset) pair. The goal is to determine if the new version offers statistically significant improvements.

Data is loaded and merged. Evaluation results from both versions are exported as CSV files, loaded, and standardized by renaming columns for clarity. Both datasets are merged on model and dataset, enabling direct comparison. For each metric, the difference between the new and old version is calculated for every (model, dataset) pair.

Descriptive statistics give overview. The mean and standard deviation are calculated for each metric and version, along with the mean difference. This step provides an initial impression of potential improvements in precision, recall, and F1 score, indicating the central tendency and spread for both system versions.

For each metric (precision, recall, F1 score), we formally test the following hypotheses:
\begitems
* Null Hypothesis ($H_0$): There is no difference in the metric between the two system versions.
* Alternative Hypothesis ($H_1$): There is a difference in the metric between the two versions (two-sided test).
\enditems

Mathematically, for each metric $M$:

$$
H_0: {\bbchar E}[M_{v2} - M_{v1}] = 0
$$
$$
H_1: {\bbchar E}[M_{v2} - M_{v1}] \neq 0
$$

Test methodology is robust. The paired t-test assesses whether the mean difference is significantly different from zero, assuming approximate normality. The Wilcoxon signed-rank test provides a non-parametric alternative that does not require normality. A p-value below 0.05 is considered statistically significant.

\midinsert \clabel[descriptive-v1-v2]{Descriptive statistics for system versions 1 and 2}
\ctable{l|cccccc}{
Metric & $\mu_1$ & $\sigma_1$ & $\mu_2$ & $\sigma_2$ & $\mu_{2-1}$ & $\sigma_{2-1}$ \crl \tskip4pt
Precision & 0.629 & 0.117 & 0.633 & 0.091 & 0.004 & 0.082 \cr
Recall    & 0.494 & 0.085 & 0.509 & 0.083 & 0.014 & 0.042 \cr
F1 score  & 0.450 & 0.123 & 0.476 & 0.103 & 0.026 & 0.067 \cr
}
~\caption/t This table provides a summary of the central tendency ($\mu$) and spread ($\sigma$) for each metric (precision, recall, F1 score) in both system versions. $\mu_1$, $\sigma_1$ refer to version 1, $\mu_2$, $\sigma_2$ to version 2, and $\mu_{2-1}$, $\sigma_{2-1}$ to the difference. Read horizontally to compare old and new versions. The final columns show the average change, indicating if, on average, the new system increases or decreases performance.
\endinsert


\midinsert \clabel[statistics-v1-v2]{Statistical test results for metric differences}
\ctable{l|cccc}{
Metric & $\mu_{2-1}$ & $\sigma_{2-1}$ & t-test p-value & Wilcoxon p-value \crl \tskip4pt
Precision & 0.004 & 0.082 & 0.840 & 0.700 \cr
Recall    & 0.014 & 0.042 & 0.140 & 0.180 \cr
F1 score  & 0.026 & 0.067 & 0.098 & 0.290 \cr
}
~\caption/t This table shows the results of both statistical tests for every metric. The Mean Diff ($\mu_{2-1}$) and Std Diff ($\sigma_{2-1}$) reflect the average and standard deviation of observed improvement. The p-values (t-test and Wilcoxon) indicate whether observed changes are statistically significant. To interpret, look for p-values below 0.05, which indicate strong evidence against the null hypothesis.
\endinsert

\medskip  \clabel[v1-v2-bar-plot]{Comparison v1-v2: Grouped bar plot}
\picw=14cm \cinspic figs/discussion/v1-v2-barplot.pdf
~\caption/f Grouped bar plot displays F1 scores for each (model, dataset) combination for both system versions. Each group contains two bars: one for the old version and one for the new version. To interpret, locate the (model, dataset) of interest along the axes, then compare the height of the bars within each group. Taller bars indicate higher F1 scores. If the bar for the new version is taller than that of the old, the new system performed better for that specific case. This figure allows a quick visual scan for systematic improvements or regressions across all groups.
\medskip

\midinsert
\medskip  \clabel[v1-v2-diff-bar-plot]{Comparison v1-v2: Difference bar plot}
\picw=10cm \cinspic figs/discussion/v1-v2-f-diff.pdf
~\caption/f Bar plot visualizes the difference in F1 score (v2 - v1) for each (model, dataset) pair. The x-axis shows different models, grouped or colored by dataset. Each bar’s height indicates how much the F1 score changed: positive values (bars above zero) represent improvements, and negative values (bars below zero) represent declines. The horizontal dotted line at zero provides a clear visual threshold—bars crossing above this line signify improvement, those below indicate regression. To interpret, examine the position and height of each bar relative to zero.
\medskip
\endinsert

\medskip  \clabel[v1-v2-data-points]{Comparison v1-v2: Scatter plot}
\picw=10cm \cinspic figs/discussion/v1-v2-data-points.pdf
~\caption/f Scatter plot compares F1 scores of the two versions for each (model, dataset) pair. The shape represents a dataset, color of the point represents a model. Each point represents one (model, dataset) instance, with the x-coordinate showing the old version’s F1 and the y-coordinate showing the new version’s F1. The diagonal dashed line marks equality ($y = x$): points above this line indicate that the new version outperformed the old for that pair, while points below indicate a decrease. The distance from the line reveals the magnitude of the difference. This figure allows the reader to quickly assess whether improvements are systematic (most points above the line) or inconsistent.
\medskip

Conclusion is based on statistical evidence. Statistical tests and visualizations indicate a trend toward improved F1 scores, but these improvements do not reach statistical significance at the 0.05 level. Mean values are generally higher for the new version, but standard deviations indicate considerable overlap.

\sec Assessment of Single Batch Approach

We want to verify whether the minimum batch approach improves the solution. This is an approach where each phase of the extraction pipeline works with only one article. The idea is that if the model uses a smaller context, it can focus more effectively on details in the text. We will refer to this approach as v3. This approach is compared with the extraction method that uses the maximum batch size, meaning that all five articles are processed in a single inference. We refer to this approach as v2, the same as in the previous section.

Statistical comparison continues for system versions two and three. Both versions are evaluated on the same set of models and datasets (media cases), with precision, recall, and F1 score measured for each (model, dataset) pair. The purpose is to identify any statistically significant improvement in the third version over the second.

Data handling is consistent. Results from both versions are loaded, standardized, and merged by model and dataset. For each metric, the difference between the new and previous version is computed for every (model, dataset) pair.

Descriptive statistics reveal distributions. The mean ($\mu$) and standard deviation ($\sigma$) for each metric and version, together with the mean and standard deviation of their differences, provide a summary of central tendency and variability. This statistical overview indicates the typical value and spread for each system version and the magnitude of observed changes.

\midinsert \clabel[descriptive-v2-v3]{Descriptive statistics for system versions 2 and 3}
\ctable{l|cccccc}{
Metric & $\mu_2$&	$\sigma_2$&	$\mu_3$&	$\sigma_3$&	$\mu_{3-2}$&	$\sigma_{3-2}$ \crl \tskip4pt
Precision&	0.622&	0.097&	0.610&	0.094&	-0.012&	0.049 \cr
Recall&	0.497&	0.087&	0.503&	0.094&	0.005&	0.059 \cr
F1 score&	0.459&	0.106&	0.465&	0.098	&0.006	&0.053 \cr
}
~\caption/t Descriptive statistics for precision, recall, and F1 score for system versions 2 and 3. $\mu_2$ and $\sigma_2$ are the mean and standard deviation for version 2, $\mu_3$ and $\sigma_3$ for version 3, and $\mu_{3-2}$, $\sigma_{3-2}$ are the mean and standard deviation of the difference (version 3 minus version 2).
\endinsert

Statistical hypotheses are formally tested. The null hypothesis for each metric asserts no difference between version 3 and version 2; the alternative hypothesis asserts a difference. The tests used are the paired t-test (which assumes normally distributed differences) and the Wilcoxon signed-rank test (which is non-parametric). A p-value less than 0.05 is interpreted as statistically significant evidence of a difference.

\midinsert \clabel[test-v2-v3]{Statistical test results for difference between versions 3 and 2}
\ctable{l|cccc}{
Metric&	$\mu_{3-2}$&	$\sigma_{3-2}$&	t-test p-value&	Wilcoxon p-value \crl \tskip4pt
Precision & -0.012 & 0.049 & 0.362 & 0.389 \cr
Recall    & 0.005  & 0.059 & 0.727 & 0.489 \cr
F1 score  & 0.006  & 0.053 & 0.667 & 0.421 \cr
}
~\caption/t Statistical test results for metric differences (version 3 minus version 2). $\mu_{3-2}$ and $\sigma_{3-2}$ denote the mean and standard deviation of the difference. p-values from the paired t-test and Wilcoxon signed-rank test are shown; p-values below 0.05 would indicate statistical significance.
\endinsert

\medskip  \clabel[v2-v3-bar-plot]{Comparison v2-v3: Grouped bar plot}
\picw=14cm \cinspic figs/discussion/v2-v3-barplot.pdf
~\caption/f Grouped bar plot showing F1 scores for each (model, dataset) combination for both system versions. Each group contains two bars: one for version 2 and one for version 3. The notation for each axis corresponds to the models and datasets. Taller bars for version 3 indicate local improvement. The plot facilitates systematic visual assessment across groups.
\medskip

\medskip  \clabel[v2-v3-diff-bar-plot]{Difference bar plot emphasizes magnitude and direction}
\picw=10cm \cinspic figs/discussion/v2-v3-f-diff.pdf
~\caption/f Bar plot displaying the difference in F1 score ($\Delta$F1 = F1$_3$ – F1$_2$) for each (model, dataset) pair. The x-axis categorizes by model, colors distinguish datasets. The horizontal dashed line at zero highlights whether the difference is positive or negative. The magnitude and direction of each bar directly visualizes the local effect of the system change.
\medskip


\medskip  \clabel[v2-v3-data-points]{Scatter plot details pairwise relationships.}
\picw=10cm \cinspic figs/discussion/v2-v3-data-points.pdf
~\caption/f Scatter plot comparing F1 scores for version 2 (x-axis) and version 3 (y-axis) for each (model, dataset) pair. Each point represents a single instance. The diagonal dashed line ($y=x$) is the reference for equality: points above the line indicate higher F1 in version 3, points below indicate higher F1 in version 2. This plot summarizes whether improvements are systematic or sporadic.
\medskip

Performance differences are minimal between system versions two and three.
Descriptive statistics show only a negligible increase in mean F1 score from 0.459 to 0.465, with the mean difference (0.006) far smaller than the standard deviation (0.053). Mean precision actually declined slightly, while mean recall improved by a similarly small margin.

Statistical tests support the absence of significant change.
The paired t-test p-values for all metrics are substantially greater than 0.05 (precision:~0.36; recall:~0.73; F1:~0.67), as are the Wilcoxon signed-rank test p-values. This statistical evidence indicates that the observed differences could easily be due to random variation rather than any systematic effect of the new system version.

Figure analysis confirms the statistical result.
In the grouped bar plots (\ref[v2-v3-bar-plot]), the bars representing system versions two and three are generally similar in height for all model-dataset pairs. In the difference bar plot (\ref[v2-v3-diff-bar-plot]), most bars cluster close to zero, with a nearly even balance of positive and negative values, further indicating the lack of consistent improvement or decline. The scatter plot (\ref[v2-v3-data-points]) shows that most points lie close to the diagonal, reinforcing that performance between the two versions is nearly equivalent for every tested combination.

Overall, the empirical evidence from all metrics, tests, and visualizations demonstrates that system version three does not yield any substantial or statistically significant improvement over version two. The approach of extracting data article by article is not more efficient than batch inference.


\label[temperature-influence]
\sec Temperature Influence

Model performance can be sensitive to the temperature parameter, which governs the stochasticity of the sampling process during inference. The influence of temperature on key evaluation metrics—precision, recall, and F1 score—was thoroughly investigated to inform optimal model configuration. The analysis combines both a visual and a statistical approach, ensuring a robust understanding of how temperature shapes the behavior of these metrics. The improved pipeline version v2 was used for testing. The evaluation was performed using the Llama 4 Scout model, with six discrete values of the temperature parameter, which were evenly distributed within the interval between 0 and 1. For each temperature value, ten evaluation runs were carried out. The relationship between temperature and each metric was explored using a comprehensive plot that integrates both distributional and trend-based information.

\medskip  \clabel[temperature]{Temperature comparison}
\picw=15cm \cinspic figs/results/temperature.pdf
~\caption/f Distribution and regression analysis of precision, recall, and F1 score as a function of the temperature parameter. For each metric, boxplots illustrate the distribution of scores at each temperature setting, with the solid and dashed lines representing linear and quadratic regression fits, respectively.
\medskip


In this visualization, the x-axis represents the temperature parameter ranging from 0.0 to 1.0, while the y-axis shows the value of the respective metric—precision, recall, or F1. Each temperature setting is associated with a boxplot reflecting the distribution of scores across multiple model runs. The boxplots indicate medians, interquartile ranges, and potential outliers, providing insight into the variability and stability of each metric at different temperatures. Superimposed on these boxplots are two regression curves: a solid line depicting the linear regression fit and a dashed line corresponding to the quadratic regression fit. These curves summarize the overall trend in metric values as a function of temperature, capturing both monotonic and potentially non-linear relationships.

To interpret this plot, one should first compare the medians and spreads of the boxplots at each temperature, thereby gauging how the central tendency and variability of the metrics change with temperature. The regression lines then provide a succinct summary of global trends: a linear decline or increase suggests a straightforward relationship, while a curved (quadratic) fit may indicate an optimal temperature region where metric performance peaks. For instance, a quadratic regression with a maximum may reveal the existence of a temperature value that maximizes the metric, guiding the choice of parameter for deployment.

To assess the statistical significance of these observed effects, a one-way analysis of variance (ANOVA) was conducted for each metric, testing the null hypothesis that mean metric values are equal across all temperatures. The results of these tests are provided in \ref[ANOVA-temperature].

\midinsert \clabel[ANOVA-temperature]{ANOVA test summary for temperature significance}
\ctable{l|ccc}{
Metric & F-statistic & p-value & Significance \crl \tskip4pt
Precision & 0.40 & 0.85 & No \cr
Recall & 5.42 & 0.00045 & {\bf Yes} \cr
F1 & 0.47 & 0.79 & No \cr
}
~\caption/t Summary of ANOVA test results for the effect of temperature on model evaluation metrics. Statistically significant results $(p < 0.05)$ are highlighted in bold.
\endinsert

In Table \ref[ANOVA-temperature], the F-statistic and p-value are reported for each metric, along with an indication of statistical significance at the standard 0.05 threshold. Only recall was found to vary significantly with temperature ($F = 5.42$, p = $0.00045$), indicating that changes in the temperature parameter can lead to meaningful differences in recall. In contrast, precision ($F = 0.40$, $p = 0.85$) and F1 score ($F = 0.47$, $p = 0.79$) showed no statistically significant dependence on temperature, suggesting stability across the tested range. The summary table should be read such that a significant result denotes substantial differences in the mean value of the metric across temperature settings, while a non-significant result implies relative invariance.

In summary, this analysis reveals that temperature tuning exerts a pronounced influence on recall but has negligible effects on precision and F1 score within the examined range. This finding is visually apparent from the combined plot, where recall displays clear shifts as temperature changes, and statistically confirmed by the ANOVA test. For practitioners, this implies that careful adjustment of the temperature parameter can optimize recall, while precision and F1 remain robust to temperature variation.

\sec Prediction Examples
We subjectively evaluate several problematic outputs. In most cases, the models deliver acceptable results, and their responses are both coherent and factually accurate. The primary concern, however, is the lack of extraction granularity, which corresponds to reduced recall. For a subset of systems, especially open-source variants, a limited amount of pretraining data in Czech is particularly evident. As a consequence, generated passages may not adhere to proper grammar. For example, DeepSeek R1 produced the phrase “scénářový autor” instead of scénárista and substituted režisér with ředitel.
\begtt
"name": "Petr Zelenka",
"roles": ["režisér", "scénářový autor", "ředitel televizního seriálu"]
\endtt
Qwen3 sometimes produces duplicate entries among both roles and entities. Another illustration highlights instruction drift, where an individual is returned without a fully specified name and the associated role is completely inappropriate.
\begtt
"name": "Donald Trump",
"roles": ["prezident USA", "americký prezident"]

"name": "Kim Čong-una sestra",
"roles": ["soustružník"]
\endtt